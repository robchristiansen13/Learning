{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-6580 Assignment 4 - Transfer Learning for Image Classification\n",
        "\n",
        "**YOUR NAME HERE**\n",
        "\n",
        "*Weber State University*"
      ],
      "metadata": {
        "id": "eRaA_UTk91jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, you'll use transfer learning to create an image classification to answer that age-old question - \"Alpaca or Not Alpaca?\"\n",
        "\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1UlqhWyAmgrIjea-eyhxEpXt714NAbhct\" width = 300/>\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "Specifically, we'll use a dataset consisting of many images of Alpacas and many images of not Alpacas (you know giraffes, bears, etc...). You'll use this dataset to build a model that does a pretty good job identifying alpacas."
      ],
      "metadata": {
        "id": "9RxzuHeL-Iyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so first we'll need to grab the dataset. I've got it stored on my Google Drive as a .zip file, and you can download it and unzip it within your local environment."
      ],
      "metadata": {
        "id": "KVee-cdgAqYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll need the gdown library. Depending on your environment, you might already have gdown. If not, you should be able to install it with the following command:"
      ],
      "metadata": {
        "id": "Z0g0xAjVWr2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the comment below and run the command. If you do need to run it, you should only need to run it once.\n",
        "# !pip install gdown"
      ],
      "metadata": {
        "id": "njBOYCfiWzKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's import the libraries we'll need."
      ],
      "metadata": {
        "id": "ppxaCX5gXCKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The Usual Suspects\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Our deepl learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "#Some libraries for downloading and unzipping files\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "#For creating a dataset from image libraries\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "#For augmenting our image data\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom"
      ],
      "metadata": {
        "id": "wWdPE57fIukA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting The Data"
      ],
      "metadata": {
        "id": "zikSWJCMZtRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's download the data. This should create an alpacas.zip file in your local directory, and then a folder called \"dataset\". Within \"dataset\" there should be two floders: \"alpaca\" and \"not alpaca\". The first contains images of alpacas, while the second contains images of, you guessed it, not alpacas. You should only need to run this once."
      ],
      "metadata": {
        "id": "Wfikh3bFCofP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=13Efb3r1czCgtFqntWU48cbdOpnsRfu0H'\n",
        "output = 'alpacas.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile('./alpacas.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ],
      "metadata": {
        "id": "rnIjXrLQCtHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now create and training and validation datasets using the [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) function from Keras. We'll create batches of 32 images each, and we note that each image is of size $160 \\times 160$. Now, we don't have a lot of data, so we won't even create a test dataset. Note that it's important to use the same random seed (in this case 6580) for these datasets. Otherwise, you might get overlap."
      ],
      "metadata": {
        "id": "eZLk0QKXYTQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (160, 160)\n",
        "directory = \"dataset/\"\n",
        "\n",
        "train_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='training',\n",
        "                                             seed=6580)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='validation',\n",
        "                                             seed=6580)"
      ],
      "metadata": {
        "id": "iib4HH6tICNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check out the labels assigned to the images. These were determined by the folder names."
      ],
      "metadata": {
        "id": "vN8U1ZBQZGNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "WCvzQQVpZOOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of the images:"
      ],
      "metadata": {
        "id": "-epHKf9jZV0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "sL8odtKBInHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Model"
      ],
      "metadata": {
        "id": "LghFfNPAZpmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we use transfer learning, let's try to build a simple convolutional model of our own. Specifically, create a binary classification model for our problem. Be sure to use some convolutional layers and some pooling layers."
      ],
      "metadata": {
        "id": "g2H7q8gYaBOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(160, 160, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "\n",
        "#END CODE\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "LrkMYLQ-ad8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now compile the model you created, using a binary crossentry loss, and an rmsprop optimizer. We'll also keep track of the accuracy."
      ],
      "metadata": {
        "id": "0YK-AbVEbHov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "PEb8TaeebWob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train! This may take a minute."
      ],
      "metadata": {
        "id": "WJNRc8QpbadC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset)"
      ],
      "metadata": {
        "id": "FvViF6-Tbl2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the validation accuracy looks over the epochs:"
      ],
      "metadata": {
        "id": "CPXtrALRb5-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aQ1boSmjcYBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That probably doesn't look great. Don't worry, it's not supposed to."
      ],
      "metadata": {
        "id": "11N6C-pLcsTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "qhi0R8rNczs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers.\n",
        "\n",
        "As we did in Lecture 8, implement a function for data augmentation. Use a Sequential keras model composed of 3 layers:\n",
        "\n",
        "* RandomFlip('horizontal')\n",
        "* RandomRotation(0.2)\n",
        "* RandomZoom(0.2)"
      ],
      "metadata": {
        "id": "6tJsctW7dBnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmenter():\n",
        "    '''\n",
        "    Create a Sequential model composed of 3 layers\n",
        "    Returns:\n",
        "        tf.keras.Sequential\n",
        "    '''\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #END CODE\n",
        "\n",
        "    return data_augmentation"
      ],
      "metadata": {
        "id": "yTAmbsPOdrW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the code you wrote."
      ],
      "metadata": {
        "id": "0VvvppGfeI-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = data_augmenter()\n",
        "\n",
        "assert(augmenter.layers[0].name.startswith('random_flip')), \"First layer must be RandomFlip\"\n",
        "assert augmenter.layers[0].mode == 'horizontal', \"RadomFlip parameter must be horizontal\"\n",
        "assert(augmenter.layers[1].name.startswith('random_rotation')), \"Second layer must be RandomRotation\"\n",
        "assert augmenter.layers[1].factor == 0.2, \"Rotation factor must be 0.2\"\n",
        "assert(augmenter.layers[2].name.startswith('random_zoom')), \"Second layer must be RandomZoom\"\n",
        "assert augmenter.layers[2].height_factor == 0.2, \"Zoom factor must be 0.2\"\n",
        "\n",
        "print('\\033[92mAll tests passed!')"
      ],
      "metadata": {
        "id": "qpZo1vukeLc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out some of the augmented images:"
      ],
      "metadata": {
        "id": "8SpB2cp7fd4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = augmenter(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "IuJsQwSXflHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the model you put together earlier does with this augmented data. (Just copy / paste the code you used above.)"
      ],
      "metadata": {
        "id": "uXcRnc43gL48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(160, 160, 3))\n",
        "x = augmenter(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#END CODE\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "UPw7NhvofxU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset)\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQA7sjaigVgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, probably not great."
      ],
      "metadata": {
        "id": "lvNmp9R1grSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transfer Learning from MobileNetV2"
      ],
      "metadata": {
        "id": "jNB96Vaag84L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNetV2 was trained on ImageNet and is optimized to run on mobile and other low-power applications. It's 155 layers deep, and very efficient for object detection and image segmentation tasks, as well as classification tasks like this one."
      ],
      "metadata": {
        "id": "t7ckiWqPhDU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this model, we'll load the pre-trained weights from ImageNet (the dataset from which it trained). We'll also normalize our inputs to be from $-1$ to $1$ (and not from $0$ to $255$)."
      ],
      "metadata": {
        "id": "KRmgIfYHhPHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "#This is the MobileNetV2 model.\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=True,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "#This scales the input values so they're appropriate for MobileNetV2\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ],
      "metadata": {
        "id": "GmJh4Q_Ch9t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the model summary below to see all the model's layers, the shapes of their outputs, and the total number of parameters, trainable and non-trainable. It's pretty big!"
      ],
      "metadata": {
        "id": "gsC9lb3qiPYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "2QVClRL2ik6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the last two layers here. These are the so called top layers, and they are responsible for the classification in the model."
      ],
      "metadata": {
        "id": "zHZz-QAEiqNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_layers = len(base_model.layers)\n",
        "print(base_model.layers[nb_layers - 2].name)\n",
        "print(base_model.layers[nb_layers - 1].name)"
      ],
      "metadata": {
        "id": "r8cduv2wi1mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how this model does on our dataset."
      ],
      "metadata": {
        "id": "ZEDbgpSwjNxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "feature_batch = base_model(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "L0VVRMDQjj5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, for each image, there are 1000 predictions! The 1000 value refers to the 1000 classes the model was pretrained on. The predictions returned by the base model below follow this format:\n",
        "\n",
        "First the class number, then a human-readable label, and last the probability of the image belonging to that class. You'll notice that there are two of these returned for each image in the batch - these the top two probabilities returned for that image."
      ],
      "metadata": {
        "id": "rXcMSoqdjxHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False\n",
        "image_var = tf.Variable(image_batch)\n",
        "pred = base_model(image_var)\n",
        "\n",
        "tf.keras.applications.mobilenet_v2.decode_predictions(pred.numpy(), top=2)"
      ],
      "metadata": {
        "id": "9zR0iuEdj_Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uh-oh. There's a whole lot of labels here, some of them hilariously wrong, but none of them say \"alpaca.\"\n",
        "\n",
        "This is because MobileNet pretrained over ImageNet doesn't have the correct labels for alpacas, so when you use the full model, all you get is a bunch of incorrectly classified images.\n",
        "\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1aE4i02438uMNK7jPA2CcCdeFpbTKzupl\" width = 300/>\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "Fortunately, you can delete the top layer, which contains all the classification labels, and create a new classification layer."
      ],
      "metadata": {
        "id": "qB127ZcYkEE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a pretrained model to modify the classifier task so that it's able to recognize alpacas. You can achieve this in three steps:\n",
        "\n",
        "1. Delete the top layer (the classification layer)\n",
        "    * Set `include_top` in `base_model` as False\n",
        "2. Add a new classifier layer\n",
        "    * Train only one layer by freezing the rest of the network\n",
        "    * As mentioned before, a single neuron is enough to solve a binary classification problem.\n",
        "3. Freeze the base model and train the newly-created classifier layer\n",
        "    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n",
        "    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer"
      ],
      "metadata": {
        "id": "X1P_-ywljWqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
        "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
        "    Arguments:\n",
        "        image_shape -- Image width and height\n",
        "        data_augmentation -- data augmentation function\n",
        "    Returns:\n",
        "    Returns:\n",
        "        tf.keras.model\n",
        "    '''\n",
        "\n",
        "\n",
        "    input_shape = image_shape + (3,)\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    base_model =\n",
        "\n",
        "    # Freeze the base model by making it non trainable\n",
        "\n",
        "    # create the input layer (Same as the imageNetv2 input size)\n",
        "\n",
        "    # apply data augmentation to the inputs\n",
        "\n",
        "    # data preprocessing using the same weights the model was trained on\n",
        "    # Already Done -> preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
        "\n",
        "    # Add the new Binary classification layers\n",
        "    # use global avg pooling to summarize the info in each channel\n",
        "\n",
        "    # create a prediction layer with one neuron (as a classifier only needs one)\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3Gbt97B1i7uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = alpaca_model(IMG_SIZE, augmenter)"
      ],
      "metadata": {
        "id": "RHLxPVfUlM-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base learning rate has been set for you, so you can go ahead and compile the new model and run it for 10 epochs:"
      ],
      "metadata": {
        "id": "utnXAc5DlpXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.01\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yKPRH53Mlvmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epochs = 10\n",
        "history = model2.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
      ],
      "metadata": {
        "id": "g9g6uUpbl1uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "moaXSPFjlzJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright! Transfer learning for the win.\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1jjV9teAyrbNFDB1o8shMLpD6CrrllxWa\" width = 300/>\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "For this assignment, you should upload the modified Jupyter notebook (in which all the code you're asked to implement has been implemented) to Canvas."
      ],
      "metadata": {
        "id": "4S1I_Yxen3i4"
      }
    }
  ]
}