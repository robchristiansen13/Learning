{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-6580 Assignment 5 - Transformers\n",
        "\n",
        "**YOUR NAME HERE**\n",
        "\n",
        "*Weber State University*\n",
        "\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZ2YbUE_QnIwPLRb0cbQrp1tv3XS5NeYJoAi6YrdvLrwziUfZbbf1uQR5Ek_zYNL7OFXXUMmZTN776hhXU4dkiMgJel=s1600\" width = 450/>\n",
        "    </div>\n",
        "</center>"
      ],
      "metadata": {
        "id": "4lZ3tie7uS1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers are a [big deal](https://youtu.be/9uw3F6rndnA?feature=shared). For this assignment you will get your hands dirty implementing the [transformer architecture](https://arxiv.org/abs/1706.03762) we've discussed in class. I know it was a lot and might have seemed confusing, but don't worry - we'll walk through it.\n",
        "\n",
        "This assignment is based upon the final programming assignment from Andrew Ng's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) course on Corsera, although it is significantly modified. Specifically, we'll be using the unit tests written for that assignment. You'll need to download them, so let's do that now. I've made them available on my Google Drive, so you'll just download them from there.\n",
        "\n",
        "First, you'll need to make sure you have the [gdown](https://pypi.org/project/gdown/) library. You probably do, but just in case you can run the code below. Installing it if you've already got it won't cause any problems - your environment will just check and see you've already got it, which will mean it does nothing. It just might take a few second to do the check."
      ],
      "metadata": {
        "id": "X8w-EntlvOFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBRXxuKLsLa2"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, now that you've got gdown, let's import it and use it to get the *public_tests.py* file that contains our unit tests."
      ],
      "metadata": {
        "id": "_GrUhj5xwt8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "HSAkOPb-sa1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=11IxRzXkTLoJKuqgnC7swCKRfjyRB7Xk6'\n",
        "output = 'public_tests.py'\n",
        "gdown.download(url,output)\n",
        "\n",
        "from public_tests import *"
      ],
      "metadata": {
        "id": "4BG3rS31sfKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright - you should now have a *public_tests.py* file in your working directory, and the unit tests from that file should now be imported and ready to run from your environment. Sweet."
      ],
      "metadata": {
        "id": "G4VJUnxOw9gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've imported the files specific to this assignment, let's import the libraries and methods we'll need for this assignment."
      ],
      "metadata": {
        "id": "IV8sYY37xRbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Our data science friends we just can't live without\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Our deep learning friends we've gotten to know a bit\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#We'll also grab a couple others that will be useful to us\n",
        "import time"
      ],
      "metadata": {
        "id": "TIHJLgfdxdUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also want to import some methods from Keras that will be useful to us as we code our transformer - and that we don't want to create from scratch."
      ],
      "metadata": {
        "id": "lYy4FJn0yXH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"
      ],
      "metadata": {
        "id": "rYp32aTAyr23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
        "from transformers import TFDistilBertForTokenClassification"
      ],
      "metadata": {
        "id": "EnNXXoY2JVea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Positional Encoding"
      ],
      "metadata": {
        "id": "pBt1Yn6xzqw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we'll do is write some function to handle positional encoding. This was covered in class, but there are also a number of videos on youtube that go over it. One example is [this one](https://youtu.be/zxQyTK8quyY?feature=shared&t=439). A good article on the reason positional encoding is done how it's done is [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/).\n",
        "\n",
        "In many sequence to sequence tasks, the relative order of the data is important to its meaning. This is certainly the case with text and translation. Order information is inherent to the structure of an RNN, and in its operation the data is given to the model in sequence.  However, for a Transformer network using multi-head attention, the data is given to the model all at once. While this can significantly reduce training time, there is no information about the relative order. This is where positional encoding comes in - you can specifically encode the positions of your inputs and pass them into the network. The values of these encodings come from these sine and cosine formulas:\n",
        "    \n",
        "$$\n",
        "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
        "\\tag{1}$$\n",
        "<br>\n",
        "$$\n",
        "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
        "\\tag{2}$$\n",
        "\n",
        "* $d$ is the dimension of the word embedding (the number of components in the vector representation of a word)\n",
        "* $pos$ is the position of the word.\n",
        "* $k$ refers to each of the different dimensions in the positional encodings, with $i$ equal to $k$ $//$ $2$ (this in integer division, so for example $6 // 2 = 7 // 2 = 3$).\n",
        "\n",
        "You can think of the positional encodings broadly as a feature that contains the information about the relative positions of words. The sum of the positional encoding and word embedding is ultimately what is fed into the model. The values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted, and is instead enriched with positional information. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data. Why the $10000$ in the denominator? It's really just a hyperparameter that seems to work well."
      ],
      "metadata": {
        "id": "jXuCFqplz2Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sine and Cosine Angles\n",
        "\n",
        "Notice that even though the sine and cosine positional encoding equations take in different arguments (`2i` versus `2i+1`, or even versus odd numbers) the inner terms for both equations are the same:\n",
        "\n",
        "$$\n",
        "\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}\n",
        "$$\n",
        "\n",
        "Consider the inner term as you calculate the positional encoding for a word in a sequence.<br>\n",
        "\n",
        "$$\n",
        "PE_{(pos, 0)}= sin\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right),\n",
        "$$\n",
        "\n",
        "since solving `2i = 0` gives `i = 0` <br>\n",
        "\n",
        "$$\n",
        "PE_{(pos, 1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right),\n",
        "$$\n",
        "\n",
        "since solving `2i + 1 = 1` gives `i = 0`\n",
        "\n",
        "The angle is the same for both! The angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are the same as well, since for both, `i = 1` and therefore the inner term is $\\left(\\frac{pos}{{10000}^{\\frac{2}{d}}}\\right)$. This relationship holds true for all paired sine and cosine curves.\n",
        "\n",
        "### Exercise 1 - get_angles\n",
        "\n",
        "For your first exercise, you'll implement the function `get_angles()` to calculate the possible angles for the sine and cosine positional encodings. In the code you'll want to replace anywhere you see *None* with appropriate code, unless the comments specifically tell you to leave a specific *None* alone. That's basically what you do for all the exercises in this assignment.\n",
        "\n",
        "**Hints**\n",
        "\n",
        "- If `k = [0, 1, 2, 3, 4, 5]`, then, `i` must be `i = [0, 0, 1, 1, 2, 2]`\n",
        "- `i = k//2`\n",
        "- Keep track of the shape of your vectors.\n",
        "- [List comprehensions](https://www.w3schools.com/python/python_lists_comprehension.asp) could be a smooth way to do this."
      ],
      "metadata": {
        "id": "tV9anrTP57XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, k, d):\n",
        "    \"\"\"\n",
        "    Get the angles for the positional encoding\n",
        "\n",
        "    Arguments:\n",
        "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
        "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
        "        d(integer) -- Encoding size\n",
        "\n",
        "    Returns:\n",
        "        angles -- (pos, d) numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    # START CODE HERE\n",
        "    # Get i from dimension span k\n",
        "    i = None\n",
        "    # Calculate the angles using pos, i and d\n",
        "    angles = None\n",
        "    # END CODE HERE\n",
        "\n",
        "    return angles"
      ],
      "metadata": {
        "id": "P1d4-Ycotf93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. Now we're going to run our unit test to check if the code above passes. This unit test is one of the ones we imported at the top of this notebook. If it doesn't pass, there's an issue with your code, and you should fix it. If it does pass the unit test will output \"All tests passed\".\n",
        "\n",
        "Also, we'll generate an example of an angle array that can be produced with this function."
      ],
      "metadata": {
        "id": "820BRihg8m4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "get_angles_test(get_angles)\n",
        "\n",
        "# Example\n",
        "position = 4\n",
        "d_model = 8\n",
        "pos_m = np.arange(position)[:, np.newaxis]\n",
        "dims = np.arange(d_model)[np.newaxis, :]\n",
        "get_angles(pos_m, dims, d_model)"
      ],
      "metadata": {
        "id": "bP6vd86pt1RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sine and Cosine Encodings\n",
        "\n",
        "Now you can use the angles you computed to calculate the sine and cosine positional encodings.\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
        "$$\n",
        "\n",
        "### Exercise 2 - positional_encoding\n",
        "\n",
        "Implement the function `positional_encoding()` to calculate the sine and cosine  positional encodings\n",
        "\n",
        "**Reminder:** Use the sine equation when $i$ is an even number and the cosine equation when $i$ is an odd number.\n",
        "\n",
        "#### Additional Hints\n",
        "* You may find\n",
        "[np.newaxis](https://numpy.org/doc/stable/user/basics.indexing.html#dimensional-indexing-tools) useful depending on the implementation you choose."
      ],
      "metadata": {
        "id": "MEgqmQwp9MCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(positions, d):\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings\n",
        "\n",
        "    Arguments:\n",
        "        positions (int) -- Maximum number of positions to be encoded\n",
        "        d (int) -- Encoding size\n",
        "\n",
        "    Returns:\n",
        "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
        "    \"\"\"\n",
        "    # START CODE HERE\n",
        "    # initialize a matrix angle_rads of all the angles\n",
        "    angle_rads = get_angles(None,\n",
        "                            None,\n",
        "                            None)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = None\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = None\n",
        "    # END CODE HERE\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "a5chYAjE9r1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "positional_encoding_test(positional_encoding, get_angles)"
      ],
      "metadata": {
        "id": "WJTqONhL92bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your output was \"All tests passed\" then good job! The Coursera assignment has the following visualization of the positional encodings. You may find it insightful. I (Dylan) must admit I did not, but I've included it in case you're interested."
      ],
      "metadata": {
        "id": "puz9dJZC9_Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('d')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3ZJtwP79-Q3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Masking\n",
        "\n",
        "Now let's look into how we handle masking. You can relax a bit for this section, as the coding is all handled for you. But, please read through it and make sure you understand it, both because doing so is important if you want to understand transformers, and because it might be useful later in the assignment."
      ],
      "metadata": {
        "id": "TvGMbP9P-YUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence.\n",
        "\n",
        "### Padding Mask\n",
        "\n",
        "Often your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five (in practice hopefully it's much larger than THAT, but this is just an example), and it is given the following sequences:\n",
        "\n",
        "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"],\n",
        "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
        "     [\"Exciting\", \"!\"]\n",
        "    ]\n",
        "\n",
        "which might get vectorized (using [label encoding](https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/)) as:\n",
        "\n",
        "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
        "     [ 56, 1285, 15, 181, 545],\n",
        "     [ 87, 600]\n",
        "    ]\n",
        "    \n",
        "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model. Doing this, our sequence would turn into:\n",
        "\n",
        "    [[ 71, 121, 4, 56, 99],\n",
        "     [ 2344, 345, 1284, 15, 0],\n",
        "     [ 56, 1285, 15, 181, 545],\n",
        "     [ 87, 600, 0, 0, 0],\n",
        "    ]\n",
        "    \n",
        "Sequences longer than the maximum length of five are truncated, and zeros are added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros are added for padding. However, these zeros will affect the softmax calculation. With softmax, you're taking the exponential of each of the terms, and $e^{0}$ is $1$! This is when a padding mask comes in handy. What we want to do is make the numbers we want to ignore $-\\infty$, and if we can't do that a very negative number like $-1e9$ wil do. This is implemented for you below. ðŸ˜‡ Just make sure you go through the code so you understand how it works.\n",
        "\n",
        "After applying this masking, the padded input sequence `[87, 600, 0, 0, 0]` would change to `[87, 600, -1e9, -1e9, -1e9]`, so that when you take the softmax, the original zeros go to $0$ in the softmax.\n",
        "\n",
        "**Note:** The [MultiheadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) layer implemented in Keras uses this masking logic."
      ],
      "metadata": {
        "id": "5RYycBFw-1h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(decoder_token_ids):\n",
        "    \"\"\"\n",
        "    Creates a matrix mask for the padding cells\n",
        "\n",
        "    Arguments:\n",
        "        decoder_token_ids -- (n, m) matrix\n",
        "\n",
        "    Returns:\n",
        "        mask -- (n, 1, m) binary tensor\n",
        "    \"\"\"\n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    # this will allow for broadcasting later when comparing sequences\n",
        "    return seq[:, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "q8PYYvzUBM0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code takes in an array of token IDs, and returns an array of indices for whether the token ID is non-zero. For example:"
      ],
      "metadata": {
        "id": "u_QHFmrKBQ1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
        "print(create_padding_mask(x))"
      ],
      "metadata": {
        "id": "BwxotJkNBxag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we multiply (1 - mask) by -1e9 and add it to the sample input sequences, the zeros are essentially set to negative infinity. Notice the difference when taking the softmax of the original sequence and the masked sequence:"
      ],
      "metadata": {
        "id": "RHzVceFnB1mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.keras.activations.softmax(x))\n",
        "print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9))"
      ],
      "metadata": {
        "id": "CZP9cBbnB9Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look-ahead Mask\n",
        "\n",
        "The look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of your training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output.\n",
        "\n",
        "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`.\n",
        "\n",
        "This mask is also implemented for you ðŸ˜‡ðŸ˜‡. Again, take a close look at the code so you understand how it works."
      ],
      "metadata": {
        "id": "b1MxAVciCBjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a lower triangular matrix filled with ones\n",
        "\n",
        "    Arguments:\n",
        "        sequence_length -- matrix size\n",
        "\n",
        "    Returns:\n",
        "        mask -- (size, size) tensor\n",
        "    \"\"\"\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "cGWcBSp4CVof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ],
      "metadata": {
        "id": "GTiwyqs-Cafo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Self-Attention\n",
        "\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihYbPind89WT3zkPp4YhtteBz760p3OYxs32V2ZULwdxWEaoiH2iU5rgr8QYIyrJcdtEN02USwFn9UxbboGAdQoIw5FQ=s1600\" width = 600/>\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
        "$$\n",
        "\n",
        "* $Q$ is the matrix of queries\n",
        "* $K$ is the matrix of keys\n",
        "* $V$ is the matrix of values\n",
        "* $M$ is the optional mask you choose to apply\n",
        "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
        "\n",
        "### Exercise 3 - scaled_dot_product_attention\n",
        "\n",
        "Implement the function `scaled_dot_product_attention()` to create attention-based representations.\n",
        "\n",
        "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead.\n",
        "    \n",
        "    Multiply (1. - mask) by -1e9 before applying the softmax.\n",
        "\n",
        "**Additional Hints**\n",
        "* You may find [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) useful for matrix multiplication.\n",
        "* The dimension of the keys is the number of rows in $K$, the matrix of keys.\n",
        "* To take the square root of a value, you may need to convert in from an integer to a float. The function [tf.cast](https://www.tensorflow.org/api_docs/python/tf/cast) might be useful to you here."
      ],
      "metadata": {
        "id": "Y_5-s64O9-bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "      q, k, v must have matching leading dimensions.\n",
        "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "      The mask has different shapes depending on its type(padding or look ahead)\n",
        "      but it must be broadcastable for addition.\n",
        "\n",
        "    Arguments:\n",
        "        q -- query shape == (..., seq_len_q, depth)\n",
        "        k -- key shape == (..., seq_len_k, depth)\n",
        "        v -- value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable\n",
        "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output -- attention_weights\n",
        "    \"\"\"\n",
        "    # START CODE HERE\n",
        "\n",
        "    matmul_qk = None  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = None\n",
        "    scaled_attention_logits = None\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None: # Don't replace this None\n",
        "        scaled_attention_logits += None\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = None  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = None  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    # END CODE HERE\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "4dCRZBKnEBiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "scaled_dot_product_attention_test(scaled_dot_product_attention)"
      ],
      "metadata": {
        "id": "PCLUyP_zEU7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did all your tests pass? If so, then great! Now let's build the encoder block."
      ],
      "metadata": {
        "id": "JLCs42qHEYfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 - The Encoder\n",
        "\n",
        "The Transformer Encoder layer pairs self-attention with a neural network to produce enhanced work encodings. In this section, you will implement the Encoder by pairing multi-head attention and a feed forward neural network.\n",
        "\n",
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihafp3hfQetahHU_fTtZoYZWEBE85_7Qi1uN9KNc8mt4uSk2dpBos8U9UNih6k7KFJZAh0MaFQIG64mA0GrA0frm7VnbQA=s1600\" width = 350/>\n",
        "        <caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</b></font></center></caption>\n",
        "    </div>\n",
        "</center>\n",
        "\n",
        "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
        "   \n",
        "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. For the `MultiHeadAttention` layer, you will use the [Keras implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation.\n",
        "* The feed forward neural network contains two Dense layers we'll implement as the function `FullyConnected`. It will also use the [Sequential API](https://keras.io/api/models/sequential/) with two dense layers to built the feed forward neural network layers."
      ],
      "metadata": {
        "id": "lS2L5yPrEq8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, embedding_dim)\n",
        "    ])"
      ],
      "metadata": {
        "id": "YYhpqc-MHR-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer\n",
        "\n",
        "Now you will pair multi-head attention and a feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization because these seem to help.\n",
        "\n",
        "### Exercise 4 - EncoderLayer\n",
        "\n",
        "In this exercise, you will implement one encoder block (Figure 2) using the `call()` method. The function should perform the following steps:\n",
        "1. You will pass the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K *should be the same*. Set the default values for `return_attention_scores` and `training`. You will also perform Dropout in this multi-head attention layer during training.\n",
        "2. Now add a skip connection by adding your original input `x` and the output of the your multi-head attention layer.\n",
        "3. After adding the skip connection, pass the output through the first normalization layer.\n",
        "4. Finally, repeat steps 1-3 but with the feed forward neural network with a dropout layer instead of the multi-head attention layer.\n",
        "\n",
        "**Additional Hints**\n",
        "    \n",
        "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`.\n",
        "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. You might also find the section on this method from \"Deep Learning with Python\" helpful. *Note that if query, key and value are the same, then this function performs self-attention.*\n",
        "* The call arguments for `self.mha` are (Where B is for batch_size, T is for target sequence shapes, and S is output_shape):\n",
        " - `query`: Query Tensor of shape (B, T, dim).\n",
        " - `value`: Value Tensor of shape (B, S, dim).\n",
        " - `key`: Optional key Tensor of shape (B, S, dim). If not given, will use value for both key and value, which is the most common case.\n",
        " - `attention_mask`: a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension.\n",
        " - `return_attention_scores`: A boolean to indicate whether the output should be attention output if True, or (attention_output, attention_scores) if False. Defaults to False.\n",
        " - `training`: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Defaults to either using the training mode of the parent layer/model, or False (inference) if there is no parent layer. Take a look at [tf.keras.layers.Dropout](https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Dropout) for more details (Additional reading in [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute))"
      ],
      "metadata": {
        "id": "KClkvgHGHS00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
        "    followed by a simple, positionwise fully connected feed-forward network.\n",
        "    This architecture includes a residual connection around each of the two\n",
        "    sub-layers, followed by layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                                  fully_connected_dim=fully_connected_dim)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask -- Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            encoder_layer_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # START CODE HERE\n",
        "        # calculate self-attention using mha(~1 line).\n",
        "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
        "        self_mha_output = None  # Self attention (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # skip connection\n",
        "        # apply layer normalization on sum of the input and the attention output to get the\n",
        "        # output of the multi-head attention layer (~1 line)\n",
        "        skip_x_attention = None  # (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
        "        ffn_output = None  # (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # apply dropout layer to ffn output during training (~1 line)\n",
        "        # use `training=training`\n",
        "        ffn_output = None\n",
        "\n",
        "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output to get the\n",
        "        # output of the encoder layer (~1 line)\n",
        "        encoder_layer_out = None  # (batch_size, input_seq_len, embedding_dim)\n",
        "        # END CODE HERE\n",
        "\n",
        "        return encoder_layer_out"
      ],
      "metadata": {
        "id": "AO63OFImInlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "EncoderLayer_test(EncoderLayer)"
      ],
      "metadata": {
        "id": "cM9V13KtItaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Encoder\n",
        "\n",
        "If you've made it this far and passed your tests, then you've now successfully implemented positional encoding, self-attention, and an encoder layer - give yourself a pat on the back. Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. You will then input your encoded embeddings to a stack of Encoder layers.\n",
        "\n",
        "<center>\n",
        "  <div>\n",
        "    <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihafp3hfQetahHU_fTtZoYZWEBE85_7Qi1uN9KNc8mt4uSk2dpBos8U9UNih6k7KFJZAh0MaFQIG64mA0GrA0frm7VnbQA=s1600\" alt=\"Encoder\" width=\"330\"/>\n",
        "    <caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</b></font></center></caption>\n",
        "  </div>\n",
        "</center>\n",
        "\n",
        "### Exercise 5 - Encoder\n",
        "\n",
        "Complete the `Encoder()` function using the `call()` method to embed your input, add positional encoding, and implement multiple encoder layers.\n",
        "\n",
        "In this exercise, you will initialize your Encoder with an Embedding layer, positional encoding, and multiple EncoderLayers. Your `call()` method will perform the following steps:\n",
        "1. Pass your input through the Embedding layer.\n",
        "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
        "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode.\n",
        "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
      ],
      "metadata": {
        "id": "zq55t0e2c4Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.embedding_dim)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder\n",
        "\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, input_seq_len)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask -- Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # START CODE HERE\n",
        "        # Pass input through the Embedding layer\n",
        "        x = None  # (batch_size, input_seq_len, embedding_dim)\n",
        "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
        "        x *= None\n",
        "        # Add the position encoding to embedding\n",
        "        x += None\n",
        "        # Pass the encoded embedding through a dropout layer\n",
        "        # use `training=training`\n",
        "        x = None\n",
        "        # Pass the output through the stack of encoding layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = None\n",
        "        # END CODE HERE\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, embedding_dim)"
      ],
      "metadata": {
        "id": "o12K6Soid2Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "Encoder_test(Encoder)"
      ],
      "metadata": {
        "id": "s1NcQ2kOd53n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 - Decoder\n",
        "\n",
        "The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
        "\n",
        "<center>\n",
        "  <div>\n",
        "    <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZ9dj6hOfJ3IroGn4Ap8MaiBUalP3XDZRbMgsJbqa9yzuVgEf6PAwKExS81gFwyql1KDuC8lutDnJOsIN5z_-PdjFFYKw=s1600\" alt=\"Encoder\" width=\"250\"/>\n",
        "    <caption><center><font color='purple'><b>Figure 3a: Transformer Decoder layer</b></font></center></caption>\n",
        "  </div>\n",
        "</center>\n",
        "\n",
        "### Decoder Layer\n",
        "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).\n",
        "\n",
        "### Exercise 6 - DecoderLayer\n",
        "    \n",
        "Implement `DecoderLayer()` using the `call()` method\n",
        "    \n",
        "1. Block 1 is a multi-head attention layer with a residual connection, and look-ahead mask. Like in the `EncoderLayer`, Dropout is defined within the multi-head attention layer.\n",
        "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a normalization layer and a residual connection, just like you did before with the `EncoderLayer`.\n",
        "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
        "    \n",
        "**Additional Hints:**\n",
        "* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention"
      ],
      "metadata": {
        "id": "a89OW2BQhWTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The decoder layer is composed by two multi-head attention blocks,\n",
        "    one that takes the new input and uses self-attention, and the other\n",
        "    one that combines it with the output of the encoder, followed by a\n",
        "    fully connected block.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate)\n",
        "\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                                  fully_connected_dim=fully_connected_dim)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        # START CODE HERE\n",
        "        # enc_output.shape == (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # BLOCK 1\n",
        "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
        "        # Dropout will be applied during training (~1 line).\n",
        "        mult_attn_out1, attn_weights_block1 = self.mha1(None, None, None, None, return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
        "        Q1 = None\n",
        "\n",
        "        # BLOCK 2\n",
        "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
        "        # Dropout will be applied during training\n",
        "        # Return attention scores as attn_weights_block2 (~1 line)\n",
        "        mult_attn_out2, attn_weights_block2 = self.mha2(None, None, None, None, return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block (~1 line)\n",
        "        mult_attn_out2 = None  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        #BLOCK 3\n",
        "        # pass the output of the second block through a ffn\n",
        "        ffn_output = None  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply a dropout layer to the ffn output\n",
        "        # use `training=training`\n",
        "        ffn_output = None\n",
        "\n",
        "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
        "        out3 = None  # (batch_size, target_seq_len, embedding_dim)\n",
        "        # END CODE HERE\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "t3Vbt881iH1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "DecoderLayer_test(DecoderLayer, create_look_ahead_mask)"
      ],
      "metadata": {
        "id": "rNfzzOR6iL_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Decoder\n",
        "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embed your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers.\n",
        "\n",
        "<center>\n",
        "  <div>\n",
        "    <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihYYpVskdVvUrEZBJHhfmQwIGpnN0uou8uKARTwtH-XV0KSLoR09TA4AAZj9A7P0kG99_Pyx3zluBngmzYrJ351CBXixUg=s1600\" alt=\"Encoder\" width=\"300\"/>\n",
        "    <caption><center><font color='purple'><b>Figure 3b: Transformer Decoder</b></font></center></caption>\n",
        "  </div>\n",
        "</center>\n",
        "\n",
        "### Exercise 7 - Decoder\n",
        "\n",
        "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers.\n",
        "\n",
        "In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps:\n",
        "1. Pass your generated output through the Embedding layer.\n",
        "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
        "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode.\n",
        "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
      ],
      "metadata": {
        "id": "o2AAvacCiQAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the target input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    decoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward  pass for the Decoder\n",
        "\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            attention_weights - Dictionary of tensors containing all the attention weights\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # START CODE HERE\n",
        "        # create word embeddings\n",
        "        x = None  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # scale embeddings by multiplying by the square root of their dimension\n",
        "        x *= None\n",
        "\n",
        "        # calculate positional encodings and add to word embedding\n",
        "        x += None\n",
        "\n",
        "        # apply a dropout layer to x\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
        "            # of block 1 and 2 (~1 line)\n",
        "            x, block1, block2 = self.dec_layers[i](None, None, None,\n",
        "                                                 None, None)\n",
        "\n",
        "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
        "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = None\n",
        "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = None\n",
        "        # END CODE HERE\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, embedding_dim)\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "n3Rwfjyaiu0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "Decoder_test(Decoder, create_look_ahead_mask, create_padding_mask)"
      ],
      "metadata": {
        "id": "z-RjUgIwiybc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Transformer\n",
        "\n",
        "This is the last exercise of the assignment. Congratulations! You've done all the hard work, now it's time to put it all together.  \n",
        "\n",
        "<center>\n",
        "  <div>\n",
        "    <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihY073zPlP55IW_LIdi52N7hTJYne_Newk8-HHzpr7rvRx_U7XW0kCVwGcJyha8fHc60Kx0pWwvqSk4OrrmKZK2eVn9-8w=s1600\" alt=\"Transformer\" width=\"550\"/>\n",
        "    <caption><center><font color='purple'><b>Figure 4: Transformer</b></font></center></caption>\n",
        "  </div>\n",
        "</center>\n",
        "\n",
        "The flow of data through the Transformer Architecture is as follows:\n",
        "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
        "    - embedding and positional encoding of your input\n",
        "    - multi-head attention on your input\n",
        "    - feed forward neural network to help detect features\n",
        "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
        "    - embedding and positional encoding of the output\n",
        "    - multi-head attention on your generated output\n",
        "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
        "    - a feed forward neural network to help detect features\n",
        "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n",
        "\n",
        "### Exercise 8 - Transformer\n",
        "\n",
        "Implement `Transformer()` using the `call()` method\n",
        "1. Pass the input through the Encoder with the appropiate mask.\n",
        "2. Pass the encoder output and the target through the Decoder with the appropiate mask.\n",
        "3. Apply a linear transformation and a softmax to get a prediction."
      ],
      "metadata": {
        "id": "hnVvcIAZi2gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               input_vocab_size=input_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_input,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               target_vocab_size=target_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_target,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_sentence -- Tensor of shape (batch_size, input_seq_len)\n",
        "                              An array of the indexes of the words in the input sentence\n",
        "            output_sentence -- Tensor of shape (batch_size, target_seq_len)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            enc_padding_mask -- Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            dec_padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            final_output -- Describe me\n",
        "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "\n",
        "        \"\"\"\n",
        "        # START CODE HERE\n",
        "        # call self.encoder with the appropriate arguments to get the encoder output\n",
        "        enc_output = None  # (batch_size, inp_seq_len, embedding_dim)\n",
        "\n",
        "        # call self.decoder with the appropriate arguments to get the decoder output\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, embedding_dim)\n",
        "        dec_output, attention_weights = self.decoder(None, None, None, None, None)\n",
        "\n",
        "        # pass decoder output through a linear layer and softmax (~2 lines)\n",
        "        final_output = None # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        # END CODE HERE\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "9nSKvKcMjYsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST\n",
        "Transformer_test(Transformer, create_look_ahead_mask, create_padding_mask)"
      ],
      "metadata": {
        "id": "cNMeSTZ5jcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "When you've completed all the exercises and can pass all the unit tests, please upload the assignment (make sure you put your name at the top) to Canvas.\n",
        "\n",
        "Thank you!"
      ],
      "metadata": {
        "id": "JTaHSkYnjgsB"
      }
    }
  ]
}