{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f1c05e",
   "metadata": {},
   "source": [
    "# CS-6580 Lecture 8 - Augmenting Convolutional Neural Networks\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ddf71-4d79-410a-84b3-f5c59d881f58",
   "metadata": {},
   "source": [
    "In our last lecture, we learned the basics of convolutional neural networks. Today, we're going to talk about some techniques that you can use to augment a convolutional neural network if you're in a situation where you don't have as much data as you'd like - a very common situation in real world applications! As a practical example, we'll work on classifying images as dogs or cats in a dataset containing images of cats and dogs. We'll use 2,000 images for training (a small set), 1,000 images for validation, and about 2,000 images for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86043c4-aa30-4dd2-a083-98f5e0471df1",
   "metadata": {},
   "source": [
    "To get this data, you'll want to upload the cats_vs_dogs.zip file from Canvas into your working directory, and then (silently!) unzip it with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818da33a-9fca-4052-832f-fcf7fbad18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open cats_vs_dogs.zip, cats_vs_dogs.zip.zip or cats_vs_dogs.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# !unzip -qq cats_vs_dogs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6cf3e-c29c-469e-ac63-ccc46b1ec949",
   "metadata": {},
   "source": [
    "This should create a folder \"cats_vs_dogs\" with subfolders of training, validation, and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69a7de-2bd1-4f26-a026-4d00d4a43626",
   "metadata": {},
   "source": [
    "And, as usual, we'll want to grab our standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680270c1-c5ab-4ce4-8586-18952f84fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a887d-e314-4002-a487-b1adb6b6ea4c",
   "metadata": {},
   "source": [
    "### Building a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efa586-d481-4b74-84c4-00efd70fa10c",
   "metadata": {},
   "source": [
    "First, we'll try to train a model from scratch using the little data we have - because you've got to start somewhere. We'll train a small convolutional neural network (covnet) to set the baseline for what can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8540b4c1-26b4-4574-94aa-f6a14d6983ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b366da71-a192-43a6-a0bc-cc55d56457e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 178, 178, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 89, 89, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 43, 43, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 41, 41, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 20, 20, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 9, 9, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 12545     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 991041 (3.78 MB)\n",
      "Trainable params: 991041 (3.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6268569f-4c65-4e91-8d7e-97812b5797dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a814b33-c112-44ef-ad8c-ef51f03546de",
   "metadata": {},
   "source": [
    "We'll need to read the pictures in and transform them into appropriately preprocessed floating-point tensors before being fed into the model. Currently, we've got the data as JPEG files. So, the steps to getting them into the model are approximately:\n",
    "\n",
    "* Read the picture files.\n",
    "* Decode the JPEG content into RGB grids of pixels.\n",
    "* Convert these into floating-point tensors.\n",
    "* Resize them to a shared size (we'll use 180 x 180)\n",
    "* Pack them into batches (we'll use batches of 32 images)\n",
    "\n",
    "That might seem like a lot, but fortunately Keras has utilities to take care of these steps automatically. Specifically, it has a utility function *image_dataset_from_directory()*, which lets you quickly set up a data pipeline that can automatically turn image files on disk into batches of preprocessed tensors. This is what we'll use here.\n",
    "\n",
    "Calling *image_dataset_from_directory(directory)* will first list the subdirectories of *directory* and assume each one contains images from one of our classes. It will then index the image files in each subdirectory. Finally, it will create and return a *tf.data.Datas.t* object configured to read these files, shuffle them, decode them to tensors, resize them to a shared size, and pack them into batches. Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca75bda0-d92c-4abb-8c28-e393f2afcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1f404e-085b-4229-bf46-251430f6044d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory cats_vs_dogs/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcats_vs_dogs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m      9\u001b[0m     base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m180\u001b[39m),\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     13\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m     14\u001b[0m     base_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m180\u001b[39m),\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/image_dataset.py:213\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory cats_vs_dogs/train"
     ]
    }
   ],
   "source": [
    "base_dir = pathlib.Path(\"cats_vs_dogs\")\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    base_dir / \"train\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    base_dir / \"validation\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    base_dir / \"test\",\n",
    "    labels = 'inferred',\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d86350-372f-406e-841d-97d321d04c65",
   "metadata": {},
   "source": [
    "Let's look at the output of one of these Dataset objects: it yields batches of $180 \\times 180$ RGB images and integer labels. There are 32 samples in each batch (the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fec50a-ccac-4b48-a366-76e9cb35411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8e540-ef1f-4c0b-b4c9-334e23afbad2",
   "metadata": {},
   "source": [
    "Alright, let's fit the model on our dataset.\n",
    "\n",
    "We'll use a ModelCheckpoint callback to save the model after each epoch. We'll configure it with the path specifying where to save the file, as well as the arguments *save_best_only=True* and *monitor=\"val_loss\"*: they tell the callback to only save a new file (overwriting any previous one) when the current value of the *val_loss* metric is lower than at any previous time during training. This way, if we start overfitting, we don't need to start training our model over again from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c013d-8a10-48ef-98c3-bfde24e64b88",
   "metadata": {},
   "source": [
    "Alright, let's do some training! This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e18616-f75a-485d-a0b7-15f525eaaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd958e-9008-4e1d-a467-81f25966929e",
   "metadata": {},
   "source": [
    "We can plot the loss and accuracy of the model over the training and validation data for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8bbf2-fe99-4116-806e-123c7c5145ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123540a3-fb2d-493e-977e-fa74e8b2c377",
   "metadata": {},
   "source": [
    "It looks like we start overfitting pretty soon - after about 9 epochs. Which isn't surprising given our small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11f547-64a5-4afd-b6d0-e53ad116ba74",
   "metadata": {},
   "source": [
    "Now let's reload our best model and try it out on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be042739-e583-4c7f-b617-586b4f72a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc862782-44fa-4f18-844b-a3f29ea481fb",
   "metadata": {},
   "source": [
    "That's OK. Not great, but OK. Given how few training samples we have, overfitting will be our number one concern. How can we avoid it? Well, get more data!\n",
    "\n",
    "However, this isn't always easy, so we'll introduce another method, specific to computer vision - data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965039e-bf2d-4ea3-b579-adfddb9644fc",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5898f16-b9d7-4c51-94f7-67404268e38a",
   "metadata": {},
   "source": [
    "Data augmentation generates more training data from existing training samples by *augmenting* the samples via a number of random transformations that yield believable-looking images.\n",
    "\n",
    "In Keras, this can be done by adding a number of *data augmentation layers* at the start of the model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391081d-8fb5-4529-a591-f00e3e5e732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16a2bc-86e2-4a69-8129-84c0391c00bf",
   "metadata": {},
   "source": [
    "These are just a few of the layers available (for more, see the [Keras documentation](https://www.tensorflow.org/tutorials/images/data_augmentation)). What our transformations do is:\n",
    "\n",
    "* Applies horizontal flipping to a random 50% of the images.\n",
    "* Rotates the input images by a random value in the range of [-10%,+10%]\n",
    "* Zooms in or out of the image by a random factor in the range [-20%,+20%]\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8a62e-fa87-44e2-b6c2-05209a7d5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820cd0e-a87a-4250-8cb3-d04f5d88a696",
   "metadata": {},
   "source": [
    "If we train our model using the data-augmentation configuration, the model will never see the same input twice. But, the inputs it sees are still heavily intercorrelated because they come from a small number of original images. As such, this technique won't completely get rid of overfitting.\n",
    "\n",
    "Note the random image augmentation layers are inactive during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01033281-3429-4cff-889c-ac00894c0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f149c-b289-4ac6-9430-1ee6745b1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=60,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba1e94-9b68-4612-9bcc-c37127a038ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d1d41-86c4-415f-9a4e-e93ad09b5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e7546-ef39-42e7-ac17-d79607085a1e",
   "metadata": {},
   "source": [
    "### Levaging a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a560f-f233-4c95-9827-51040d4311c1",
   "metadata": {},
   "source": [
    "A common and highly effective approach to deep learning on small image datasets is to use a pretrained model. A *pretrained model* is a model that was previously trained on a large dataset, typically on a large-scale image classification task. If this original dataset is large enough and general enough, the hierarchy of features learned by the pretrained model can effectively act as a generic model of the visual world, and prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task.\n",
    "\n",
    "For instance, you might train a model on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained model for something like identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning.\n",
    "\n",
    "For our case, we'll use a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and  dogs, and you can thus expect it to perform well on the dogs-versus-cats classification problem.\n",
    "\n",
    "There are two ways to use a pretrained model: *feature extraction* and *fine tuning*. We'll start with feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b371f45-e13f-4e58-b8bf-75e82dee59aa",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea10b54-9b8a-4df8-b1ef-27de5c8fc60e",
   "metadata": {},
   "source": [
    "Feature extraction consists of using the representations learned by a previously trained model to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\n",
    "\n",
    "Most convnets are made of two parts: first, a series of convolution and pooling layers, and then a densely connected classifier. The first part is called the *convolutional base* of the model. Feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output.\n",
    "\n",
    "Why only reuse the convolutional base? Could we reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and, therefore, more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which are likely to be useful regardless of the computer vision problem at hand. But the representations learned by the classifier will necessarily be specific to the set of classes on which the model was trained—they will only contain information about the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about where objects are located in the input image; these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless. \n",
    "\n",
    "Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”). So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base. \n",
    "\n",
    "In this case, because the ImageNet class set contains multiple dog and cat classes, it’s likely to be beneficial to reuse the information contained in the densely connected layers of the original model. But we’ll choose not to, in order to cover the more general case where the class set of the new problem doesn’t overlap the class set of the original model. Let’s put this into practice by using the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from cat and dog images, and then train a dogs-versus-cats classifier on top of these features. The VGG16 model, among others, comes prepackaged with Keras. You can import it from the keras.applications module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6af559-b77b-4111-8f47-cb3d620db8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223f6f3-1a59-457e-8260-c138e95c41ed",
   "metadata": {},
   "source": [
    "We pass three arguments to the constructor: \n",
    "\n",
    "* weights specifies the weight checkpoint from which to initialize the model. \n",
    "* *include_top* refers to including (or not) the densely connected classifier on top of the network. By default, this densely connected classifier corresponds to the 1,000 classes from ImageNet. Because we intend to use our own densely connected classifier (with only two classes: cat and dog), we don’t need to include it. \n",
    "* *input_shape* is the shape of the image tensors that we’ll feed to the network. This argument is purely optional: if we don’t pass it, the network will be able to process inputs of any size. Here we pass it so that we can visualize (in the following summary) how the size of the feature maps shrinks with each new convolution and pooling layer. \n",
    "\n",
    "Here’s the detail of the architecture of the VGG16 convolutional base. It’s similar to the simple convnets you’re already familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79b6af-fb2b-4004-b11d-2561020a2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04d704-ebc0-4175-9888-c5ed45e8bdf0",
   "metadata": {},
   "source": [
    "The final feature map has shape (5, 5, 512). That’s the feature map on top of which we’ll stick a densely connected classifier. \n",
    "\n",
    "At this point, there are two ways we could proceed: \n",
    "\n",
    "* Run the convolutional base over our dataset, record its output to a NumPy array on disk, and then use this data as input to a standalone, densely connected classifier. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, this technique won’t allow us to use data augmentation. \n",
    "\n",
    "* Extend the model we have (conv_base) by adding Dense layers on top, and run the whole thing from end to end on the input data. This will allow us to use data augmentation, because every input image goes through the convolutional base every time it’s seen by the model. But for the same reason, this technique is far more expensive than the first.\n",
    "\n",
    "Well do the first - not because it's better, but just because it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ddc83-c7af-4648-ac10-ca8de64387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2ca61-b4cf-4601-9d72-63b218740db3",
   "metadata": {},
   "source": [
    "Importantly, *predict()* only expects images, not labels, but our current dataset yields batches that contain both images and their labels. Moreover, the VGG16 model expects inputs that are preprocessed with the function keras.applications.vgg16.preprocess_input, which scales pixel values to an appropriate range. \n",
    "\n",
    "The extracted features are currently of shape (samples, 5, 5, 512):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da77dd-a07e-4668-8334-7868d13ffeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2373ef-b12c-40b6-ad19-58f1717dbb2f",
   "metadata": {},
   "source": [
    "At this point, we can define our densely connected classifier and train it on the data and labels that we just recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2f33b-bdbf-44fd-8adf-645e58ec5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708826-0d0d-4d0b-8c57-474767525d04",
   "metadata": {},
   "source": [
    "Training is very fast because we only have to deal with two Dense layers—an epoch takes less than one second even on CPU. \n",
    "\n",
    "Let’s look at the loss and accuracy curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb31cb-50f0-47b2-b0f8-d12f5f3357e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81813ac2-ed82-4fd9-ac6a-ec4e6d139b81",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of about 97%—much better than we achieved in the previous section with the small model trained from scratch. This is a bit of an unfair comparison, however, because ImageNet contains many dog and cat instances, which means that our pretrained model already has the exact knowledge required for the task at hand. This won’t always be the case when you use pretrained features. \n",
    "\n",
    "However, the plots also indicate that we’re overfitting almost from the start— despite using dropout with a fairly large rate. That’s because this technique doesn’t use data augmentation, which is essential for preventing overfitting with small image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afecdb3-843b-46b8-b1a2-453b6a48243f",
   "metadata": {},
   "source": [
    "#### Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13475d-bd1d-44b9-b19b-0c9c0190bcd1",
   "metadata": {},
   "source": [
    "Another widely used technique for model reuse, complementary to feature extraction, is fine-tuning. Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called fine-tuning because it slightly adjusts the more abstract representations of the model being reused in order to make them more relevant for the problem at hand. \n",
    "\n",
    "I stated earlier that it’s necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. For the same reason, it’s only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn’t already trained, the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed. Thus the steps for fine-tuning a network are as follows: \n",
    "\n",
    "1. Add our custom network on top of an already-trained base network. \n",
    "2. Freeze the base network. \n",
    "3. Train the part we added. \n",
    "4. Unfreeze some layers in the base network. (Note that you should not unfreeze “batch normalization” layers, which are not relevant here since there are no such layers in VGG16. Batch normalization and its impact on finetuning is explained in the next chapter.) \n",
    "5. Jointly train both these layers and the part we added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a4e7f-196f-44a3-b531-064458a7d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "# x = data_augmentation(inputs)\n",
    "x = keras.applications.vgg16.preprocess_input(inputs)\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ad8c-2f65-4e5a-a1d6-5e2976601012",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base  = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ddc30-be1e-4425-b851-f3c08accb9d8",
   "metadata": {},
   "source": [
    "We’ll fine-tune the last three convolutional layers, which means all layers up to *block4_ pool* should be frozen, and the layers *block5_conv1*, *block5_conv2*, and *block5_conv3* should be trainable.\n",
    "\n",
    "Why not fine-tune more layers? Why not fine-tune the entire convolutional base? You could. But you need to consider the following: \n",
    "\n",
    "* Earlier layers in the convolutional base encode more generic, reusable features, whereas layers higher up encode more specialized features. It’s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers. \n",
    "* The more parameters you’re training, the more you’re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset. \n",
    "\n",
    "Thus, in this situation, it’s a good strategy to fine-tune only the top two or three layers in the convolutional base. Let’s set this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464255a-0127-46d4-ae27-30cc134cb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb09481-f138-4ef9-a2b5-818cd5e18dc5",
   "metadata": {},
   "source": [
    "Now we can begin fine-tuning the model. We’ll do this with the RMSprop optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the three layers we’re fine-tuning. Updates that are too large may harm these representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e318515-a9f3-4f79-9198-54606af9f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527fee3c-4c97-4a4d-81b2-bcd7450b69d1",
   "metadata": {},
   "source": [
    "We can finally evaluate this model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762d582-1df6-4b82-b747-ce426063da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4efb9-b50e-4458-a2a0-91ada2bf4459",
   "metadata": {},
   "source": [
    "Alright!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
