{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-6580 Lecture 12 - Architecture Patterns\n",
        "**Dylan Zwick**\n",
        "\n",
        "*Weber State University*"
      ],
      "metadata": {
        "id": "HajuXpKX7iGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we're going to dive into some architectures that are frequently utilized in the building of neural networks - particularly but not exclusively convolutional neural networks.\n",
        "\n",
        "When you're designing a model, what you're really doing is designing its *hypothesis space* - the space of possible functions over which gradient descent can search, parametrized by the model's weights. Like feature engineering, a good hypotheses space encodes *prior knowledge* about the problem you're trying to solve. For example, using convolutional layers means that you expect the relevant patterns to be translation invariant."
      ],
      "metadata": {
        "id": "4pGD4NTC_PsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good model architecture is one that reduces the size of the search space while not significantly limiting effective solutions. It's about making the problem simpler for gradient descent to solve - which is a pretty simple process that needs all the help it can get."
      ],
      "metadata": {
        "id": "Su_eufv3AED9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today, we'll review a few essential architecture best practices - *residual connections, batch normalization*, and *separable convolutions*. We'll then apply them to the cats vs. dogs problem we investigated a couple weeks ago."
      ],
      "metadata": {
        "id": "5diha15rAi3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important general idea when architecting, well, really anything, is that you want what you do to be *modular*, *hierarchical*, and *reusable*. Being modular means it's broken down into the fundamental classes of problem. Being hierarchical means that patterns are applied at multiple levels of abstraction and complexity. Being reusable means that when the same problems are encountered in different contexts the same solutions are applied."
      ],
      "metadata": {
        "id": "1T1Tg-n0Ay1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning model architecture is primarily about maknig clever use of modularity, hierarchy, and reuse. Neural network architectures are structured into repeated groups of layers (usually called \"blocks\"). These layers are then structures into pyramid-like hierarchies - the number of filters grows with layer depth, while the size of the feature maps shrink."
      ],
      "metadata": {
        "id": "r4GIPY-EB-tX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally speaking, a deep stack of narrow layers performs better than a shallow stack of large layers. However, there's a limit to how deep you can stack layers, due to the problem of *vanishing gradients*. One way to mitigate this problem is with *residual connections*."
      ],
      "metadata": {
        "id": "BxGrJZPACfkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connecitons"
      ],
      "metadata": {
        "id": "hSvSb30JCuQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation in a sequential deep learning model is kind of similar to a game of \"Telephone\", in that every time information is transmitted noise is introduced into the signal.\n",
        "\n",
        "If you have a chain of functions,\n",
        "\n",
        "<center>\n",
        "  $\\displaystyle f_{4}(f_{3}(f_{2}(f_{1}(x))))$,\n",
        "</center>\n",
        "\n",
        "the goal is to adjust the parameters of each based on the error recorded on the output $f_{4}$. To adjunt $f_{1}$, you need to percolate error information through $f_{2}$, $f_{3}$, and $f_{4}$. However, each time you do this you introduce some noise. If your function chain is too deep, this noise can overwhelm the signal, and backpropagation stops working.\n",
        "\n",
        "Residual connections attempt to ameliorate this problem. The approach is simple. Just att the input of a layer back to its output.\n",
        "\n",
        "<center>\n",
        "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYTZ4pAWwc9a_fSGtlNhOZ-lUmMsMxTfE6oIIs3fp3ZgvndfRwMAUUgOiR8A3PIFhWRH8qrG1-LvpgfhN0HhATOvQVnz=s2560\" width=200>\n",
        "</center>\n",
        "\n",
        "Note that adding the input back implies the output has the same shape as the input. The way this is managed is through a $1 \\times 1$ convolutional layer without activation, with strides to match any downsampling."
      ],
      "metadata": {
        "id": "ocdZ7lqACxFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalizations"
      ],
      "metadata": {
        "id": "NHCU1vGyJMGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Normalization* is a broad category of methods that seek to make different samples seen by a machine learning model more similar to each other, which helps the model learn and generalize well to new data.\n",
        "\n",
        "One very common type of data normalization is centering the data on zero, and then giving the data a unit standand deviation. This is just a constant addition applied to all the data points, and then a constant multiplication. This is, in effect, making the assumption that the data follows a normal distribution, which is frequently but not always reasonable.\n",
        "\n",
        "Batch normalization applies this to layers *within* a network. Just because the data entering a neural network layer has a $0$ mean or unit variance, there's no reason to expect the output will. Well, batch normalization insures it does.\n",
        "\n",
        "Nobody really understands exactly how batch normalization helps. There are theories, but no certainty. However, there's no debate that for certain types of problems it does.\n",
        "\n",
        "One thing to note about batch normalization - if you're using it, you *don't* need or want a bias term."
      ],
      "metadata": {
        "id": "MOURumu_J2AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depthwise Separable Convolutions"
      ],
      "metadata": {
        "id": "CUyDm6HFLvs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depthwise separable convolutions are kind of amazing. Not amazing in so far as how they work. That's kind of cool, but nothing profound. What's amazing about them is how well they tend to work. It's essentially a drop-in replacement for *Conv2D* that will make your model smaller, and cause it to perform a few percentage points *better* on many tasks. How cool is that?\n",
        "\n",
        "The depthwise separable convolution is so named because it deals with the depth dimension â€” the number of channels. Specifically, it separates the convolution into 2 parts: a depthwise convolution and a pointwise convolution.\n",
        "\n",
        "For the depthwise convolution, it applies a convolution to each layer of the image *independently*. It then does a pointwise $1 \\times 1$ convolution. This is equivalent to separating the learning of spatial features and the learning of channel-wise features. This is basically assuming the different channels are highly independent.\n",
        "\n",
        "Depthwise separable convolution requires significantly fewer parameters and involves fewer computations compared to regular convolutions."
      ],
      "metadata": {
        "id": "X8StgwgPLy7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats vs. Dogs II"
      ],
      "metadata": {
        "id": "pt4dupXiMamX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the comment below and run the command. If you do need to run it, you should only need to run it once.\n",
        "# !pip install gdown"
      ],
      "metadata": {
        "id": "j31REkUWPzUh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Usual Suspects\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Our deepl learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "#Some OS Libraries\n",
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "#Some libraries for downloading and unzipping files\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "#For creating a dataset from image libraries\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "#For augmenting our image data\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom"
      ],
      "metadata": {
        "id": "NU84KWOBOZLU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1m8tc0BAcDy6J9KNkWH1MNKzbBaD5Y32S'\n",
        "output = 'cats_vs_dogs.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1zhrLU8MPhWi",
        "outputId": "52f21e3d-f4db-4e10-ab3d-25692a69b6e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1m8tc0BAcDy6J9KNkWH1MNKzbBaD5Y32S\n",
            "From (redirected): https://drive.google.com/uc?id=1m8tc0BAcDy6J9KNkWH1MNKzbBaD5Y32S&confirm=t&uuid=063189fd-1d29-493b-ba1a-0818242729b7\n",
            "To: /content/cats_vs_dogs.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228M/228M [00:03<00:00, 57.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cats_vs_dogs.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq cats_vs_dogs.zip"
      ],
      "metadata": {
        "id": "2xobPTbfOdSE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_base_dir = pathlib.Path(\"cats_vs_dogs\")\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsOUCrkmSJUO",
        "outputId": "38cdd1e1-f662-4afd-a995-c52248373d6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "CTuBCg5DRI84"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
        "\n",
        "for size in [32, 64, 128, 256, 512]:\n",
        "    residual = x\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    residual = layers.Conv2D(\n",
        "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "ekBTavrmR0-9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"new_convnet_model.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmpm63pSR6_m",
        "outputId": "b301aca1-926b-4444-f81c-4b350d3df66e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 15s 159ms/step - loss: 0.7119 - accuracy: 0.5540 - val_loss: 0.6984 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.6594 - accuracy: 0.5965 - val_loss: 0.7027 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.6437 - accuracy: 0.6205 - val_loss: 0.6944 - val_accuracy: 0.4990\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 10s 145ms/step - loss: 0.6212 - accuracy: 0.6525 - val_loss: 0.7160 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 10s 147ms/step - loss: 0.6086 - accuracy: 0.6810 - val_loss: 0.7585 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 9s 145ms/step - loss: 0.6058 - accuracy: 0.6780 - val_loss: 0.7476 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 10s 148ms/step - loss: 0.5784 - accuracy: 0.7080 - val_loss: 0.7576 - val_accuracy: 0.5340\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 10s 162ms/step - loss: 0.5738 - accuracy: 0.7115 - val_loss: 1.1109 - val_accuracy: 0.5030\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 9s 144ms/step - loss: 0.5504 - accuracy: 0.7195 - val_loss: 0.7663 - val_accuracy: 0.5260\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 10s 144ms/step - loss: 0.5376 - accuracy: 0.7390 - val_loss: 0.7510 - val_accuracy: 0.5720\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 0.5190 - accuracy: 0.7485 - val_loss: 0.8118 - val_accuracy: 0.5470\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 10s 149ms/step - loss: 0.4987 - accuracy: 0.7715 - val_loss: 0.6268 - val_accuracy: 0.6500\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 10s 150ms/step - loss: 0.4959 - accuracy: 0.7690 - val_loss: 0.6318 - val_accuracy: 0.6630\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 10s 148ms/step - loss: 0.4738 - accuracy: 0.7735 - val_loss: 0.7951 - val_accuracy: 0.6100\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 10s 145ms/step - loss: 0.4724 - accuracy: 0.7795 - val_loss: 0.5278 - val_accuracy: 0.7310\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 9s 147ms/step - loss: 0.4476 - accuracy: 0.7940 - val_loss: 0.8151 - val_accuracy: 0.6180\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 0.4386 - accuracy: 0.7975 - val_loss: 0.5761 - val_accuracy: 0.7120\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 10s 147ms/step - loss: 0.4313 - accuracy: 0.7945 - val_loss: 0.6115 - val_accuracy: 0.6860\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 11s 163ms/step - loss: 0.4049 - accuracy: 0.8220 - val_loss: 0.8917 - val_accuracy: 0.5910\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 10s 145ms/step - loss: 0.4083 - accuracy: 0.8095 - val_loss: 0.8167 - val_accuracy: 0.6740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPzMvCdTSl3",
        "outputId": "198bab5d-212b-48f6-a189-ae2e61b9402d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 180, 180, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 180, 180, 3)          0         ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " rescaling_2 (Rescaling)     (None, 180, 180, 3)          0         ['sequential[2][0]']          \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 176, 176, 32)         2400      ['rescaling_2[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 176, 176, 32)         128       ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 176, 176, 32)         0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_20 (Separ  (None, 176, 176, 32)         1312      ['activation_20[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 176, 176, 32)         128       ['separable_conv2d_20[0][0]'] \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 176, 176, 32)         0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_21 (Separ  (None, 176, 176, 32)         1312      ['activation_21[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooli  (None, 88, 88, 32)           0         ['separable_conv2d_21[0][0]'] \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 88, 88, 32)           1024      ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 88, 88, 32)           0         ['max_pooling2d_10[0][0]',    \n",
            "                                                                     'conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 88, 88, 32)           128       ['add_10[0][0]']              \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 88, 88, 32)           0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_22 (Separ  (None, 88, 88, 64)           2336      ['activation_22[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 88, 88, 64)           256       ['separable_conv2d_22[0][0]'] \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 88, 88, 64)           0         ['batch_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_23 (Separ  (None, 88, 88, 64)           4672      ['activation_23[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooli  (None, 44, 44, 64)           0         ['separable_conv2d_23[0][0]'] \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 44, 44, 64)           2048      ['add_10[0][0]']              \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 44, 44, 64)           0         ['max_pooling2d_11[0][0]',    \n",
            "                                                                     'conv2d_14[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 44, 44, 64)           256       ['add_11[0][0]']              \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 44, 44, 64)           0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_24 (Separ  (None, 44, 44, 128)          8768      ['activation_24[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 44, 44, 128)          512       ['separable_conv2d_24[0][0]'] \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 44, 44, 128)          0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_25 (Separ  (None, 44, 44, 128)          17536     ['activation_25[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooli  (None, 22, 22, 128)          0         ['separable_conv2d_25[0][0]'] \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 22, 22, 128)          8192      ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 22, 22, 128)          0         ['max_pooling2d_12[0][0]',    \n",
            "                                                                     'conv2d_15[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 22, 22, 128)          512       ['add_12[0][0]']              \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 22, 22, 128)          0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_26 (Separ  (None, 22, 22, 256)          33920     ['activation_26[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 22, 22, 256)          1024      ['separable_conv2d_26[0][0]'] \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 22, 22, 256)          0         ['batch_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_27 (Separ  (None, 22, 22, 256)          67840     ['activation_27[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " max_pooling2d_13 (MaxPooli  (None, 11, 11, 256)          0         ['separable_conv2d_27[0][0]'] \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 11, 11, 256)          32768     ['add_12[0][0]']              \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 11, 11, 256)          0         ['max_pooling2d_13[0][0]',    \n",
            "                                                                     'conv2d_16[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 11, 11, 256)          1024      ['add_13[0][0]']              \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 11, 11, 256)          0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_28 (Separ  (None, 11, 11, 512)          133376    ['activation_28[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 11, 11, 512)          2048      ['separable_conv2d_28[0][0]'] \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 11, 11, 512)          0         ['batch_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " separable_conv2d_29 (Separ  (None, 11, 11, 512)          266752    ['activation_29[0][0]']       \n",
            " ableConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " max_pooling2d_14 (MaxPooli  (None, 6, 6, 512)            0         ['separable_conv2d_29[0][0]'] \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 6, 6, 512)            131072    ['add_13[0][0]']              \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 6, 6, 512)            0         ['max_pooling2d_14[0][0]',    \n",
            "                                                                     'conv2d_17[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 512)                  0         ['add_14[0][0]']              \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 512)                  0         ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1)                    513       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 721857 (2.75 MB)\n",
            "Trainable params: 718849 (2.74 MB)\n",
            "Non-trainable params: 3008 (11.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"new_convnet_model.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZ9mnmwXlTZ",
        "outputId": "59661d89-e8c5-4d46-88ab-37ac73f402a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 3s 40ms/step - loss: 0.5059 - accuracy: 0.7435\n",
            "Test accuracy: 0.743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "WiVZGoW_TUNO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"old_convnet_model.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8d9X9gxTfak",
        "outputId": "dab9c03d-2c7f-4f91-85c6-f862adfa5cb1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 9s 100ms/step - loss: 0.7132 - accuracy: 0.5245 - val_loss: 0.6905 - val_accuracy: 0.5120\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 4s 64ms/step - loss: 0.6941 - accuracy: 0.5380 - val_loss: 0.6799 - val_accuracy: 0.6550\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 4s 63ms/step - loss: 0.7123 - accuracy: 0.5825 - val_loss: 0.7164 - val_accuracy: 0.5230\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 5s 82ms/step - loss: 0.6801 - accuracy: 0.6025 - val_loss: 0.6547 - val_accuracy: 0.5920\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 4s 64ms/step - loss: 0.6224 - accuracy: 0.6485 - val_loss: 0.5965 - val_accuracy: 0.6580\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 5s 76ms/step - loss: 0.5984 - accuracy: 0.6735 - val_loss: 0.6161 - val_accuracy: 0.6690\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 4s 64ms/step - loss: 0.5607 - accuracy: 0.7145 - val_loss: 0.5580 - val_accuracy: 0.7040\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 4s 67ms/step - loss: 0.5346 - accuracy: 0.7220 - val_loss: 0.5500 - val_accuracy: 0.7220\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 5s 78ms/step - loss: 0.4920 - accuracy: 0.7675 - val_loss: 0.5577 - val_accuracy: 0.7120\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 4s 63ms/step - loss: 0.4462 - accuracy: 0.7985 - val_loss: 0.5794 - val_accuracy: 0.7160\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 5s 68ms/step - loss: 0.4187 - accuracy: 0.8065 - val_loss: 0.5047 - val_accuracy: 0.7530\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 5s 72ms/step - loss: 0.3503 - accuracy: 0.8510 - val_loss: 0.8699 - val_accuracy: 0.7110\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 4s 62ms/step - loss: 0.3171 - accuracy: 0.8735 - val_loss: 0.6094 - val_accuracy: 0.7200\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 6s 85ms/step - loss: 0.2508 - accuracy: 0.8970 - val_loss: 0.7294 - val_accuracy: 0.6880\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 4s 64ms/step - loss: 0.1927 - accuracy: 0.9240 - val_loss: 0.7428 - val_accuracy: 0.7490\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 6s 86ms/step - loss: 0.1528 - accuracy: 0.9390 - val_loss: 0.8018 - val_accuracy: 0.7590\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 4s 62ms/step - loss: 0.1121 - accuracy: 0.9620 - val_loss: 0.8194 - val_accuracy: 0.7840\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 4s 64ms/step - loss: 0.1057 - accuracy: 0.9660 - val_loss: 1.2426 - val_accuracy: 0.7180\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 5s 81ms/step - loss: 0.0952 - accuracy: 0.9630 - val_loss: 1.2233 - val_accuracy: 0.7290\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 6s 87ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 0.9666 - val_accuracy: 0.7710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ib0bUhqLTlu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d96ae2a-be3e-46a0-e5c7-4299d45ed1c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " rescaling_3 (Rescaling)     (None, 180, 180, 3)       0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 178, 178, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 89, 89, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 87, 87, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 43, 43, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 41, 41, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 20, 20, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 18, 18, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 9, 9, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 12545     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 991041 (3.78 MB)\n",
            "Trainable params: 991041 (3.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"old_convnet_model.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "XEuRUhPIXjFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef1c3f0-081d-47e1-d5a0-5ea086cb569e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 3s 36ms/step - loss: 0.5341 - accuracy: 0.7445\n",
            "Test accuracy: 0.744\n"
          ]
        }
      ]
    }
  ]
}