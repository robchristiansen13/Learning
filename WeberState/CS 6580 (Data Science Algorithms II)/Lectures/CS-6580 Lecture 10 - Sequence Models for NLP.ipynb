{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbf7d4a5-6f8f-4add-a4a2-6a5d9774e524",
      "metadata": {
        "id": "dbf7d4a5-6f8f-4add-a4a2-6a5d9774e524"
      },
      "source": [
        "# CS-6580 Lecture 10 - Sequence Models for NLP\n",
        "**Dylan Zwick**\n",
        "\n",
        "*Weber State University*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e9ce17-84c9-4b0e-8e31-a98f0b90b0f3",
      "metadata": {
        "id": "f0e9ce17-84c9-4b0e-8e31-a98f0b90b0f3"
      },
      "source": [
        "In our last lecture, we reviewed the basics of NLP, and looked at how we can build a basic sentiment classification model for predicting the sentiment of a collection of IMDB movie reviews.\n",
        "\n",
        "Today, we'll continue to explore this problem by using a more sophisticated model - a sequence model - for the same predictions.\n",
        "\n",
        "First, we'll import our favorite libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "faa01c2b-031c-408c-8320-490c9cb46023",
      "metadata": {
        "tags": [],
        "id": "faa01c2b-031c-408c-8320-490c9cb46023"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, if you don't already have it, you'll want to import and structure the review data. Please note that you only need to do this once."
      ],
      "metadata": {
        "id": "oXWpdh93MyqW"
      },
      "id": "oXWpdh93MyqW"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c3a918-efda-49f0-fae6-ed326a43b9e3",
        "id": "x5-7RxrZNIKh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  38.3M      0  0:00:02  0:00:02 --:--:-- 38.3M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz;"
      ],
      "id": "x5-7RxrZNIKh"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "oX6xox7PNIKt"
      },
      "outputs": [],
      "source": [
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "id": "oX6xox7PNIKt"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "243dYDbuNIKt"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "id": "243dYDbuNIKt"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "Q4QDWTtiNIKt"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "id": "Q4QDWTtiNIKt"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3189a2-54eb-4f45-fa56-d2f25d613c5c",
        "id": "NWZTZyFtNIKt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "id": "NWZTZyFtNIKt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Bidirectional LSTM"
      ],
      "metadata": {
        "id": "SGig5wSFKbRL"
      },
      "id": "SGig5wSFKbRL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into sequence models for NLP, I want to mention bidirectional RNNs, because those have proven to be the most successful RNNs for NLP.\n",
        "\n",
        "A bidirectional RNN uses two regular RNNs, and processes the input sequence going both ways. For problems like temperature forecasting this might not be very valuable, but for things like NLP in which order matters but the direction of the order isn't quite as important this combined approach can be valuable.\n",
        "\n",
        "There is a *Bidirectional* layer in Keras, which takes as its first argument a recurrent layer instance, and creates a second, separate instance of this recurrent layer going the opposite direction. It then trains on both."
      ],
      "metadata": {
        "id": "JlsOD1nzKh2q"
      },
      "id": "JlsOD1nzKh2q"
    },
    {
      "cell_type": "markdown",
      "id": "bd89fe27-8e8b-42d7-95bd-82e44de38bf8",
      "metadata": {
        "id": "bd89fe27-8e8b-42d7-95bd-82e44de38bf8"
      },
      "source": [
        "## The Sequence Model Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce6de5d-52c5-4d03-ae63-18412b3634ec",
      "metadata": {
        "id": "2ce6de5d-52c5-4d03-ae63-18412b3634ec"
      },
      "source": [
        "Last time, we saw that we were able to improve our model by using a bigram bag of words. A bigram tokenizes the text using both one and two word tokens, which means that it attempts to incorporate a small bit of order structure. So, it appears that word order can actually be important when understanding text. Surprise!\n",
        "\n",
        "\n",
        "Let's vectorize our text data into integers, but limit ourselves to just 20,000 word dimensions and just the first 600 words of any text observation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds) #Construct our map from words to integers\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y))"
      ],
      "metadata": {
        "id": "5kaZfzbdOVIt"
      },
      "id": "5kaZfzbdOVIt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to one-hot encode the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple bidirectional LSTM."
      ],
      "metadata": {
        "id": "z02owjyWOd_X"
      },
      "id": "z02owjyWOd_X"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmFUD_QqPEs-",
        "outputId": "32516ba3-b6e5-46e5-a3d1-59d23d62d31f"
      },
      "id": "UmFUD_QqPEs-",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                5128448   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5128513 (19.56 MB)\n",
            "Trainable params: 5128513 (19.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s train our model. **Warning** - This model can take a long time to train. In fact, if you're using just a CPU, it can take a *very* long time to train."
      ],
      "metadata": {
        "id": "3PXUwD7_PLKl"
      },
      "id": "3PXUwD7_PLKl"
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKWANIwFPRT0",
        "outputId": "ee50e533-ad6c-41d0-b74a-e176328f24ab"
      },
      "id": "cKWANIwFPRT0",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 164s 254ms/step - loss: 0.5482 - accuracy: 0.7175 - val_loss: 0.5109 - val_accuracy: 0.7386\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.3471 - accuracy: 0.8674 - val_loss: 0.3067 - val_accuracy: 0.8822\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 162s 260ms/step - loss: 0.2701 - accuracy: 0.8989 - val_loss: 0.2863 - val_accuracy: 0.8878\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.2307 - accuracy: 0.9190 - val_loss: 0.3064 - val_accuracy: 0.8818\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.1944 - accuracy: 0.9337 - val_loss: 0.2856 - val_accuracy: 0.8886\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.1721 - accuracy: 0.9417 - val_loss: 0.2954 - val_accuracy: 0.8796\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.1427 - accuracy: 0.9535 - val_loss: 0.3734 - val_accuracy: 0.8500\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.1257 - accuracy: 0.9585 - val_loss: 0.4447 - val_accuracy: 0.8500\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.0934 - accuracy: 0.9700 - val_loss: 0.3896 - val_accuracy: 0.8694\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.0785 - accuracy: 0.9749 - val_loss: 0.3999 - val_accuracy: 0.8714\n",
            "782/782 [==============================] - 96s 121ms/step - loss: 0.3007 - accuracy: 0.8823\n",
            "Test acc: 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One reason this trains so slowly is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000) (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do.\n",
        "\n",
        "The model also only gets to about 87% test accuracy—it doesn’t perform nearly as well as our binary unigram model! Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn’t a great idea. Is there a better way? Yes! *Word embeddings*."
      ],
      "metadata": {
        "id": "L9MsorZfPerE"
      },
      "id": "L9MsorZfPerE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings"
      ],
      "metadata": {
        "id": "ker9hqqlPsfz"
      },
      "id": "ker9hqqlPsfz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you encode something via one-hot encoding, you’re making a feature engineering decision. You’re injecting into your model a fundamental assumption about the structure of your feature space. That assumption is that the different tokens you’re encoding are all independent from each other: indeed, one-hot vectors are all orthogonal to one another. And in the case of words, that assumption is clearly wrong. Words form a structured space: they share information with each other. The words “movie” and “film” are interchangeable in most sentences, so the vector that represents “movie” should not be orthogonal to the vector that represents “film”—they should be the same vector, or close enough.\n",
        "\n",
        "To get a bit more abstract, the geometric relationship between two word vectors should reflect the semantic relationship between these words. For instance, in a reasonable word vector space, you would expect synonyms to be embedded into similar word vectors, and in general, you would expect the geometric distance (such as the cosine distance or L2 distance) between any two word vectors to relate to the “semantic distance” between the associated words. Words that mean different things should lie far away from each other, whereas related words should be closer.\n",
        "\n",
        "Word embeddings are vector representations of words that achieve exactly this: they map human language into a structured geometric space.\n",
        "\n",
        "Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (the same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors). It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.\n",
        "\n",
        "Besides being dense representations, word embeddings are also structured representations, and their structure is learned from data. Similar words get embedded in close locations, and further, specific directions in the embedding space are meaningful. To make this clearer, let’s look at a concrete example.\n",
        "\n",
        "In the figure below, four words are embedded on a 2D plane: cat, dog, wolf, and tiger. With the vector representations we chose here, some semantic relationships between these words can be encoded as geometric transformations. For instance, the same vector allows us to go from cat to tiger and from dog to wolf: this vector could be interpreted as the “from pet to wild animal” vector. Similarly, another vector lets us go from dog to cat and from wolf to tiger, which could be interpreted as a “from canine to feline” vector."
      ],
      "metadata": {
        "id": "wrEZvTY5Pvwi"
      },
      "id": "wrEZvTY5Pvwi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <div>\n",
        "        <img src=\"https://lh3.googleusercontent.com/drive-viewer/AEYmBYQFxhSaePYBxmIvvVRO0Q9Q5F4EKQANecAiQCgcibCNs0i_KcnxYxSFEADvjfQiYwUJLfHeOpsUw7aOl4Y2oNeimQPYpA=s2560\"/ width = 300>\n",
        "    </div>\n",
        "</center>"
      ],
      "metadata": {
        "id": "Hofv2CTStUmZ"
      },
      "id": "Hofv2CTStUmZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In real-world word-embedding spaces the dimensions don't usually map so obviously into categories people understand, but sometimes they get close, and some common examples of meaningful dimensions are “gender” vectors and “plural” vectors. For instance, by adding a “female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature thousands of such potentially useful vectors."
      ],
      "metadata": {
        "id": "CbMVRUgGQVGU"
      },
      "id": "CbMVRUgGQVGU"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let’s look at how to use such an embedding space in practice. There are two ways to obtain word embeddings:\n",
        "\n",
        " * Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
        "\n",
        " * Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called *pretrained word embeddings*."
      ],
      "metadata": {
        "id": "MIjPTImpup2Y"
      },
      "id": "MIjPTImpup2Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Learning Word Embeddings with the Embedding Layer"
      ],
      "metadata": {
        "id": "EqV_vMmEQmUB"
      },
      "id": "EqV_vMmEQmUB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is there some ideal word-embedding space that would perfectly map human language and could be used for any natural language processing task? Possibly, but we don't have one yet. Also, there is no such a thing as human language— there are many different languages, and they aren’t isomorphic to one another, because a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment-analysis model may look different from the perfect embedding space for an English-language legal-document classification model, because the importance of certain semantic relationships varies from task to task.\n",
        "\n",
        "It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer: the *Embedding* layer."
      ],
      "metadata": {
        "id": "pDcYU-yEQqzk"
      },
      "id": "pDcYU-yEQqzk"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "PMphcjaHQ5jK"
      },
      "id": "PMphcjaHQ5jK",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, looks up these integers in an internal dictionary, and returns the associated vectors. It’s effectively a dictionary lookup."
      ],
      "metadata": {
        "id": "A8yvBFW8Q8VT"
      },
      "id": "A8yvBFW8Q8VT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Embedding layer takes as input a tensor of integers, of shape (batch_size, sequence_length), where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality).\n",
        "\n",
        "When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model.\n",
        "\n",
        "Let’s build a model that includes an Embedding layer and benchmark it on our task."
      ],
      "metadata": {
        "id": "V3zEluKWREpi"
      },
      "id": "V3zEluKWREpi"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpwsDnSbRT46",
        "outputId": "b36953dd-72ce-4d13-e11e-b76caa5b19ff"
      },
      "id": "RpwsDnSbRT46",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 53s 79ms/step - loss: 0.4963 - accuracy: 0.7620 - val_loss: 0.6158 - val_accuracy: 0.7842\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.3348 - accuracy: 0.8728 - val_loss: 0.3148 - val_accuracy: 0.8790\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.2666 - accuracy: 0.9011 - val_loss: 0.3112 - val_accuracy: 0.8730\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.2301 - accuracy: 0.9191 - val_loss: 0.3398 - val_accuracy: 0.8650\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 0.1897 - accuracy: 0.9348 - val_loss: 0.3400 - val_accuracy: 0.8770\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 0.1620 - accuracy: 0.9451 - val_loss: 0.3671 - val_accuracy: 0.8814\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 25s 41ms/step - loss: 0.1451 - accuracy: 0.9523 - val_loss: 0.3652 - val_accuracy: 0.8804\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 26s 42ms/step - loss: 0.1307 - accuracy: 0.9575 - val_loss: 0.3588 - val_accuracy: 0.8642\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.1089 - accuracy: 0.9648 - val_loss: 0.3845 - val_accuracy: 0.8708\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.0980 - accuracy: 0.9699 - val_loss: 0.4288 - val_accuracy: 0.8700\n",
            "782/782 [==============================] - 15s 17ms/step - loss: 0.3395 - accuracy: 0.8604\n",
            "Test acc: 0.860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It trains much faster than the one-hot model (since the LSTM only has to process 256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable. However, we’re still some way off from the results of our basic bigram model. Part of the reason why is simply that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model truncates sequences after 600 words."
      ],
      "metadata": {
        "id": "La62VsGnRaSh"
      },
      "id": "La62VsGnRaSh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding and Masking"
      ],
      "metadata": {
        "id": "0oPHECRARtOD"
      },
      "id": "0oPHECRARtOD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the *output_sequence_length=max_ length* option in TextVectorization (with max_length equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches.\n",
        "\n",
        " We’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
        "\n",
        " We need some way to tell the RNN that it should skip these iterations. There’s an API for that: *masking*.\n",
        "\n",
        " The Embedding layer is capable of generating a “mask” that corresponds to its input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape (batch_size, sequence_length), where the entry mask[i, t] indicates where timestep t of sample i should be skipped or not (the timestep will be skipped if mask[i, t] is 0 or False, and processed otherwise).\n",
        "\n",
        " By default, this option isn’t active—you can turn it on by passing mask_zero=True to your Embedding layer.\n",
        "\n",
        " Let's try retraining our model with masking enabled."
      ],
      "metadata": {
        "id": "52hQVtxcRwKh"
      },
      "id": "52hQVtxcRwKh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, you will almost never have to manage masks by hand. Instead, Keras will automatically pass on the mask to every layer that is able to process it (as a piece of metadata attached to the sequence it represents). This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also be used by the loss function to skip masked steps in the output sequence.\n",
        "\n",
        " Let’s try retraining our model with masking enabled."
      ],
      "metadata": {
        "id": "ZpLK1lLHSYbk"
      },
      "id": "ZpLK1lLHSYbk"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgfdX593SeVh",
        "outputId": "17d6be8d-712b-4f50-a359-ef6fda952f30"
      },
      "id": "rgfdX593SeVh",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 63s 90ms/step - loss: 0.4255 - accuracy: 0.7994 - val_loss: 0.3178 - val_accuracy: 0.8672\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.2635 - accuracy: 0.8938 - val_loss: 0.3480 - val_accuracy: 0.8492\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.1942 - accuracy: 0.9265 - val_loss: 0.2970 - val_accuracy: 0.8750\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.1461 - accuracy: 0.9460 - val_loss: 0.3484 - val_accuracy: 0.8738\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1065 - accuracy: 0.9630 - val_loss: 0.3618 - val_accuracy: 0.8746\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.0753 - accuracy: 0.9743 - val_loss: 0.3987 - val_accuracy: 0.8728\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.0569 - accuracy: 0.9818 - val_loss: 0.4459 - val_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.0409 - accuracy: 0.9867 - val_loss: 0.5084 - val_accuracy: 0.8738\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.0313 - accuracy: 0.9897 - val_loss: 0.5351 - val_accuracy: 0.8620\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.0242 - accuracy: 0.9924 - val_loss: 0.5963 - val_accuracy: 0.8650\n",
            "782/782 [==============================] - 18s 18ms/step - loss: 0.3068 - accuracy: 0.8712\n",
            "Test acc: 0.871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small but noticeable improvement."
      ],
      "metadata": {
        "id": "LeaqgiLCSivv"
      },
      "id": "LeaqgiLCSivv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Pretrained Word Embeddings"
      ],
      "metadata": {
        "id": "Ki1ArPohS8B6"
      },
      "id": "Ki1ArPohS8B6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—one that captures generic aspects of language structure. The rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect that the features you need are fairly generic—that is, common visual features or semantic features. In this case, it makes sense to reuse features learned on a different problem."
      ],
      "metadata": {
        "id": "OGDeC1aUTB2D"
      },
      "id": "OGDeC1aUTB2D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored in the early 2000s, but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2Vec algorithm (https://code.google .com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, such as gender."
      ],
      "metadata": {
        "id": "v29AJgCjTHpY"
      },
      "id": "v29AJgCjTHpY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford .edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data."
      ],
      "metadata": {
        "id": "Hi5OUW-zTMAm"
      },
      "id": "Hi5OUW-zTMAm"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let’s look at how you can get started using GloVe embeddings in a Keras model. The same method is valid for Word2Vec embeddings or any other word-embedding database. We’ll start by downloading the GloVe files and parse them. We’ll then load the word vectors into a Keras Embedding layer, which we’ll use to build a new model.\n",
        "\n",
        " First, let’s download the GloVe word embeddings precomputed on the 2014 English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens). Downloading and unzipping this can take a minute. You should only need to do it once."
      ],
      "metadata": {
        "id": "08NPxzWQTU8n"
      },
      "id": "08NPxzWQTU8n"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g6CT8x7Te0u",
        "outputId": "77d301a7-6350-4017-d49b-1c56bb25abed"
      },
      "id": "_g6CT8x7Te0u",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-15 13:37:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-02-15 13:37:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-02-15 13:37:46--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-02-15 13:40:25 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation."
      ],
      "metadata": {
        "id": "Ihlxekx9TgvO"
      },
      "id": "Ihlxekx9TgvO"
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqITQKYRTlUO",
        "outputId": "0c03e481-f8f6-4694-c07c-232118dfcb8f"
      },
      "id": "LqITQKYRTlUO",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s build an embedding matrix that you can load into an Embedding layer. It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization)."
      ],
      "metadata": {
        "id": "qlLCp7LPTqW2"
      },
      "id": "qlLCp7LPTqW2"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "cj16dnfDT0R-"
      },
      "id": "cj16dnfDT0R-",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use a Constant initializer to load the pretrained embeddings in an Embedding layer. So as not to disrupt the pretrained representations during training, we freeze the layer via trainable=False:"
      ],
      "metadata": {
        "id": "8XqEShffT4PX"
      },
      "id": "8XqEShffT4PX"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rCoYNvTCT9An"
      },
      "id": "rCoYNvTCT9An",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned embeddings."
      ],
      "metadata": {
        "id": "uP5V34KpUBc9"
      },
      "id": "uP5V34KpUBc9"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOkEITmFUGYl",
        "outputId": "f38ae303-516f-461d-d12f-bf4ccd82d9fa"
      },
      "id": "hOkEITmFUGYl",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 64)                34048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2034113 (7.76 MB)\n",
            "Trainable params: 34113 (133.25 KB)\n",
            "Non-trainable params: 2000000 (7.63 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 80s 108ms/step - loss: 0.5788 - accuracy: 0.6927 - val_loss: 0.4528 - val_accuracy: 0.7948\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 65s 103ms/step - loss: 0.4478 - accuracy: 0.7965 - val_loss: 0.3981 - val_accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 61s 98ms/step - loss: 0.3995 - accuracy: 0.8258 - val_loss: 0.3704 - val_accuracy: 0.8318\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 64s 102ms/step - loss: 0.3675 - accuracy: 0.8403 - val_loss: 0.3456 - val_accuracy: 0.8476\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 61s 98ms/step - loss: 0.3429 - accuracy: 0.8536 - val_loss: 0.3337 - val_accuracy: 0.8546\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 64s 102ms/step - loss: 0.3201 - accuracy: 0.8659 - val_loss: 0.3280 - val_accuracy: 0.8562\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 61s 97ms/step - loss: 0.3009 - accuracy: 0.8738 - val_loss: 0.3233 - val_accuracy: 0.8610\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 63s 100ms/step - loss: 0.2861 - accuracy: 0.8813 - val_loss: 0.3077 - val_accuracy: 0.8684\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.2713 - accuracy: 0.8871 - val_loss: 0.3124 - val_accuracy: 0.8652\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 60s 95ms/step - loss: 0.2567 - accuracy: 0.8939 - val_loss: 0.3034 - val_accuracy: 0.8742\n",
            "782/782 [==============================] - 23s 21ms/step - loss: 0.2931 - accuracy: 0.8786\n",
            "Test acc: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll find that on this particular task, pretrained embeddings aren’t very helpful, because the dataset contains enough samples that it is possible to learn a specialized enough embedding space from scratch. However, leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset."
      ],
      "metadata": {
        "id": "qgAUYNusUJqZ"
      },
      "id": "qgAUYNusUJqZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next time - Transformers!"
      ],
      "metadata": {
        "id": "i2_2GfQFTb4u"
      },
      "id": "i2_2GfQFTb4u"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}