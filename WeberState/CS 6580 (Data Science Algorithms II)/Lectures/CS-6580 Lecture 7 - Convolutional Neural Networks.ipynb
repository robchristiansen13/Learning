{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f1c05e",
   "metadata": {},
   "source": [
    "# CS-6580 Lecture 7 - Convolutional Neural Networks\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6675f",
   "metadata": {},
   "source": [
    "In this lecture we'll discuss the basic concepts behind a \"convolutional neural network\", or CNN, and how they can be used to significantly improve performance when there are certain types of structure to the data. In particular, CNNs have been extremely useful when dealing with images, and are one of the most important machine learning tools in the field of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077aaaab-9e0c-448b-b84c-419d9257c43c",
   "metadata": {},
   "source": [
    "The concept here is similar to an RNN is that it doesn't fundamentally alter the methodology of the neural network - it modifies its structure to address an underlying structure in the data. In the case of an RNN this structure was the sequential order within the data. In the case of a CNN its the relative importance of data that is close together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f631db",
   "metadata": {},
   "source": [
    "**The Problem With Images**\n",
    "\n",
    "In our first assignment, we created a basic sequential neural network to learn how to classify hand-written digits - and it did quite well! However, while it's not a trivial problem, it's much, *much* simpler than most computer vision problems.\n",
    "\n",
    "One reason for this is the size of the data for each observation. For the hand-written digits, we had a $28 \\times 28$ grid, and each grid had a greyscale score between 0 and 255, which meant 784 inputs per observation. That's not a trivial number, but it's much smaller than the amount of data in a standard digital image. For a standard digital image something like a $200 \\times 200$ grid is much more common, and instead of there being a single greyscale value, there are typically 3 (RGB) color values, which would mean each observation would have about $200 \\times 200 \\times 3 = 120,000$ data points. That's a lot of data, and consequently a lot of weights to learn! With that many weights, convergence can be slow, and you need *a lot* of data if you want to avoid overfitting.\n",
    "\n",
    "So, what does a convolutional neural network do? It looks for patterns by applying \"convolution filters\" to the images, and then learns to classify the images based upon the information from these filters - which is usually a much smaller amount of information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7dff7c",
   "metadata": {},
   "source": [
    "**Convolution Filters**\n",
    "\n",
    "What is a convolution filter? Let's look at it in the context of an exceptionally easy image classification problem - differentiating Xs and Os. So, instead of trying to learn 10 digits, here we're just learning two characters. Further, let's suppose we've only got a $9 \\times 9$ grid and that each pixel in the grid is a bitmap, which means it's either a $0$ or a $1$. Here we could probably train just a simple perceptron (logistic regression) model and it would do a decent job.\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYTbYudLJAOAdM2O1vgTOSBRmWaGS-9EVuZyVH-k2RTuFqZg6YcwWXN-Sql8452dv87GFKpOLVvPeGWJQM8ylyX3xacY=s1600\" width=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ba5ce",
   "metadata": {},
   "source": [
    "However, this example highlights some important aspects about how we might use a convolutional neural network. For example, there are features in the data that we'd expect to see for an \"X\" that we would not expect to see for an \"O\". We wouldn't expect to see a hard left-to-right or right-to-left diagonal line on an \"O\", but we'd definitely expect to see one on an \"X\". Even more, we wouldn't expect to see a crossed line on an \"O\", but we'd absolutely expect to find one on an \"X\". Conversely, on an \"O\" we'd expect to see curves and vertical / horizontal lines.\n",
    "\n",
    "The important thing here is that these features that are critical for distinguishing between an \"X\" and an \"O\" relate to pixels that are *near* each other. In other words, the interactions between inputs that are - in some sense - close to each other are important, and it would be nice if this were reflected within our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a286c34",
   "metadata": {},
   "source": [
    "This is the idea behind a convolution filter, which is the foundation of a convolutional neural network. What a convolution filter does it it takes a pattern of interest (like the left to right line below), and moves over the data attempting to determine whether and where that pattern occurs. Generally speaking, in mathematics a \"convolution\" is some measure of how similar two things are to each other. They come up *all the time* in things like signal processing.\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYRFqA4p2B7lTh5HKzEsYpyEFBubwK8gGLP7Y21ht69iSAsZlA9uhWC0Gw9cMGToJBviHyWf-1dnguCx0ZrKtZu3wMiIhg=s1600\" width=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2129b",
   "metadata": {},
   "source": [
    "The way it does this is by taking the \"convolution\" of the pattern with a segment of the data.\n",
    "\n",
    "For example, we could format our data is such a way that a black pixel is represented by a 1, and a white pixel is represented by a -1. Then, we could do a pixel-wise multiplication of our pattern with a segment of our image, and then calculate the average of this multiplication. This will give us a number between -1 and 1. A 1 would incidate perfect alignment with our pattern, while a -1 would indicate an exact misalignment.\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYRIyps5zVaWyp4XzetvVfDiNxbZjn9nSFudfu0x6Kc7rI_Au8ce44AeSW7anvUzR7p4EpoNWrnuT4Yv7rfW8dzUFx57zw=s1600\" width=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532742f",
   "metadata": {},
   "source": [
    "We then move through the image, calculating this convolution on various segments, and forming an array of convolutions.\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYTl6fIKcoL0yVtfsLrCRjxuTo7l16JZ5lf8Kjzn-NQ6EafRSN0itjNkan1T4BgLOSjhOxVMnsphILgesD65BUScxOEgIg=s1600\" width=1200>\n",
    "</center>\n",
    "    \n",
    "This array can then inform us as to which segments of the image correspond with our pattern, and which do not. We then repeat this for the various patterns of interest for our problem.\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://lh3.googleusercontent.com/drive-viewer/AEYmBYSXUMm5nT9KBSBKxRmZ-M6jt40H9v18MQ9FcgiPiOAeTYw7ln6i3T525HxizO44dZkzvWB-1btsJKe77FlyWZQMCu2b=s1600\" width=1200>\n",
    "</center>\n",
    "\n",
    "These convolutions are then sent to the *convolution layer* of our neural network. The convolution layer is the main building block of a CNN, and we note that it's a restricted layer - the nodes from the previous layer are only connected to a subset of nodes within the convolution layer! This can significantly decrease the number of weights in the neural network, which can decrease the variance and increase the training speed. Big win!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30039c",
   "metadata": {},
   "source": [
    "A few important things to note about CNNs:\n",
    "\n",
    "* Generally speaking the filters are *learned*, not prescribed as in the example above. This is one of the things that's so cool about machine learning - with enough data and the right setup, the program will actually learn which patterns are important and look for those.\n",
    "* The approach is only applicable for certain types of data. Specifically, data where there is a connection between adjacent or near entries.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://lh3.googleusercontent.com/drive-viewer/AEYmBYSASlkMdeNMSFu6rSlwHPIEm1tCm_0T_O1BrnV9AfxRfBRa2FEpcRWkb_4tH0AZ31M1jImjsbjacxGzpCmYEkbsaWls=s1600\" scale=1200>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9bd7d-992c-4273-af45-f54f1e8ca760",
   "metadata": {},
   "source": [
    "**Implementing CNNs in Keras**\n",
    "\n",
    "We'll now see how we can use convolutional neural networks in Keras, and show how they can be used to improve performance even on our old favorite MNIST dataset. But first, we'll import our favorite libraries, including our new favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cdb963b-3a51-4be3-a716-8d7f1464bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 09:09:36.719170: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-06 09:09:36.719310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-06 09:09:36.827308: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-06 09:09:37.081880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 09:09:41.029530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213214b-2316-4df0-9b95-9a349de182f0",
   "metadata": {},
   "source": [
    "Now, we'll grab the MNIST dataset from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3296a88-97a1-43a7-91f9-4d4469331b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701515c-d8a7-4176-83b6-40e5caaaf1d0",
   "metadata": {},
   "source": [
    "In previous lectures, we've used a Sequential neural network model. Such models are easy to use, but their applicability is quite limited: it can only express models with a single input and a single output, applying one layer after the other in a sequential fashion. In practice, it's pretty common to encounter models with multiple inputs (say, an image and its metadata), multiple outputs (different things you want to predict about the data), or a nonlinear topology (the \"shape\" of the neural network).\n",
    "\n",
    "In cases where a linear model is insufficient, models are build in Keras with the Functional API. This is what most Keras models in the \"wild\" use. Here's a convolutional neural network. We'll first talk about the Functional API aspects of it, and then the actual convolutional layers themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c4e789-f685-4d84-9ff6-30b8c7d48fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28,28,1), name=\"Handwritten Digits\")\n",
    "x = keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\") (inputs)\n",
    "x = keras.layers.MaxPooling2D(pool_size = 2)(x)\n",
    "x = keras.layers.Conv2D(filters = 64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9057c72-7219-4456-af8f-8a154df05c4c",
   "metadata": {},
   "source": [
    "Let's go over this step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ed980-be18-4abe-a088-64b04e3b61ea",
   "metadata": {},
   "source": [
    "We started by declacing an *Input*. Note that we can give names to these inputs, but it's optional. The inputs object holds information about the shape and possibly the dtype of the data that the model will process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c795a7b-09c3-4c48-9c18-5509e1bcfd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 28, 28, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ecd91-3b4b-480c-a95e-2575d0dab231",
   "metadata": {},
   "source": [
    "Here \"None\" just means we haven't specified how much data we'll be using in our training set. That's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8042428-65fd-46e2-a507-0a4c01268694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594f666-fca8-4322-92f8-315131241c43",
   "metadata": {},
   "source": [
    "Next, we create a layer, \"x\", and call it on the input. We then fed this through a sequence of new layers, always calling \"x\" upon itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bc88c-556c-4430-83b7-4de6cee20ac8",
   "metadata": {},
   "source": [
    "Finally, we defined the nature of our output. We then instantiated our model by calling the *Model* function in Keras with the specified inputs and outputs. The outputs encode all the layers that create them. We can check out a summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee7d7d6-d84f-49b4-9be6-4ad001fd0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Handwritten Digits (InputL  [(None, 28, 28, 1)]       0         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104202 (407.04 KB)\n",
      "Trainable params: 104202 (407.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c922db9-f7f6-452d-b04d-4198cab6d048",
   "metadata": {},
   "source": [
    "In total, we have a svelte 104,202 parameters. Do you know what these types of models are called? They're called \"non-parametric\" models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3fe35-4158-4fcb-8d23-4ed96e826eba",
   "metadata": {},
   "source": [
    "Alright, now let's go through the various layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ca234-830b-4e11-83f0-6408996f69a8",
   "metadata": {},
   "source": [
    "In the MNIST dataset, the first layer of the model takes a feature map of size $(28, 28, 1)$, and outputs a feature map of size $(26, 26, 32)$. The number $32$ here is specified, and is the number of filter that we learn for that first layer. So, what's produced from this first 2D convolution layer is a feature map of size $(26,26,32)$ - the 32 filters applied to each $3 \\times 3$ grid in the image. Note that we don't specify what these filters are - that's part of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2da88-6980-43a7-9a62-49917514d8ce",
   "metadata": {},
   "source": [
    "OK, so what about that *max_pooling2D* function? What's that? Well, what it does is it takes each $2 \\times 2$ subgrid, and it produces the maximum value observed there for each feature. In other words, it aggressively downsamples our feature maps.\n",
    "\n",
    "Why would we want to do this? Well, let's take a look at what would happen if we didn't have these pooling layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eee60b38-3b70-4aea-ae9e-4a2117a17695",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28,28,1), name=\"Handwritten Digits\")\n",
    "x = keras.layers.Conv2D(filters=32,kernel_size=3,activation=\"relu\") (inputs)\n",
    "x = keras.layers.Conv2D(filters = 64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "model_without_pooling = keras.Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7eb84-6454-4696-bc28-6c9df4377514",
   "metadata": {},
   "source": [
    "We can get a summary of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5449d65-bc1b-42e8-bdb7-24e1a02c234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Handwritten Digits (InputL  [(None, 28, 28, 1)]       0         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 22, 22, 128)       73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 61952)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                619530    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 712202 (2.72 MB)\n",
      "Trainable params: 712202 (2.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_without_pooling.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f96e8-8af6-466c-bee1-56518ccf7184",
   "metadata": {},
   "source": [
    "What are some issues with this model?\n",
    "\n",
    "* We want to make sure that, after a few layers, the convolution maps from disparate regions of our image are interacting with each other. If we only had three Conv2D layers then the $3 \\times 3$ windows in the third layer would only contain information coming from $7 \\times 7$ windows in the initial input. The high-level patterns learned will still be very small with regard to the initial input, which may not be enough to learn what we need to learn. So, why not just put in enough convolution layers to fully shrink down our image? Well, that leads to our second point.\n",
    "\n",
    "* If we only had 3 convolution layers, then the final  feature map would have 712,202 parameters. That's a lot! Unless you have a lot of data, that's way too many parameters, and you'll get intense overfitting. So, those pooling layers help decrease the overall size of your model, which makes it faster to train and more robust (less prone to overfitting). Please note that if we added a bunch more convolution layers, we'd have a ridiculously large number of parameters - much more than every pixel in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0789755-edc8-4b97-b086-6653c1b8904f",
   "metadata": {},
   "source": [
    "Alright, let's see how these models actually perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee3623e1-914a-4695-aa03-ef88005e742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 09:09:51.387982: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 148s 157ms/step - loss: 0.1199 - accuracy: 0.9636\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 120s 128ms/step - loss: 0.0421 - accuracy: 0.9870\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0293 - accuracy: 0.9910\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 140s 149ms/step - loss: 0.0216 - accuracy: 0.9934\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 127s 135ms/step - loss: 0.0157 - accuracy: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0341dc5c10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_without_pooling.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_without_pooling.fit(train_images, train_labels, epochs=5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf831c7-dacb-4084-af4e-af25cc593572",
   "metadata": {},
   "source": [
    "That's a pretty great accuracy! On... the training data. Let's see how it does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33c2538b-2f96-47b4-90ee-5c823471c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0299 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.029887810349464417, 0.9904000163078308]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_without_pooling.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7f58047-b4e7-49b5-8ea4-20fe0d25c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 09:21:00.915312: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 23s 24ms/step - loss: 0.1574 - accuracy: 0.9521\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0433 - accuracy: 0.9864\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0300 - accuracy: 0.9905\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0233 - accuracy: 0.9932\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 20s 22ms/step - loss: 0.0184 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f03342271f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a263b10-1c39-4ee7-88e8-71d66f4d4810",
   "metadata": {},
   "source": [
    "Similarly great accuracy! Let's check out how it does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92a011d5-5261-46ce-84aa-88108b50914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0285 - accuracy: 0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028450777754187584, 0.9908999800682068]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761bb85-b580-4e65-8526-0b7fb054df54",
   "metadata": {},
   "source": [
    "Look at that! It does a little better, plus it trains a lot faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
