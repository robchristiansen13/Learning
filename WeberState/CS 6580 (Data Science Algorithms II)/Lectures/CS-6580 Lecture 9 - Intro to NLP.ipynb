{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf7d4a5-6f8f-4add-a4a2-6a5d9774e524",
   "metadata": {},
   "source": [
    "# CS-6580 Lecture 9 - Intro to NLP\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9ce17-84c9-4b0e-8e31-a98f0b90b0f3",
   "metadata": {},
   "source": [
    "Today will be the first in a sequence of lectures around NLP, RNNs, and Transformers. We'll start today with some of the basics of NLP and approaches to feature engineering and model building with words. Today, we'll focus on viewing words as a set - the \"bag-of-words\" approach. Next time, we'll look at them as a sequence. Next week, we'll take a look at transformers.\n",
    "\n",
    "But first, let's import (some of) our favorite libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa01c2b-031c-408c-8320-490c9cb46023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 15:43:31.111306: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-13 15:43:31.111411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-13 15:43:31.183990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-13 15:43:31.317344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-13 15:43:33.405226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89fe27-8e8b-42d7-95bd-82e44de38bf8",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6de5d-52c5-4d03-ae63-18412b3634ec",
   "metadata": {},
   "source": [
    "In computer science, we refer to human languages, like English or Mandarin, as “natural” languages, to distinguish them from languages that were designed for machines, like Assembly, LISP, or XML. Every machine language was designed: its starting point was a human engineer writing down a set of formal rules to describe what statements you could make in that language and what they meant. Rules came first, and people only started using the language once the rule set was complete. With human language, it’s the reverse: usage comes first, rules arise later. Natural language was shaped by an evolution process, much like biological organisms— that’s what makes it “natural.” Its “rules,” like the grammar of English, were formalized after the fact and are often ignored or broken by its users. As a result, while machine-readable language is highly structured and rigorous, using precise syntactic rules to weave together exactly defined concepts from a fixed vocabulary, natural language is messy—ambiguous, chaotic, sprawling, and constantly in flux. \n",
    "\n",
    "Creating algorithms that can make sense of natural language is a big deal: language, and in particular text, underpins most of our communications and our cultural production. The internet is mostly text. Language is how we store almost all of our knowledge. Our very thoughts are largely built upon language. However, the ability to understand natural language has long eluded machines. Some people once naively thought that you could simply write down the “rule set of English,” much like one can write down the rule set of LISP. Early attempts to build natural language processing (NLP) systems were thus made through the lens of “applied linguistics.” Engineers and linguists would handcraft complex sets of rules to perform basic machine translation or create simple chatbots—like the famous ELIZA program from the 1960s, which used pattern matching to sustain very basic conversation. But language is a rebellious thing: it’s not easily pliable to formalization. After several decades of effort, the capabilities of these systems remained disappointing. \n",
    "\n",
    "Handcrafted rules held out as the dominant approach well into the 1990s. But starting in the late 1980s, faster computers and greater data availability started making a better alternative viable. When you find yourself building systems that are big piles of ad hoc rules, as a clever engineer, you’re likely to start asking: “Could I use a corpus of data to automate the process of finding these rules? Could I search for the rules within some kind of rule space, instead of having to come up with them myself?” And just like that, you’ve graduated to doing machine learning. And so, in the late 1980s, we started seeing machine learning approaches to natural language processing. The earliest ones were based on decision trees—the intent was literally to automate the development of the kind of if/then/else rules of previous systems. Then statistical approaches started gaining speed, starting with logistic regression. Over time, learned parametric models fully took over, and linguistics came to be seen as more of a hindrance than a useful tool. Frederick Jelinek, an early speech recognition researcher, joked in the 1990s: “Every time I fire a linguist, the performance of the speech recognizer goes up.” \n",
    "\n",
    "That’s what modern NLP is about: using machine learning and large datasets to give computers the ability not to understand language, which is a more lofty goal, but to ingest a piece of language as input and return something useful, like predicting the following: \n",
    "\n",
    "* “What’s the topic of this text?” (text classification) \n",
    "* “Does this text contain abuse?” (content filtering) \n",
    "* “Does this text sound positive or negative?” (sentiment analysis) \n",
    "* “What should be the next word in this incomplete sentence?” (language modeling) \n",
    "* “How would you say this in German?” (translation) \n",
    "* “How would you summarize this article in one paragraph?” (summarization) \n",
    "* etc.\n",
    "\n",
    "Of course, keep in mind the text-processing models you will train won’t possess a human-like understanding of language; rather, they simply look for statistical regularities in their input data, which turns out to be sufficient to perform well on many simple tasks. In much the same way that computer vision is pattern recognition applied to pixels, NLP is pattern recognition applied to words, sentences, and paragraphs. The toolset of NLP—decision trees, logistic regression—only saw slow evolution from the 1990s to the early 2010s. Most of the research focus was on feature engineering. However, around 2014-2015, things started changing. Multiple researchers began to investigate the language-understanding capabilities of recurrent neural networks, in particular LSTM. \n",
    "\n",
    "In early 2015, Keras made available the first open source, easy-to-use implementation of LSTM, just at the start of a massive wave of renewed interest in recurrent neural networks—until then, there had only been “research code” that couldn’t be readily reused. Then from 2015 to 2017, recurrent neural networks dominated the booming NLP scene. Bidirectional LSTM models, in particular, set the state of the art on many important tasks, from summarization to question-answering to machine translation. Finally, around 2017–2018, a new architecture rose to replace RNNs: the Transformer, which we will learn about next week. Transformers unlocked considerable progress across the field in a short period of time, and today most NLP systems are based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cec1c0-ebc7-4653-b244-4194b7ed8a07",
   "metadata": {},
   "source": [
    "## Preparing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b4279-1789-4263-94b1-6f6fed23fc4b",
   "metadata": {},
   "source": [
    "Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. Vectorizing text is the process of transforming text into numeric tensors. Text vectorization processes come in many shapes and forms, but they all follow the same template: \n",
    "\n",
    "* First, you standardize the text to make it easier to process, such as by converting it to lowercase or removing punctuation. \n",
    "* You split the text into units (called tokens), such as characters, words, or groups of words. This is called tokenization. \n",
    "* You convert each such token into a numerical vector. This will usually involve first indexing all tokens present in the data. \n",
    "\n",
    "Let’s review each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231520bc-7e6a-44ee-bf85-6dd756629c59",
   "metadata": {},
   "source": [
    "### Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbaddde-5a90-4847-8e8f-05ea0dfe711c",
   "metadata": {},
   "source": [
    "Consider these two sentences: \n",
    "\n",
    "* “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”\n",
    "* “Sunset came; I stared at the México sky. Isn’t nature splendid?” \n",
    "\n",
    "They’re very similar—in fact, they’re almost identical. Yet, if you were to convert them to byte strings, they would end up with very different representations, because “i” and “I” are two different characters, “Mexico” and “México” are two different words, “isnt” isn’t “isn’t,” and so on. A machine learning model doesn’t know a priori that “i” and “I” are the same letter, that “é” is an “e” with an accent, or that “staring” and “stared” are two forms of the same verb. \n",
    "\n",
    "Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with. It’s not exclusive to machine learning, either—you’d have to do the same thing if you were building a search engine. One of the simplest and most widespread standardization schemes is “convert to lowercase and remove punctuation characters.” Our two sentences would become: \n",
    "\n",
    "* “sunset came i was staring at the mexico sky isnt nature splendid” \n",
    "* “sunset came i stared at the méxico sky isnt nature splendid” \n",
    "\n",
    "Much closer already. Another common transformation is to convert special characters to a standard form, such as replacing “é” with “e,” “æ” with “ae,” and so on. Our token “méxico” would then become “mexico”. Lastly, a much more advanced standardization pattern that is more rarely used in a machine learning context is stemming: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation, like turning “caught” and “been catching” into “[catch]” or “cats” into “[cat]”. With stemming, “was staring” and “stared” would become something like “[stare]”, and our two similar sentences would finally end up with an identical encoding: “sunset came i [stare] at the mexico sky isnt nature splendid” With these standardization techniques, your model will require less training data and will generalize better—it won’t need abundant examples of both “Sunset” and “sunset” to learn that they mean the same thing, and it will be able to make sense of “México” even if it has only seen “mexico” in its training set. Of course, standardization may also erase some amount of information, so always keep the context in mind: for instance, if you’re writing a model that extracts questions from interview articles, it should definitely treat “?” as a separate token instead of dropping it, because it’s a useful signal for this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebae7a-d79b-4b7a-9225-7e6d759dbd3e",
   "metadata": {},
   "source": [
    "### Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167e065-955a-4ea1-8963-19e4d5af6c7c",
   "metadata": {},
   "source": [
    "Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called tokenization. You could do this in three different ways: \n",
    "\n",
    "* Word-level tokenization—Where tokens are space-separated (or punctuation-separated) substrings. A variant of this is to further split words into subwords when applicable—for instance, treating “staring” as “star+ing” or “called” as “call+ed.” \n",
    "* N-gram tokenization—Where tokens are groups of N consecutive words. For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams). \n",
    "* Character-level tokenization—Where each character is its own token. In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition. \n",
    "\n",
    "In general, you’ll always use either word-level or N-gram tokenization. There are two kinds of text-processing models: those that care about word order, called *sequence models*, and those that treat input words as a set, discarding their original order, called *bag-of-words models*. If you’re building a sequence model, you’ll use word-level tokenization, and if you’re building a bag-of-words model, you’ll use N-gram tokenization. N-grams are a way to artificially inject a small amount of local word order information into the model. \n",
    "\n",
    "Once your text is split into tokens, you need to encode each token into a numerical representation. You could potentially do this in a stateless way, such as by hashing each token into a fixed binary vector, but in practice, the way you’d go about it is to build an index of all terms found in the training data (the “vocabulary”), and assign a unique integer to each entry in the vocabulary. Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or 30,000 most common words found in the training data. Any text dataset tends to feature an extremely large number of unique terms, most of which only show up once or twice—indexing those rare terms would result in an excessively large feature space, where most features would have almost no information content. \n",
    "\n",
    "Now, there’s an important detail here that we shouldn’t overlook: when we look up a new token in our vocabulary index, it may not necessarily exist. Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so doing token_index = vocabulary[\"cherimoya\"] may result in a KeyError. To handle this, you should use an “out of vocabulary” index (abbreviated as OOV index)—a catch-all for any token that wasn’t in the index. It’s usually index 1: you’re actually doing token_index = vocabulary.get(token, 1). When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”). “Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There are two special tokens that you will commonly use: the OOV token (index 1), and the mask token (index 0). While the OOV token means “here was a word we did not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use it in particular to pad sequence data: because data batches need to be contiguous, all sequences in a batch of sequence data must have the same length, so shorter sequences should be padded to the length of the longest sequence. \n",
    "\n",
    "Keras provides a TextVectorization layer, which is fast and efficient and can be dropped directly into a tf.data pipeline or a Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a808de-ccac-4eee-acd1-3c87d93f9a98",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0bb81-e6e7-481d-b308-2428b986e4a1",
   "metadata": {},
   "source": [
    "Once your text is split into tokens, you need to encode each token into a numerical representation. You could potentially do this in a stateless way, such as by hashing each token into a fixed binary vector, but in practice, the way you’d go about it is to build an index of all terms found in the training data (the “vocabulary”), and assign a unique integer to each entry in the vocabulary. Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or 30,000 most common words found in the training data. Any text dataset tends to feature an extremely large number of unique terms, most of which only show up once or twice—indexing those rare terms would result in an excessively large feature space, where most features would have almost no information content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c1c08-6100-4703-be60-0cb136515ea3",
   "metadata": {},
   "source": [
    "Now, there’s an important detail here that we shouldn’t overlook: when we look up a new token in our vocabulary index, it may not necessarily exist. Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so doing token_index = vocabulary[\"cherimoya\"] may result in a KeyError. To handle this, you should use an “out of vocabulary” index (abbreviated as OOV index)—a catch-all for any token that wasn’t in the index. It’s usually index 1: you’re actually doing token_index = vocabulary.get(token, 1). When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”). \n",
    "\n",
    "“Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There are two special tokens that you will commonly use: the OOV token (index 1), and the mask token (index 0). While the OOV token means “here was a word we did not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use it in particular to pad sequence data: because data batches need to be contiguous, all sequences in a batch of sequence data must have the same length, so shorter sequences should be padded to the length of the longest sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9d57-2b67-4350-b4b6-f56e3a566369",
   "metadata": {},
   "source": [
    "If we wanted to implement all of this in pure Python, it wouldn't be so hard. Perhaps something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc3a1412-7a22-4c71-893a-250d88047031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9aaef-5b02-4257-994b-1435c7c3b7fd",
   "metadata": {},
   "source": [
    "It works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30535a7-3f6d-42ab-8c66-2db96d795a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9531b5-72ef-440b-87f4-b19fc9e316a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43cffd-d65d-454f-85da-0e5fbc42a126",
   "metadata": {},
   "source": [
    "However, using something like this wouldn't be very performant. In practice, you'll want to work with the Keras *TextVectorization* layer, which is fast and efficient and can be dropped directly into a *tf.data* pipeline or a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38955f60-f2a4-4822-b212-85a2d1ced477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\", #Configures the layer to return sequences of words encoded as integer indices. There are several other output model available, which we'll see in a bit.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17359321-215d-4aac-9d62-efd9471e52c6",
   "metadata": {},
   "source": [
    "By default, the *TextVectorization* layer will use the setting “convert to lowercase and remove punctuation” for text standardization, and “split on whitespace” for tokenization. But importantly, you can provide custom functions for standardization and tokenization, which means the layer is flexible enough to handle any use case. Note that such custom functions should operate on tf.string tensors, not regular Python strings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18431bdb-d3cb-44b5-b85b-b1205ddf8334",
   "metadata": {},
   "source": [
    "To index the vocabulary of a text corpus, we call the *adapt()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d5e1c9d-6f45-4a7c-ad1d-6d82dfcf53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b57e9347-89b0-435a-9624-6fc4c43d4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20f735-aaa1-47f5-b1d3-2cf9ee1bea21",
   "metadata": {},
   "source": [
    "We can try this out on a test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "444de276-1a36-4ce3-ba7d-71e554205c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7add4ea-1437-47ea-bdc2-808bd50cef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab1ab4-7609-4fbd-add7-1a7a967fdead",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Order in Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b02ae9-1626-45a0-ae5c-cc90c91d1cbe",
   "metadata": {},
   "source": [
    "How a machine learning model should represent individual words is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we know how to handle those. They should be encoded as dimensions in a feature space, or as category vectors (word vectors in this case). A much more problematic question, however, is how to encode the way words are woven into sentences: word order. \n",
    "\n",
    "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure of English is quite different from that of Japanese. Even within a given language, you can typically say the same thing in different ways by reshuffling the words a bit. Even further, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise. Order is clearly important, but its relationship to meaning isn’t straightforward. \n",
    "\n",
    "How to represent word order is the pivotal question from which different kinds of NLP architectures spring. The simplest thing you could do is just discard order and treat text as an unordered set of words—this gives you bag-of-words models. You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the recurrent models we've previously discussed. Finally, a hybrid approach is also possible: the Transformer is technically order-agnostic, yet it injects word-position information into the representations it processes, which enables it to simultaneously look at different parts of a sentence (unlike RNNs) while still being order-aware. Because they take into account word order, both RNNs and Transformers are called sequence models. \n",
    "\n",
    "Historically, most early applications of machine learning to NLP just involved bag-of-words models. Interest in sequence models only started rising in 2015, with the rebirth of recurrent neural networks. Today, both approaches remain relevant. Let’s see how they work, and when to leverage which. \n",
    "\n",
    "This week, we’ll demonstrate each approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e12948-3192-4f3a-90d2-c953662814a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113938a2-2463-46c6-8c9e-90aa44f8bb83",
   "metadata": {},
   "source": [
    "Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it: (this can take some time, and you only need to do it once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "733ac87d-e5bc-4c0c-873a-9aa07b7db8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "315711b9-af15-46f1-a14f-f825f2da84fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d542f0-5e04-443c-83df-eb82be18fa1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7be83-af66-41c2-b0ff-0a78584af90f",
   "metadata": {},
   "source": [
    "Next, let’s prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val: (Only need to run this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8976c9d-ad0b-43f7-9e16-0da296e4966d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72530a2-0a9a-490b-b380-6440893cddef",
   "metadata": {},
   "source": [
    "Remember how last week we used the image_dataset_from_directory utility to create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the text_dataset_from_directory utility. Let’s create three Dataset objects for training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29a7a69f-0359-4d5f-b894-6247ae9f976b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095fdcb-eb5e-4e97-8f01-e0126bd70e36",
   "metadata": {},
   "source": [
    "These datasets yield inputs that are TensorFlow tf.string tensors and targets that are int32 tensors encoding the value “0” or “1.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f8875aa-552d-4446-b7fc-9dba40428a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"Yes, this is one of THOSE movies, so terrible, so insipid, so trite, that you will not be able to stop laughing. I have watched comedies, good comedies, and laughed less than my wife and I laughed at this movie. The other comments give the idea well enough. The characters are so unpleasant you cheer the rats on, the effects are so poorly done you wonder whose elementary school art class was in charge, the acting-- oh the acting-- talk about tired dialogue and embarrassing pauses.<br /><br />But the rat, yes, the big rat. Why we didn't get to see the rat until the end rather surprised me. Often the 'big one' isn't shown until the end because the budget is limited and good effects chew up so much money. I surmise, however, that in this case the big rat was hidden until the end because the filmmakers were ashamed that the best they had was a guy running around dressed up like a woodchuck with third-world dentistry.<br /><br />The most sublime part of the whole movie is the elevator scene. After figuring out that the rats couldn't stand loud noise (migraines from the bad acting?), the main dude rigs up a fire alarm to send the rats into a frenzy. If you've ever wanting to see a pair of rats waltz while blood squirts out of their heads like a geyser, this film is for you. Really, you need to rent it and see for yourself.<br /><br />But not for more than 99\\xc2\\xa2, OK?\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2a5e2-049d-4e4b-8529-ed0a8edce163",
   "metadata": {},
   "source": [
    "## Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216945b-2ac8-4a32-a213-4e291dd1c952",
   "metadata": {},
   "source": [
    "### One-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45379b2e-6c86-4751-be4d-46be1303bac4",
   "metadata": {},
   "source": [
    "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens. You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams). If you use a bag of single words, the sentence “the cat sat on the mat” becomes \n",
    "\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"} \n",
    "\n",
    "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance, using binary encoding (multi-hot), you’d encode a text as a vector with as many dimensions as there are words in your vocabulary—with 0s almost everywhere and some 1s for dimensions that encode words present in the text. Let’s try this on our task. First, let’s process our raw text datasets with a TextVectorization layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, unigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dac22de0-6e6a-48da-9341-1ca92e03e9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8a5d8-5a23-46da-9aba-8b65afcb522d",
   "metadata": {},
   "source": [
    "You can try to inspect the output of one of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b21dd06a-4e26-4517-9550-5dbbe5dfcc01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766a243-97ba-4a9c-9092-c0ed2384a661",
   "metadata": {},
   "source": [
    "Now, let's write a reusable model-building function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b1e5ea7-a440-4d03-81ba-629890abc52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c2fa3-b393-4e30-ba01-fbc0ef319b3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, let's train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d0acca7-6343-4b55-93f7-217131aeef29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3239 - accuracy: 0.8701 - val_loss: 0.2649 - val_accuracy: 0.8896\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1925 - accuracy: 0.9283 - val_loss: 0.2785 - val_accuracy: 0.8872\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1498 - accuracy: 0.9458 - val_loss: 0.3032 - val_accuracy: 0.8838\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1217 - accuracy: 0.9562 - val_loss: 0.3315 - val_accuracy: 0.8772\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1000 - accuracy: 0.9653 - val_loss: 0.3625 - val_accuracy: 0.8756\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0823 - accuracy: 0.9718 - val_loss: 0.3936 - val_accuracy: 0.8734\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.0678 - accuracy: 0.9770 - val_loss: 0.4247 - val_accuracy: 0.8722\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0556 - accuracy: 0.9811 - val_loss: 0.4557 - val_accuracy: 0.8710\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.4897 - val_accuracy: 0.8704\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0368 - accuracy: 0.9872 - val_loss: 0.5259 - val_accuracy: 0.8698\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2795 - accuracy: 0.8866\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6fd248-2ddb-4403-9c98-020ac2da81b1",
   "metadata": {},
   "source": [
    "This gets us a pretty decent test accuracy! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as negative samples), the “naive baseline” we could reach without training an actual model would only be 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f593c9b-7f80-4db5-ae42-e70d484435ad",
   "metadata": {},
   "source": [
    "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately. For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams). \n",
    "\n",
    "With bigrams, our sentence becomes \n",
    "\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"} \n",
    "\n",
    "The TextVectorization layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an ngrams=N argument as in the following listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0aab4f25-0604-4fb8-91e1-e69a82917a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753dec2-1aba-49d2-812f-d381fa6d91c0",
   "metadata": {},
   "source": [
    "Let’s test how our model performs when trained on such binary-encoded bags of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3a24153-3db4-46ba-8fba-e8be9b429533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3049 - accuracy: 0.8770 - val_loss: 0.2544 - val_accuracy: 0.8962\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1508 - accuracy: 0.9447 - val_loss: 0.2776 - val_accuracy: 0.8954\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0962 - accuracy: 0.9672 - val_loss: 0.3198 - val_accuracy: 0.8914\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0624 - accuracy: 0.9797 - val_loss: 0.3716 - val_accuracy: 0.8884\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0400 - accuracy: 0.9877 - val_loss: 0.4351 - val_accuracy: 0.8836\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0248 - accuracy: 0.9931 - val_loss: 0.5007 - val_accuracy: 0.8814\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0146 - accuracy: 0.9956 - val_loss: 0.5659 - val_accuracy: 0.8814\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.6364 - val_accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.7123 - val_accuracy: 0.8796\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.7902 - val_accuracy: 0.8798\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2565 - accuracy: 0.9002\n",
      "Test acc: 0.900\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f760ac1-109c-4463-a5bf-141f20bc8ab7",
   "metadata": {},
   "source": [
    "Even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b446d0-f9ab-4add-98cf-dacc709618ee",
   "metadata": {},
   "source": [
    "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text: {\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1, \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1} If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is likely a negative one. Here’s how you’d count bigram occurrences with the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8553223b-3681-4aac-bdbd-5935eb7b31fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872b11f-2efa-4a80-a73a-d9701fe8c3db",
   "metadata": {},
   "source": [
    "Now, of course, some words are bound to occur more often than others no matter what the text is about. The words “the,” “a,” “is,” and “are” will always dominate your word count histograms, drowning out other words—despite being pretty much useless features in a classification context. How could we address this? You already guessed it: via normalization. We could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19,988 zero entries), a property called “sparsity.” That’s a great property to have, as it dramatically reduces compute load and reduces the risk of overfitting. If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use should be divide-only. What, then, should we use as the denominator? The best practice is to go with something called TF-IDF normalization—TF-IDF stands for “term frequency, inverse document frequency.” TF-IDF is so common that it’s built into the TextVectorization layer. All you need to do to start using it is to switch the output_mode argument to \"tf_idf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99812aa-5b8f-4801-b9a3-fe2783d72f81",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "        <img src=\"https://lh3.googleusercontent.com/drive-viewer/AEYmBYSnLgNybFvs1ZJeXR0Xfk4J2HCtPc_9nvoc0v4Bar70-vdCX0F-tluBa3qVo72JuYnlnDpv3TIbM-KadFDqQY68xEuQdA=s2560\"/>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29a9d595-c08a-4a79-b3c1-763abcba373c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678c375-1c2c-4f8b-ac4b-dc297b849e9b",
   "metadata": {},
   "source": [
    "Let’s train a new model with this scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d74cfa2-e54e-44aa-a580-75cf513903b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3561 - accuracy: 0.8583 - val_loss: 0.4469 - val_accuracy: 0.8468\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1602 - accuracy: 0.9421 - val_loss: 0.4495 - val_accuracy: 0.8662\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0974 - accuracy: 0.9681 - val_loss: 0.5762 - val_accuracy: 0.8552\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0617 - accuracy: 0.9808 - val_loss: 0.5012 - val_accuracy: 0.8760\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0404 - accuracy: 0.9882 - val_loss: 0.5480 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0258 - accuracy: 0.9933 - val_loss: 0.5573 - val_accuracy: 0.8826\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0169 - accuracy: 0.9960 - val_loss: 0.6562 - val_accuracy: 0.8802\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.7700 - val_accuracy: 0.8792\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.8089 - val_accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.9060 - val_accuracy: 0.8780\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4350 - accuracy: 0.8502\n",
      "Test acc: 0.850\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90492bd5-4610-4241-9105-fb3fbb318e9a",
   "metadata": {},
   "source": [
    "This gets us a test accuracy on the IMDB classification task that isn't better than we did with just 2-grams. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
