{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199523 entries, 0 to 199522\n",
      "Data columns (total 40 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   AAGE      199523 non-null  int64 \n",
      " 1   ACLSWKR   199523 non-null  object\n",
      " 2   ADTIND    199523 non-null  int64 \n",
      " 3   ADTOCC    199523 non-null  int64 \n",
      " 4   AHGA      199523 non-null  object\n",
      " 5   AHRSPAY   199523 non-null  int64 \n",
      " 6   AHSCOL    199523 non-null  object\n",
      " 7   AMARITL   199523 non-null  object\n",
      " 8   AMJIND    199523 non-null  object\n",
      " 9   AMJOCC    199523 non-null  object\n",
      " 10  ARACE     199523 non-null  object\n",
      " 11  AREORGN   199523 non-null  object\n",
      " 12  ASEX      199523 non-null  object\n",
      " 13  AUNMEM    199523 non-null  object\n",
      " 14  AUNTYPE   199523 non-null  object\n",
      " 15  AWKSTAT   199523 non-null  object\n",
      " 16  CAPGAIN   199523 non-null  int64 \n",
      " 17  CAPLOSS   199523 non-null  int64 \n",
      " 18  DIVVAL    199523 non-null  int64 \n",
      " 19  FILESTAT  199523 non-null  object\n",
      " 20  GRINREG   199523 non-null  object\n",
      " 21  GRINST    199523 non-null  object\n",
      " 22  HHDFMX    199523 non-null  object\n",
      " 23  HHDREL    199523 non-null  object\n",
      " 24  MIGMTR1   199523 non-null  object\n",
      " 25  MIGMTR3   199523 non-null  object\n",
      " 26  MIGMTR4   199523 non-null  object\n",
      " 27  MIGSAME   199523 non-null  object\n",
      " 28  MIGSUN    199523 non-null  object\n",
      " 29  NOEMP     199523 non-null  int64 \n",
      " 30  PARENT    199523 non-null  object\n",
      " 31  PEFNTVTY  199523 non-null  object\n",
      " 32  PEMNTVTY  199523 non-null  object\n",
      " 33  PENATVTY  199523 non-null  object\n",
      " 34  PRCITSHP  199523 non-null  object\n",
      " 35  SEOTR     199523 non-null  int64 \n",
      " 36  VETQVA    199523 non-null  object\n",
      " 37  VETYN     199523 non-null  int64 \n",
      " 38  WKSWORK   199523 non-null  int64 \n",
      " 39  CLASS     199523 non-null  int64 \n",
      "dtypes: int64(12), object(28)\n",
      "memory usage: 60.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99762 entries, 0 to 99761\n",
      "Data columns (total 39 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   AAGE      99762 non-null  int64 \n",
      " 1   ACLSWKR   99762 non-null  object\n",
      " 2   ADTIND    99762 non-null  int64 \n",
      " 3   ADTOCC    99762 non-null  int64 \n",
      " 4   AHGA      99762 non-null  object\n",
      " 5   AHRSPAY   99762 non-null  int64 \n",
      " 6   AHSCOL    99762 non-null  object\n",
      " 7   AMARITL   99762 non-null  object\n",
      " 8   AMJIND    99762 non-null  object\n",
      " 9   AMJOCC    99762 non-null  object\n",
      " 10  ARACE     99762 non-null  object\n",
      " 11  AREORGN   99762 non-null  object\n",
      " 12  ASEX      99762 non-null  object\n",
      " 13  AUNMEM    99762 non-null  object\n",
      " 14  AUNTYPE   99762 non-null  object\n",
      " 15  AWKSTAT   99762 non-null  object\n",
      " 16  CAPGAIN   99762 non-null  int64 \n",
      " 17  CAPLOSS   99762 non-null  int64 \n",
      " 18  DIVVAL    99762 non-null  int64 \n",
      " 19  FILESTAT  99762 non-null  object\n",
      " 20  GRINREG   99762 non-null  object\n",
      " 21  GRINST    99762 non-null  object\n",
      " 22  HHDFMX    99762 non-null  object\n",
      " 23  HHDREL    99762 non-null  object\n",
      " 24  MIGMTR1   99762 non-null  object\n",
      " 25  MIGMTR3   99762 non-null  object\n",
      " 26  MIGMTR4   99762 non-null  object\n",
      " 27  MIGSAME   99762 non-null  object\n",
      " 28  MIGSUN    99762 non-null  object\n",
      " 29  NOEMP     99762 non-null  int64 \n",
      " 30  PARENT    99762 non-null  object\n",
      " 31  PEFNTVTY  99762 non-null  object\n",
      " 32  PEMNTVTY  99762 non-null  object\n",
      " 33  PENATVTY  99762 non-null  object\n",
      " 34  PRCITSHP  99762 non-null  object\n",
      " 35  SEOTR     99762 non-null  int64 \n",
      " 36  VETQVA    99762 non-null  object\n",
      " 37  VETYN     99762 non-null  int64 \n",
      " 38  WKSWORK   99762 non-null  int64 \n",
      "dtypes: int64(11), object(28)\n",
      "memory usage: 29.7+ MB\n",
      "df_train.info(): None vs df_test.info(): None\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score # balanced_accuracy_score with adjusted=True is Informedness\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_train = pd.read_csv('Datasets for Assignment 3/census-income.csv')\n",
    "df_test = pd.read_csv('Datasets for Assignment 3/census-income-test.csv')\n",
    "print(f\"df_train.info(): {df_train.info()} vs df_test.info(): {df_test.info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 299285 entries, 0 to 99761\n",
      "Data columns (total 41 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   AAGE      299285 non-null  int64 \n",
      " 1   ACLSWKR   299285 non-null  object\n",
      " 2   ADTIND    299285 non-null  int64 \n",
      " 3   ADTOCC    299285 non-null  int64 \n",
      " 4   AHGA      299285 non-null  object\n",
      " 5   AHRSPAY   299285 non-null  int64 \n",
      " 6   AHSCOL    299285 non-null  object\n",
      " 7   AMARITL   299285 non-null  object\n",
      " 8   AMJIND    299285 non-null  object\n",
      " 9   AMJOCC    299285 non-null  object\n",
      " 10  ARACE     299285 non-null  object\n",
      " 11  AREORGN   299285 non-null  object\n",
      " 12  ASEX      299285 non-null  object\n",
      " 13  AUNMEM    299285 non-null  object\n",
      " 14  AUNTYPE   299285 non-null  object\n",
      " 15  AWKSTAT   299285 non-null  object\n",
      " 16  CAPGAIN   299285 non-null  int64 \n",
      " 17  CAPLOSS   299285 non-null  int64 \n",
      " 18  DIVVAL    299285 non-null  int64 \n",
      " 19  FILESTAT  299285 non-null  object\n",
      " 20  GRINREG   299285 non-null  object\n",
      " 21  GRINST    299285 non-null  object\n",
      " 22  HHDFMX    299285 non-null  object\n",
      " 23  HHDREL    299285 non-null  object\n",
      " 24  MIGMTR1   299285 non-null  object\n",
      " 25  MIGMTR3   299285 non-null  object\n",
      " 26  MIGMTR4   299285 non-null  object\n",
      " 27  MIGSAME   299285 non-null  object\n",
      " 28  MIGSUN    299285 non-null  object\n",
      " 29  NOEMP     299285 non-null  int64 \n",
      " 30  PARENT    299285 non-null  object\n",
      " 31  PEFNTVTY  299285 non-null  object\n",
      " 32  PEMNTVTY  299285 non-null  object\n",
      " 33  PENATVTY  299285 non-null  object\n",
      " 34  PRCITSHP  299285 non-null  object\n",
      " 35  SEOTR     299285 non-null  int64 \n",
      " 36  VETQVA    299285 non-null  object\n",
      " 37  VETYN     299285 non-null  int64 \n",
      " 38  WKSWORK   299285 non-null  int64 \n",
      " 39  CLASS     299285 non-null  int64 \n",
      " 40  FILE      299285 non-null  object\n",
      "dtypes: int64(12), object(29)\n",
      "memory usage: 95.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Some of the values in the train set are not in the test set\n",
    "# Rather than figure out which value is missing from the one hot encoding\n",
    "# I am going to add the additional CLASS column to the test file\n",
    "# and a FILE column so I can track which rows belong to which file\n",
    "\n",
    "df_train['FILE'] = 'Train'\n",
    "\n",
    "df_test['CLASS'] = 100\n",
    "df_test['FILE'] = 'Test'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "# df_all.iloc[0:10,:].to_csv(\"df_all_top.csv\") # Export the last 10 rows\n",
    "# df_all.iloc[-10:,:].to_csv(\"df_all_bottom.csv\") # Export the last 10 rows\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to One-Hot Encode vs Label Encode?\n",
    "\n",
    "```To prevent biases from being introduced, One-Hot Encoding is preferable for nominal data (where there is no inherent order among categories). Label encoding, however, might be more appropriate for ordinal data (where categories naturally have an order)```\n",
    "\n",
    "So we should one-hot encode columns like class of worker, state of residence, etc. After reviewing the column descriptions I decided to one-hot encode all the following columns:\n",
    "\n",
    "'ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR'\n",
    "\n",
    "**https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(df, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale):\n",
    "\n",
    "    def OneHotEncode (df, columns_to_one_hot_encode):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        onehotencode = OneHotEncoder()      \n",
    "\n",
    "        for item in columns_to_one_hot_encode:\n",
    "            df[item] = df[item].astype('category') # Must convert the strings to category numbers for One Hot to work\n",
    "            df[item + '_new'] = df[item].cat.codes # Rob: Need to research this more\n",
    "            # print(f\" Column: {item}\")\n",
    "            # print(df[item + '_new'])\n",
    "\n",
    "        OneHot_df = pd.DataFrame(onehotencode.fit_transform(df[columns_to_one_hot_encode]).toarray())\n",
    "\n",
    "        PostOneHot_df = df.join(OneHot_df) # Appends the OneHot_df to the original dataframe to create a new one\n",
    "        PostOneHot_df[:-100] # Check results from the One Hot Encoding\n",
    "        PostOneHot_df = PostOneHot_df.drop(columns=columns_to_one_hot_encode)\n",
    "        df = PostOneHot_df\n",
    "        return df\n",
    "\n",
    "    def StripSpaces (df):\n",
    "        # I noticed some of the columns get imported with leading spaces. I want to strip() these right away\n",
    "        for column in df.select_dtypes(include=object): # Only review the columns with a str datatype\n",
    "            df[column] = df[column].apply(lambda x: x.strip())\n",
    "        return df\n",
    "    \n",
    "    def PreLabelEncode(df):\n",
    "        #   Before label encoding we want to apply some value judgements to the data to give the resulting labels some ranking\n",
    "        #   education\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Children\", \"0\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Less than 1st grade\", \"1\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"1st 2nd 3rd or 4th grade\", \"2\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"5th or 6th grade\", \"3\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"7th and 8th grade\", \"4\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"9th grade\", \"5\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"10th grade\", \"6\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"11th grade\", \"7\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"12th grade no diploma\", \"8\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"High school graduate\", \"9\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Some college but no degree\", \"10\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-occup /vocational\", \"11\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-academic program\", \"12\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Bachelors degree(BA AB BS)\", \"13\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Masters degree(MA MS MEng MEd MSW MBA)\", \"14\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Doctorate degree(PhD EdD)\", \"15\")) # Sorry Dr. Feuz, but the professional doctorates have you beat in earning potential\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Prof school degree (MD DDS DVM LLB JD)\", \"16\"))\n",
    "\n",
    "        #   enrolled in edu inst last wk\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"High school\", \"1\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"College or university\", \"2\"))\n",
    "\n",
    "        #   live in this house 1 year ago\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Not in universe under 1 year old\", \"0\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   migration prev res in sunbelt\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"?\", \"0\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Not in universe\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   fill inc questionnaire for veteran's admin\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def LabelEncode(df, columns_to_label_encode):\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        LabelEncode = LabelEncoder()\n",
    "\n",
    "        for item in columns_to_label_encode:\n",
    "            df[item]= LabelEncode.fit_transform(df[item])\n",
    "            # print(f\"Post Label Encoding for {item}: {df[item].unique()}\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def StandardScale(df, columns_to_scale):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaled_columns = scaler.fit_transform(df[columns_to_scale])\n",
    "        df[columns_to_scale] = scaled_columns\n",
    "\n",
    "        return df\n",
    "    \n",
    "    df = StripSpaces(df)\n",
    "    df = OneHotEncode(df, columns_to_one_hot_encode)\n",
    "    df = PreLabelEncode(df)\n",
    "    df = LabelEncode(df, columns_to_label_encode)\n",
    "    df = StandardScale(df, columns_to_scale)\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's label encode some of the columns, but first let's update the columns so they have an inherent rank order\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell helps view the values we want to label encode\n",
    "# df_test['VETQVA'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.drop(columns='CLASS')\n",
    "\n",
    "columns_to_one_hot_encode = ['ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR']\n",
    "columns_to_label_encode = ['AHGA','AHSCOL','MIGSAME','MIGSUN','VETQVA',]\n",
    "columns_to_scale = ['AAGE','AHRSPAY','CAPGAIN','CAPLOSS','DIVVAL','NOEMP','WKSWORK',]\n",
    "\n",
    "df_all = Preprocessing(df_all, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale)\n",
    "\n",
    "# print(f\"df_train.info(): {df_train.info()} vs df_test.info(): {df_test.info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Kaggle page showing the best categorical classifiers for a given data set:\n",
    "* https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn\n",
    "* Comment about grid search: https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn/comments#135499\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAGE       float64\n",
       "AHGA         int64\n",
       "AHRSPAY    float64\n",
       "AHSCOL       int64\n",
       "CAPGAIN    float64\n",
       "            ...   \n",
       "463        float64\n",
       "464        float64\n",
       "465        float64\n",
       "466        float64\n",
       "467        float64\n",
       "Length: 509, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.info: <bound method DataFrame.info of             AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0       1.723284    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1       1.051194     2 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "2      -0.741047    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "3      -1.144301     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      -1.099495     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...          ...   ...       ...     ...       ...       ...       ...   \n",
      "199518  2.350569    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "199519  1.364836    14 -0.201599       0  1.281645 -0.136584 -0.096422   \n",
      "199520  0.558328     2 -0.201599       0 -0.092435 -0.136584 -0.020049   \n",
      "199521 -0.830659    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "199522 -0.113762    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "        MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  \\\n",
      "0             0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "1             1       2 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "2             0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "3             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "4             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "...         ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "199518        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199519        2       1 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199520        0       0  1.709970  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "199521        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199522        2       1  1.709970  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "        465  466  467  \n",
      "0       1.0  0.0  0.0  \n",
      "1       1.0  0.0  0.0  \n",
      "2       1.0  0.0  0.0  \n",
      "3       1.0  0.0  0.0  \n",
      "4       1.0  0.0  0.0  \n",
      "...     ...  ...  ...  \n",
      "199518  1.0  0.0  0.0  \n",
      "199519  1.0  0.0  0.0  \n",
      "199520  1.0  0.0  0.0  \n",
      "199521  1.0  0.0  0.0  \n",
      "199522  1.0  0.0  0.0  \n",
      "\n",
      "[199523 rows x 508 columns]> vs df_test.info: <bound method DataFrame.info of            AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0      0.155074     9 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1      0.423910     3 -0.201599       0 -0.092435 -0.136584  1.189027   \n",
      "2     -1.457943     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "3      0.020656    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      0.647940    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...         ...   ...       ...     ...       ...       ...       ...   \n",
      "99757 -0.920271     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99758  1.185612    14 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99759 -0.472211    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99760 -0.203374     5 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99761  1.454448    12 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "       MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  465  \\\n",
      "0            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "1            0       0 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "2            0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
      "3            2       1  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "4            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "...        ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "99757        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99758        0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99759        2       1  0.018533  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99760        0       0  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99761        2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "\n",
      "       466  467  \n",
      "0      0.0  0.0  \n",
      "1      0.0  0.0  \n",
      "2      0.0  0.0  \n",
      "3      0.0  0.0  \n",
      "4      0.0  0.0  \n",
      "...    ...  ...  \n",
      "99757  0.0  0.0  \n",
      "99758  0.0  0.0  \n",
      "99759  0.0  0.0  \n",
      "99760  0.0  0.0  \n",
      "99761  0.0  0.0  \n",
      "\n",
      "[99762 rows x 508 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Separate the files once again\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "df_train = df_all.loc[df_all['FILE'] == 'Train']\n",
    "df_train = df_train.drop(columns='FILE')\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_all.loc[df_all['FILE'] == 'Test']\n",
    "df_test = df_test.drop(columns='FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.info: <bound method DataFrame.info of             AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0       1.723284    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1       1.051194     2 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "2      -0.741047    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "3      -1.144301     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      -1.099495     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...          ...   ...       ...     ...       ...       ...       ...   \n",
      "199518  2.350569    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "199519  1.364836    14 -0.201599       0  1.281645 -0.136584 -0.096422   \n",
      "199520  0.558328     2 -0.201599       0 -0.092435 -0.136584 -0.020049   \n",
      "199521 -0.830659    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "199522 -0.113762    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "        MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  \\\n",
      "0             0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "1             1       2 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "2             0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "3             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "4             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "...         ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "199518        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199519        2       1 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199520        0       0  1.709970  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "199521        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199522        2       1  1.709970  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "        465  466  467  \n",
      "0       1.0  0.0  0.0  \n",
      "1       1.0  0.0  0.0  \n",
      "2       1.0  0.0  0.0  \n",
      "3       1.0  0.0  0.0  \n",
      "4       1.0  0.0  0.0  \n",
      "...     ...  ...  ...  \n",
      "199518  1.0  0.0  0.0  \n",
      "199519  1.0  0.0  0.0  \n",
      "199520  1.0  0.0  0.0  \n",
      "199521  1.0  0.0  0.0  \n",
      "199522  1.0  0.0  0.0  \n",
      "\n",
      "[199523 rows x 508 columns]> vs df_test.info: <bound method DataFrame.info of            AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0      0.155074     9 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1      0.423910     3 -0.201599       0 -0.092435 -0.136584  1.189027   \n",
      "2     -1.457943     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "3      0.020656    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      0.647940    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...         ...   ...       ...     ...       ...       ...       ...   \n",
      "99757 -0.920271     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99758  1.185612    14 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99759 -0.472211    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99760 -0.203374     5 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99761  1.454448    12 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "       MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  465  \\\n",
      "0            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "1            0       0 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "2            0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
      "3            2       1  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "4            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "...        ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "99757        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99758        0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99759        2       1  0.018533  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99760        0       0  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99761        2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "\n",
      "       466  467  \n",
      "0      0.0  0.0  \n",
      "1      0.0  0.0  \n",
      "2      0.0  0.0  \n",
      "3      0.0  0.0  \n",
      "4      0.0  0.0  \n",
      "...    ...  ...  \n",
      "99757  0.0  0.0  \n",
      "99758  0.0  0.0  \n",
      "99759  0.0  0.0  \n",
      "99760  0.0  0.0  \n",
      "99761  0.0  0.0  \n",
      "\n",
      "[99762 rows x 508 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_train.info: {df_train.info} vs df_test.info: {df_test.info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (197527, 507) compared to X_test.shape (99762, 507)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "X_test = df_test.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "\n",
    "y = df_train['CLASS'].values # Only include Class\n",
    "\n",
    "# Initially I want a smaller training set so I can evaluate many models faster\n",
    "X_train, X_test_discard, y_train, y_test_discard = train_test_split(X, y, test_size = 0.01) # Intentionally setting aside a \"test\" set that I will not use\n",
    "                                                    \n",
    "print(f\"X_train.shape {X_train.shape} compared to X_test.shape {X_test.shape}\")\n",
    "\n",
    "# Would normally run the following line, but CLASS isn't in the test data\n",
    "# X_test = df_test.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "\n",
    "# print(f\"X_test.shape {X_test.shape} compared to df_test.shape {df_test.info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# classifiers = [\n",
    "#     # SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "#     # NuSVC(probability=True),\n",
    "#     # DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GradientBoostingClassifier(),\n",
    "#     # GaussianNB(),\n",
    "#     # LinearDiscriminantAnalysis(),\n",
    "#     # QuadraticDiscriminantAnalysis()\n",
    "#     ]\n",
    "\n",
    "# # Logging for Visual Comparison\n",
    "# log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "# log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "# for clf in classifiers:\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     name = clf.__class__.__name__\n",
    "    \n",
    "#     print(\"=\"*30)\n",
    "#     print(name)\n",
    "    \n",
    "#     print('****Results****')\n",
    "#     train_predictions = clf.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, train_predictions)\n",
    "#     print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "#     ll = log_loss(y_test, train_predictions)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "#     log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "#     # log = log.append(log_entry)\n",
    "    \n",
    "# print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the high level results using only 5% of the data:\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "I will narrow in on the 3 most promising models (RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()))and rerun with 25% of the training data\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "Having determined that GradientBoostingClassifier is the lowest overall model using log_loss (log_loss is a cost function where we want the lowest value unlike utility functions where we want the highest), we can now Cross Validate and GridSearch to find the best hyperparameters\n",
    "Credit:  https://www.kaggle.com/code/hatone/gradientboostingclassifier-with-gridsearchcv/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import numpy as np\n",
    "\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10]\n",
    "#     }\n",
    "\n",
    "# clf = GridSearchCV(GradientBoostingClassifier(), parameters, n_jobs=-1, scoring=\"neg_mean_squared_error\") # GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.score(X_train, y_train))\n",
    "# print(clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingGrid Search ran for hours without success. Random Forest wasn't much worse so giving that a shot.\n",
    "\n",
    "https://stackoverflow.com/questions/50993867/increasing-n-jobs-has-no-effect-on-gridsearchcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "parameters = [{\n",
    "    'n_estimators': [20, 30, 40, 50, 60], \n",
    "    'max_features': [100, 200, 300, 400, 500], \n",
    "    'criterion': ['log_loss'] # Need to consider with gini would be better\n",
    "    }] \n",
    "\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-4, scoring=\"neg_mean_squared_error\") # GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the RandomForest() did perform the GridSearch successfully in only a few minutes. Here are the results:\n",
    "-0.0015236567762630313\n",
    "{'criterion': 'entropy', 'max_features': 200, 'n_estimators': 50}\n",
    "\n",
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset X_test to the values from df_test rather than the results of the split\n",
    "# X_test = df_test.values # Include ALL columns except CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be good to write code that would loop through all the columns and print out the uniques to add decisions about one-hot vs label encoding vs scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that takes one column and generates 9 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(criterion = 'log_loss', max_features = 200, n_estimators = 50)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape} vs X_test.shape: {X_test.shape}\")\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "print(f\"result size: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1:507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Program to save a NumPy array to a text file\n",
    "  \n",
    "# Displaying the array\n",
    "print('Array:\\n', result)\n",
    "\n",
    "result.dtype\n",
    "\n",
    "np.savetxt(\"Christiansen_Rob.txt\", result, newline=\"\\n\", fmt = '%i')\n",
    "# result.T.tofile('Christiansen_Rob.txt', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
