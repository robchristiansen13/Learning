{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score # balanced_accuracy_score with adjusted=True is Informedness\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_train = pd.read_csv('Datasets for Assignment 3/census-income.csv')\n",
    "df_test = pd.read_csv('Datasets for Assignment 3/census-income-test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to One-Hot Encode vs Label Encode?\n",
    "\n",
    "```To prevent biases from being introduced, One-Hot Encoding is preferable for nominal data (where there is no inherent order among categories). Label encoding, however, might be more appropriate for ordinal data (where categories naturally have an order)```\n",
    "\n",
    "So we should one-hot encode columns like class of worker, state of residence, etc. After reviewing the column descriptions I decided to one-hot encode all the following columns:\n",
    "\n",
    "'ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR'\n",
    "\n",
    "**https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(df_train, df_test, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale):\n",
    "\n",
    "    def OneHotEncode (df, columns_to_one_hot_encode):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        onehotencode = OneHotEncoder()      \n",
    "\n",
    "        for item in columns_to_one_hot_encode:\n",
    "            df[item] = df[item].astype('category') # Must convert the strings to category numbers for One Hot to work\n",
    "            df[item + '_new'] = df[item].cat.codes # Rob: Need to research this more\n",
    "            # print(f\" Column: {item}\")\n",
    "            # print(df[item + '_new'])\n",
    "\n",
    "        OneHot_df = pd.DataFrame(onehotencode.fit_transform(df[columns_to_one_hot_encode]).toarray())\n",
    "\n",
    "        PostOneHot_df = df.join(OneHot_df) # Appends the OneHot_df to the original dataframe to create a new one\n",
    "        PostOneHot_df[:-100] # Check results from the One Hot Encoding\n",
    "        PostOneHot_df = PostOneHot_df.drop(columns=columns_to_one_hot_encode)\n",
    "        df = PostOneHot_df\n",
    "        return df\n",
    "\n",
    "    def StripSpaces (df):\n",
    "        # I noticed some of the columns get imported with leading spaces. I want to strip() these right away\n",
    "        for column in df.select_dtypes(include=object): # Only review the columns with a str datatype\n",
    "            df[column] = df[column].apply(lambda x: x.strip())\n",
    "        return df\n",
    "    \n",
    "    def PreLabelEncode(df):\n",
    "        #   Before label encoding we want to apply some value judgements to the data to give the resulting labels some ranking\n",
    "        #   education\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Children\", \"0\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Less than 1st grade\", \"1\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"1st 2nd 3rd or 4th grade\", \"2\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"5th or 6th grade\", \"3\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"7th and 8th grade\", \"4\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"9th grade\", \"5\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"10th grade\", \"6\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"11th grade\", \"7\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"12th grade no diploma\", \"8\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"High school graduate\", \"9\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Some college but no degree\", \"10\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-occup /vocational\", \"11\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-academic program\", \"12\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Bachelors degree(BA AB BS)\", \"13\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Masters degree(MA MS MEng MEd MSW MBA)\", \"14\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Doctorate degree(PhD EdD)\", \"15\")) # Sorry Dr. Feuz, but the professional doctorates have you beat in earning potential\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Prof school degree (MD DDS DVM LLB JD)\", \"16\"))\n",
    "\n",
    "        #   enrolled in edu inst last wk\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"High school\", \"1\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"College or university\", \"2\"))\n",
    "\n",
    "        #   live in this house 1 year ago\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Not in universe under 1 year old\", \"0\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   migration prev res in sunbelt\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"?\", \"0\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Not in universe\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   fill inc questionnaire for veteran's admin\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def LabelEncode(df, columns_to_label_encode):\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        LabelEncode = LabelEncoder()\n",
    "\n",
    "        for item in columns_to_label_encode:\n",
    "            df[item]= LabelEncode.fit_transform(df[item])\n",
    "            print(f\"Post Label Encoding for {item}: {df[item].unique()}\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def StandardScale(df, columns_to_scale):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaled_columns = scaler.fit_transform(df[columns_to_scale])\n",
    "        df[columns_to_scale] = scaled_columns\n",
    "\n",
    "        return df\n",
    "    \n",
    "    df_train = StripSpaces(df_train)\n",
    "    df_train = OneHotEncode(df_train, columns_to_one_hot_encode)\n",
    "    df_train = PreLabelEncode(df_train)\n",
    "    df_train = LabelEncode(df_train, columns_to_label_encode)\n",
    "    df_train = StandardScale(df_train, columns_to_scale)\n",
    "\n",
    "    print(f\"df_test before processing: {df_test.info()}\")    \n",
    "    df_test = StripSpaces(df_test)\n",
    "    df_test = OneHotEncode(df_test, columns_to_one_hot_encode)\n",
    "    df_test = PreLabelEncode(df_test)\n",
    "    df_test = LabelEncode(df_test, columns_to_label_encode)\n",
    "    df_test = StandardScale(df_test, columns_to_scale)    \n",
    "    print(f\"df_test after processing: {df_test.info()}\")    \n",
    "    \n",
    "    return(df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's label encode some of the columns, but first let's update the columns so they have an inherent rank order\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, ' Private', 6, ..., ' Not in universe', 2, 12],\n",
       "       [44, ' Self-employed-not incorporated', 37, ...,\n",
       "        ' Not in universe', 2, 26],\n",
       "       [2, ' Not in universe', 0, ..., ' Not in universe', 0, 0],\n",
       "       ...,\n",
       "       [24, ' Self-employed-not incorporated', 1, ...,\n",
       "        ' Not in universe', 2, 52],\n",
       "       [30, ' Private', 45, ..., ' Not in universe', 2, 52],\n",
       "       [67, ' Not in universe', 0, ..., ' Not in universe', 2, 0]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Not in universe', ' No', ' Yes']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell helps view the values we want to label encode\n",
    "df_test['VETQVA'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Label Encoding for AHGA: [16  2 13  0  5  6  1  4 11 15  3  8 10 14  7 12  9]\n",
      "Post Label Encoding for AHSCOL: [0 1 2]\n",
      "Post Label Encoding for MIGSAME: [0 1 2]\n",
      "Post Label Encoding for MIGSUN: [0 2 1]\n",
      "Post Label Encoding for VETQVA: [0 1 2]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99762 entries, 0 to 99761\n",
      "Data columns (total 39 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   AAGE      99762 non-null  int64 \n",
      " 1   ACLSWKR   99762 non-null  object\n",
      " 2   ADTIND    99762 non-null  int64 \n",
      " 3   ADTOCC    99762 non-null  int64 \n",
      " 4   AHGA      99762 non-null  object\n",
      " 5   AHRSPAY   99762 non-null  int64 \n",
      " 6   AHSCOL    99762 non-null  object\n",
      " 7   AMARITL   99762 non-null  object\n",
      " 8   AMJIND    99762 non-null  object\n",
      " 9   AMJOCC    99762 non-null  object\n",
      " 10  ARACE     99762 non-null  object\n",
      " 11  AREORGN   99762 non-null  object\n",
      " 12  ASEX      99762 non-null  object\n",
      " 13  AUNMEM    99762 non-null  object\n",
      " 14  AUNTYPE   99762 non-null  object\n",
      " 15  AWKSTAT   99762 non-null  object\n",
      " 16  CAPGAIN   99762 non-null  int64 \n",
      " 17  CAPLOSS   99762 non-null  int64 \n",
      " 18  DIVVAL    99762 non-null  int64 \n",
      " 19  FILESTAT  99762 non-null  object\n",
      " 20  GRINREG   99762 non-null  object\n",
      " 21  GRINST    99762 non-null  object\n",
      " 22  HHDFMX    99762 non-null  object\n",
      " 23  HHDREL    99762 non-null  object\n",
      " 24  MIGMTR1   99762 non-null  object\n",
      " 25  MIGMTR3   99762 non-null  object\n",
      " 26  MIGMTR4   99762 non-null  object\n",
      " 27  MIGSAME   99762 non-null  object\n",
      " 28  MIGSUN    99762 non-null  object\n",
      " 29  NOEMP     99762 non-null  int64 \n",
      " 30  PARENT    99762 non-null  object\n",
      " 31  PEFNTVTY  99762 non-null  object\n",
      " 32  PEMNTVTY  99762 non-null  object\n",
      " 33  PENATVTY  99762 non-null  object\n",
      " 34  PRCITSHP  99762 non-null  object\n",
      " 35  SEOTR     99762 non-null  int64 \n",
      " 36  VETQVA    99762 non-null  object\n",
      " 37  VETYN     99762 non-null  int64 \n",
      " 38  WKSWORK   99762 non-null  int64 \n",
      "dtypes: int64(11), object(28)\n",
      "memory usage: 29.7+ MB\n",
      "df_test before processing: None\n",
      "Post Label Encoding for AHGA: [ 9  3  0 16  6  5 13  2 11 12 15  1 14  4 10  8  7]\n",
      "Post Label Encoding for AHSCOL: [0 2 1]\n",
      "Post Label Encoding for MIGSAME: [0 2 1]\n",
      "Post Label Encoding for MIGSUN: [0 1 2]\n",
      "Post Label Encoding for VETQVA: [0 1 2]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99762 entries, 0 to 99761\n",
      "Columns: 506 entries, AAGE to 466\n",
      "dtypes: float64(474), int64(6), int8(26)\n",
      "memory usage: 367.8 MB\n",
      "df_test after processing: None\n"
     ]
    }
   ],
   "source": [
    "columns_to_one_hot_encode = ['ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR']\n",
    "columns_to_label_encode = ['AHGA','AHSCOL','MIGSAME','MIGSUN','VETQVA',]\n",
    "columns_to_scale = ['AAGE','AHRSPAY','CAPGAIN','CAPLOSS','DIVVAL','NOEMP','WKSWORK',]\n",
    "\n",
    "df_train, df_test = Preprocessing(df_train, df_test, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Kaggle page showing the best categorical classifiers for a given data set:\n",
    "* https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn\n",
    "* Comment about grid search: https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn/comments#135499\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99762 entries, 0 to 99761\n",
      "Columns: 506 entries, AAGE to 466\n",
      "dtypes: float64(474), int64(6), int8(26)\n",
      "memory usage: 367.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (49880, 507) compared to X_test.shape (149643, 507)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "y = df_train['CLASS'].values # Only include Class\n",
    "\n",
    "# Initially I want a smaller training set so I can evaluate many models faster\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.75)\n",
    "                                                    \n",
    "print(f\"X_train.shape {X_train.shape} compared to X_test.shape {X_test.shape}\")\n",
    "\n",
    "# Would normally run the following line, but CLASS isn't in the test data\n",
    "# X_test = df_test.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49880, 507)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 95.0876%\n",
      "Log Loss: 0.16906899189455807\n",
      "==============================\n",
      "AdaBoostClassifier\n",
      "****Results****\n",
      "Accuracy: 95.2006%\n",
      "Log Loss: 0.5717980553404296\n",
      "==============================\n",
      "GradientBoostingClassifier\n",
      "****Results****\n",
      "Accuracy: 95.5060%\n",
      "Log Loss: 0.1199094202439641\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    # SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    # NuSVC(probability=True),\n",
    "    # DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    # GaussianNB(),\n",
    "    # LinearDiscriminantAnalysis(),\n",
    "    # QuadraticDiscriminantAnalysis()\n",
    "    ]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    # log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the high level results using only 5% of the data:\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "I will narrow in on the 3 most promising models (RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()))and rerun with 25% of the training data\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "Having determined that GradientBoostingClassifier is the lowest overall model using log_loss (log_loss is a cost function where we want the lowest value unlike utility functions where we want the highest), we can now Cross Validate and GridSearch to find the best hyperparameters\n",
    "Credit:  https://www.kaggle.com/code/hatone/gradientboostingclassifier-with-gridsearchcv/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset X_test to the values from df_test rather than the results of the split\n",
    "X_test = df_test.values # Include ALL columns except CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1, scoring=\"neg_mean_squared_log_error\") # GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)\n",
    "\n",
    "result = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be good to write code that would loop through all the columns and print out the uniques to add decisions about one-hot vs label encoding vs scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that takes one column and generates 9 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Program to save a NumPy array to a text file\n",
    "  \n",
    "# Displaying the array\n",
    "print('Array:\\n', result)\n",
    "file = open(\"Christiansen_Rob.txt\", \"w+\")\n",
    " \n",
    "# Saving the array in a text file\n",
    "content = str(result)\n",
    "file.write(content)\n",
    "file.close()\n",
    " \n",
    "# Displaying the contents of the text file\n",
    "file = open(\"Christiansen_Rob.txt\", \"r\")\n",
    "content = file.read()\n",
    " \n",
    "print(\"\\nContent in file1.txt:\\n\", content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
