{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199523 entries, 0 to 199522\n",
      "Data columns (total 40 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   AAGE      199523 non-null  int64 \n",
      " 1   ACLSWKR   199523 non-null  object\n",
      " 2   ADTIND    199523 non-null  int64 \n",
      " 3   ADTOCC    199523 non-null  int64 \n",
      " 4   AHGA      199523 non-null  object\n",
      " 5   AHRSPAY   199523 non-null  int64 \n",
      " 6   AHSCOL    199523 non-null  object\n",
      " 7   AMARITL   199523 non-null  object\n",
      " 8   AMJIND    199523 non-null  object\n",
      " 9   AMJOCC    199523 non-null  object\n",
      " 10  ARACE     199523 non-null  object\n",
      " 11  AREORGN   199523 non-null  object\n",
      " 12  ASEX      199523 non-null  object\n",
      " 13  AUNMEM    199523 non-null  object\n",
      " 14  AUNTYPE   199523 non-null  object\n",
      " 15  AWKSTAT   199523 non-null  object\n",
      " 16  CAPGAIN   199523 non-null  int64 \n",
      " 17  CAPLOSS   199523 non-null  int64 \n",
      " 18  DIVVAL    199523 non-null  int64 \n",
      " 19  FILESTAT  199523 non-null  object\n",
      " 20  GRINREG   199523 non-null  object\n",
      " 21  GRINST    199523 non-null  object\n",
      " 22  HHDFMX    199523 non-null  object\n",
      " 23  HHDREL    199523 non-null  object\n",
      " 24  MIGMTR1   199523 non-null  object\n",
      " 25  MIGMTR3   199523 non-null  object\n",
      " 26  MIGMTR4   199523 non-null  object\n",
      " 27  MIGSAME   199523 non-null  object\n",
      " 28  MIGSUN    199523 non-null  object\n",
      " 29  NOEMP     199523 non-null  int64 \n",
      " 30  PARENT    199523 non-null  object\n",
      " 31  PEFNTVTY  199523 non-null  object\n",
      " 32  PEMNTVTY  199523 non-null  object\n",
      " 33  PENATVTY  199523 non-null  object\n",
      " 34  PRCITSHP  199523 non-null  object\n",
      " 35  SEOTR     199523 non-null  int64 \n",
      " 36  VETQVA    199523 non-null  object\n",
      " 37  VETYN     199523 non-null  int64 \n",
      " 38  WKSWORK   199523 non-null  int64 \n",
      " 39  CLASS     199523 non-null  int64 \n",
      "dtypes: int64(12), object(28)\n",
      "memory usage: 60.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99762 entries, 0 to 99761\n",
      "Data columns (total 39 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   AAGE      99762 non-null  int64 \n",
      " 1   ACLSWKR   99762 non-null  object\n",
      " 2   ADTIND    99762 non-null  int64 \n",
      " 3   ADTOCC    99762 non-null  int64 \n",
      " 4   AHGA      99762 non-null  object\n",
      " 5   AHRSPAY   99762 non-null  int64 \n",
      " 6   AHSCOL    99762 non-null  object\n",
      " 7   AMARITL   99762 non-null  object\n",
      " 8   AMJIND    99762 non-null  object\n",
      " 9   AMJOCC    99762 non-null  object\n",
      " 10  ARACE     99762 non-null  object\n",
      " 11  AREORGN   99762 non-null  object\n",
      " 12  ASEX      99762 non-null  object\n",
      " 13  AUNMEM    99762 non-null  object\n",
      " 14  AUNTYPE   99762 non-null  object\n",
      " 15  AWKSTAT   99762 non-null  object\n",
      " 16  CAPGAIN   99762 non-null  int64 \n",
      " 17  CAPLOSS   99762 non-null  int64 \n",
      " 18  DIVVAL    99762 non-null  int64 \n",
      " 19  FILESTAT  99762 non-null  object\n",
      " 20  GRINREG   99762 non-null  object\n",
      " 21  GRINST    99762 non-null  object\n",
      " 22  HHDFMX    99762 non-null  object\n",
      " 23  HHDREL    99762 non-null  object\n",
      " 24  MIGMTR1   99762 non-null  object\n",
      " 25  MIGMTR3   99762 non-null  object\n",
      " 26  MIGMTR4   99762 non-null  object\n",
      " 27  MIGSAME   99762 non-null  object\n",
      " 28  MIGSUN    99762 non-null  object\n",
      " 29  NOEMP     99762 non-null  int64 \n",
      " 30  PARENT    99762 non-null  object\n",
      " 31  PEFNTVTY  99762 non-null  object\n",
      " 32  PEMNTVTY  99762 non-null  object\n",
      " 33  PENATVTY  99762 non-null  object\n",
      " 34  PRCITSHP  99762 non-null  object\n",
      " 35  SEOTR     99762 non-null  int64 \n",
      " 36  VETQVA    99762 non-null  object\n",
      " 37  VETYN     99762 non-null  int64 \n",
      " 38  WKSWORK   99762 non-null  int64 \n",
      "dtypes: int64(11), object(28)\n",
      "memory usage: 29.7+ MB\n",
      "df_train.info(): None vs df_test.info(): None\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score # balanced_accuracy_score with adjusted=True is Informedness\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_train = pd.read_csv('Datasets for Assignment 3/census-income.csv')\n",
    "df_test = pd.read_csv('Datasets for Assignment 3/census-income-test.csv')\n",
    "print(f\"df_train.info(): {df_train.info()} vs df_test.info(): {df_test.info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 299285 entries, 0 to 99761\n",
      "Data columns (total 41 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   AAGE      299285 non-null  int64 \n",
      " 1   ACLSWKR   299285 non-null  object\n",
      " 2   ADTIND    299285 non-null  int64 \n",
      " 3   ADTOCC    299285 non-null  int64 \n",
      " 4   AHGA      299285 non-null  object\n",
      " 5   AHRSPAY   299285 non-null  int64 \n",
      " 6   AHSCOL    299285 non-null  object\n",
      " 7   AMARITL   299285 non-null  object\n",
      " 8   AMJIND    299285 non-null  object\n",
      " 9   AMJOCC    299285 non-null  object\n",
      " 10  ARACE     299285 non-null  object\n",
      " 11  AREORGN   299285 non-null  object\n",
      " 12  ASEX      299285 non-null  object\n",
      " 13  AUNMEM    299285 non-null  object\n",
      " 14  AUNTYPE   299285 non-null  object\n",
      " 15  AWKSTAT   299285 non-null  object\n",
      " 16  CAPGAIN   299285 non-null  int64 \n",
      " 17  CAPLOSS   299285 non-null  int64 \n",
      " 18  DIVVAL    299285 non-null  int64 \n",
      " 19  FILESTAT  299285 non-null  object\n",
      " 20  GRINREG   299285 non-null  object\n",
      " 21  GRINST    299285 non-null  object\n",
      " 22  HHDFMX    299285 non-null  object\n",
      " 23  HHDREL    299285 non-null  object\n",
      " 24  MIGMTR1   299285 non-null  object\n",
      " 25  MIGMTR3   299285 non-null  object\n",
      " 26  MIGMTR4   299285 non-null  object\n",
      " 27  MIGSAME   299285 non-null  object\n",
      " 28  MIGSUN    299285 non-null  object\n",
      " 29  NOEMP     299285 non-null  int64 \n",
      " 30  PARENT    299285 non-null  object\n",
      " 31  PEFNTVTY  299285 non-null  object\n",
      " 32  PEMNTVTY  299285 non-null  object\n",
      " 33  PENATVTY  299285 non-null  object\n",
      " 34  PRCITSHP  299285 non-null  object\n",
      " 35  SEOTR     299285 non-null  int64 \n",
      " 36  VETQVA    299285 non-null  object\n",
      " 37  VETYN     299285 non-null  int64 \n",
      " 38  WKSWORK   299285 non-null  int64 \n",
      " 39  CLASS     299285 non-null  int64 \n",
      " 40  FILE      299285 non-null  object\n",
      "dtypes: int64(12), object(29)\n",
      "memory usage: 95.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Some of the values in the train set are not in the test set\n",
    "# Rather than figure out which value is missing from the one hot encoding\n",
    "# I am going to add the additional CLASS column to the test file\n",
    "# and a FILE column so I can track which rows belong to which file\n",
    "\n",
    "df_train['FILE'] = 'Train'\n",
    "\n",
    "df_test['CLASS'] = 100\n",
    "df_test['FILE'] = 'Test'\n",
    "\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "# df_all.iloc[0:10,:].to_csv(\"df_all_top.csv\") # Export the last 10 rows\n",
    "# df_all.iloc[-10:,:].to_csv(\"df_all_bottom.csv\") # Export the last 10 rows\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to One-Hot Encode vs Label Encode?\n",
    "\n",
    "```To prevent biases from being introduced, One-Hot Encoding is preferable for nominal data (where there is no inherent order among categories). Label encoding, however, might be more appropriate for ordinal data (where categories naturally have an order)```\n",
    "\n",
    "So we should one-hot encode columns like class of worker, state of residence, etc. After reviewing the column descriptions I decided to one-hot encode all the following columns:\n",
    "\n",
    "'ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR'\n",
    "\n",
    "**https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(df, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale):\n",
    "\n",
    "    def OneHotEncode (df, columns_to_one_hot_encode):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        onehotencode = OneHotEncoder()      \n",
    "\n",
    "        for item in columns_to_one_hot_encode:\n",
    "            df[item] = df[item].astype('category') # Must convert the strings to category numbers for One Hot to work\n",
    "            df[item + '_new'] = df[item].cat.codes # Rob: Need to research this more\n",
    "            # print(f\" Column: {item}\")\n",
    "            # print(df[item + '_new'])\n",
    "\n",
    "        OneHot_df = pd.DataFrame(onehotencode.fit_transform(df[columns_to_one_hot_encode]).toarray())\n",
    "\n",
    "        PostOneHot_df = df.join(OneHot_df) # Appends the OneHot_df to the original dataframe to create a new one\n",
    "        PostOneHot_df[:-100] # Check results from the One Hot Encoding\n",
    "        PostOneHot_df = PostOneHot_df.drop(columns=columns_to_one_hot_encode)\n",
    "        df = PostOneHot_df\n",
    "        return df\n",
    "\n",
    "    def StripSpaces (df):\n",
    "        # I noticed some of the columns get imported with leading spaces. I want to strip() these right away\n",
    "        for column in df.select_dtypes(include=object): # Only review the columns with a str datatype\n",
    "            df[column] = df[column].apply(lambda x: x.strip())\n",
    "        return df\n",
    "    \n",
    "    def PreLabelEncode(df):\n",
    "        #   Before label encoding we want to apply some value judgements to the data to give the resulting labels some ranking\n",
    "        #   education\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Children\", \"0\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Less than 1st grade\", \"1\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"1st 2nd 3rd or 4th grade\", \"2\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"5th or 6th grade\", \"3\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"7th and 8th grade\", \"4\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"9th grade\", \"5\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"10th grade\", \"6\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"11th grade\", \"7\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"12th grade no diploma\", \"8\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"High school graduate\", \"9\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Some college but no degree\", \"10\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-occup /vocational\", \"11\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Associates degree-academic program\", \"12\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Bachelors degree(BA AB BS)\", \"13\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Masters degree(MA MS MEng MEd MSW MBA)\", \"14\"))\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Doctorate degree(PhD EdD)\", \"15\")) # Sorry Dr. Feuz, but the professional doctorates have you beat in earning potential\n",
    "        df[\"AHGA\"] = df[\"AHGA\"].apply(lambda x: x.replace(\"Prof school degree (MD DDS DVM LLB JD)\", \"16\"))\n",
    "\n",
    "        #   enrolled in edu inst last wk\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"High school\", \"1\"))\n",
    "        df[\"AHSCOL\"] = df[\"AHSCOL\"].apply(lambda x: x.replace(\"College or university\", \"2\"))\n",
    "\n",
    "        #   live in this house 1 year ago\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Not in universe under 1 year old\", \"0\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSAME\"] = df[\"MIGSAME\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   migration prev res in sunbelt\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"?\", \"0\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Not in universe\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"MIGSUN\"] = df[\"MIGSUN\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        #   fill inc questionnaire for veteran's admin\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Not in universe\", \"0\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"No\", \"1\"))\n",
    "        df[\"VETQVA\"] = df[\"VETQVA\"].apply(lambda x: x.replace(\"Yes\", \"2\"))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def LabelEncode(df, columns_to_label_encode):\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        LabelEncode = LabelEncoder()\n",
    "\n",
    "        for item in columns_to_label_encode:\n",
    "            df[item]= LabelEncode.fit_transform(df[item])\n",
    "            # print(f\"Post Label Encoding for {item}: {df[item].unique()}\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def StandardScale(df, columns_to_scale):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        scaled_columns = scaler.fit_transform(df[columns_to_scale])\n",
    "        df[columns_to_scale] = scaled_columns\n",
    "\n",
    "        return df\n",
    "    \n",
    "    df = StripSpaces(df)\n",
    "    df = OneHotEncode(df, columns_to_one_hot_encode)\n",
    "    df = PreLabelEncode(df)\n",
    "    df = LabelEncode(df, columns_to_label_encode)\n",
    "    df = StandardScale(df, columns_to_scale)\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's label encode some of the columns, but first let's update the columns so they have an inherent rank order\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell helps view the values we want to label encode\n",
    "# df_test['VETQVA'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.drop(columns='CLASS')\n",
    "\n",
    "columns_to_one_hot_encode = ['ACLSWKR', 'ADTIND', 'ADTOCC', 'AMARITL', 'AMJIND', 'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1', 'MIGMTR3', 'MIGMTR4', 'PARENT', 'PEFNTVTY', 'PEMNTVTY', 'PENATVTY', 'PRCITSHP', 'SEOTR']\n",
    "columns_to_label_encode = ['AHGA','AHSCOL','MIGSAME','MIGSUN','VETQVA',]\n",
    "columns_to_scale = ['AAGE','AHRSPAY','CAPGAIN','CAPLOSS','DIVVAL','NOEMP','WKSWORK',]\n",
    "\n",
    "df_all = Preprocessing(df_all, columns_to_one_hot_encode, columns_to_label_encode, columns_to_scale)\n",
    "\n",
    "# print(f\"df_train.info(): {df_train.info()} vs df_test.info(): {df_test.info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Kaggle page showing the best categorical classifiers for a given data set:\n",
    "* https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn\n",
    "* Comment about grid search: https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn/comments#135499\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAGE       float64\n",
       "AHGA         int64\n",
       "AHRSPAY    float64\n",
       "AHSCOL       int64\n",
       "CAPGAIN    float64\n",
       "            ...   \n",
       "463        float64\n",
       "464        float64\n",
       "465        float64\n",
       "466        float64\n",
       "467        float64\n",
       "Length: 509, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.info: <bound method DataFrame.info of             AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0       1.723284    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1       1.051194     2 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "2      -0.741047    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "3      -1.144301     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      -1.099495     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...          ...   ...       ...     ...       ...       ...       ...   \n",
      "199518  2.350569    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "199519  1.364836    14 -0.201599       0  1.281645 -0.136584 -0.096422   \n",
      "199520  0.558328     2 -0.201599       0 -0.092435 -0.136584 -0.020049   \n",
      "199521 -0.830659    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "199522 -0.113762    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "        MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  \\\n",
      "0             0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "1             1       2 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "2             0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "3             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "4             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "...         ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "199518        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199519        2       1 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199520        0       0  1.709970  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "199521        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199522        2       1  1.709970  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "        465  466  467  \n",
      "0       1.0  0.0  0.0  \n",
      "1       1.0  0.0  0.0  \n",
      "2       1.0  0.0  0.0  \n",
      "3       1.0  0.0  0.0  \n",
      "4       1.0  0.0  0.0  \n",
      "...     ...  ...  ...  \n",
      "199518  1.0  0.0  0.0  \n",
      "199519  1.0  0.0  0.0  \n",
      "199520  1.0  0.0  0.0  \n",
      "199521  1.0  0.0  0.0  \n",
      "199522  1.0  0.0  0.0  \n",
      "\n",
      "[199523 rows x 508 columns]> vs df_test.info: <bound method DataFrame.info of            AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0      0.155074     9 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1      0.423910     3 -0.201599       0 -0.092435 -0.136584  1.189027   \n",
      "2     -1.457943     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "3      0.020656    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      0.647940    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...         ...   ...       ...     ...       ...       ...       ...   \n",
      "99757 -0.920271     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99758  1.185612    14 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99759 -0.472211    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99760 -0.203374     5 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99761  1.454448    12 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "       MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  465  \\\n",
      "0            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "1            0       0 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "2            0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
      "3            2       1  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "4            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "...        ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "99757        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99758        0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99759        2       1  0.018533  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99760        0       0  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99761        2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "\n",
      "       466  467  \n",
      "0      0.0  0.0  \n",
      "1      0.0  0.0  \n",
      "2      0.0  0.0  \n",
      "3      0.0  0.0  \n",
      "4      0.0  0.0  \n",
      "...    ...  ...  \n",
      "99757  0.0  0.0  \n",
      "99758  0.0  0.0  \n",
      "99759  0.0  0.0  \n",
      "99760  0.0  0.0  \n",
      "99761  0.0  0.0  \n",
      "\n",
      "[99762 rows x 508 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Separate the files once again\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "df_train = df_all.loc[df_all['FILE'] == 'Train']\n",
    "df_train = df_train.drop(columns='FILE')\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test = df_all.loc[df_all['FILE'] == 'Test']\n",
    "df_test = df_test.drop(columns='FILE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.info: <bound method DataFrame.info of             AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0       1.723284    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1       1.051194     2 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "2      -0.741047    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "3      -1.144301     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      -1.099495     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...          ...   ...       ...     ...       ...       ...       ...   \n",
      "199518  2.350569    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "199519  1.364836    14 -0.201599       0  1.281645 -0.136584 -0.096422   \n",
      "199520  0.558328     2 -0.201599       0 -0.092435 -0.136584 -0.020049   \n",
      "199521 -0.830659    13 -0.201599       1 -0.092435 -0.136584 -0.101067   \n",
      "199522 -0.113762    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "        MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  \\\n",
      "0             0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "1             1       2 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "2             0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "3             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "4             2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "...         ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "199518        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199519        2       1 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199520        0       0  1.709970  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "199521        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
      "199522        2       1  1.709970  ...  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "        465  466  467  \n",
      "0       1.0  0.0  0.0  \n",
      "1       1.0  0.0  0.0  \n",
      "2       1.0  0.0  0.0  \n",
      "3       1.0  0.0  0.0  \n",
      "4       1.0  0.0  0.0  \n",
      "...     ...  ...  ...  \n",
      "199518  1.0  0.0  0.0  \n",
      "199519  1.0  0.0  0.0  \n",
      "199520  1.0  0.0  0.0  \n",
      "199521  1.0  0.0  0.0  \n",
      "199522  1.0  0.0  0.0  \n",
      "\n",
      "[199523 rows x 508 columns]> vs df_test.info: <bound method DataFrame.info of            AAGE  AHGA   AHRSPAY  AHSCOL   CAPGAIN   CAPLOSS    DIVVAL  \\\n",
      "0      0.155074     9 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "1      0.423910     3 -0.201599       0 -0.092435 -0.136584  1.189027   \n",
      "2     -1.457943     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "3      0.020656    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "4      0.647940    16 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "...         ...   ...       ...     ...       ...       ...       ...   \n",
      "99757 -0.920271     0 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99758  1.185612    14 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99759 -0.472211    11 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99760 -0.203374     5 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "99761  1.454448    12 -0.201599       0 -0.092435 -0.136584 -0.101067   \n",
      "\n",
      "       MIGSAME  MIGSUN     NOEMP  ...  458  459  460  461  462  463  464  465  \\\n",
      "0            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "1            0       0 -0.404326  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "2            0       0 -0.827186  ...  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
      "3            2       1  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "4            0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "...        ...     ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "99757        0       0 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99758        0       0  0.864252  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99759        2       1  0.018533  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99760        0       0  1.287111  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "99761        2       1 -0.827186  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
      "\n",
      "       466  467  \n",
      "0      0.0  0.0  \n",
      "1      0.0  0.0  \n",
      "2      0.0  0.0  \n",
      "3      0.0  0.0  \n",
      "4      0.0  0.0  \n",
      "...    ...  ...  \n",
      "99757  0.0  0.0  \n",
      "99758  0.0  0.0  \n",
      "99759  0.0  0.0  \n",
      "99760  0.0  0.0  \n",
      "99761  0.0  0.0  \n",
      "\n",
      "[99762 rows x 508 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_train.info: {df_train.info} vs df_test.info: {df_test.info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (197527, 507) compared to X_test.shape (99762, 507)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "X_test = df_test.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "\n",
    "y = df_train['CLASS'].values # Only include Class\n",
    "\n",
    "# Initially I want a smaller training set so I can evaluate many models faster\n",
    "X_train, X_test_discard, y_train, y_test_discard = train_test_split(X, y, test_size = 0.01) # Intentionally setting aside a \"test\" set that I will not use\n",
    "                                                    \n",
    "print(f\"X_train.shape {X_train.shape} compared to X_test.shape {X_test.shape}\")\n",
    "\n",
    "# Would normally run the following line, but CLASS isn't in the test data\n",
    "# X_test = df_test.drop(columns='CLASS').values # Include ALL columns except CLASS\n",
    "\n",
    "# print(f\"X_test.shape {X_test.shape} compared to df_test.shape {df_test.info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# classifiers = [\n",
    "#     # SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "#     # NuSVC(probability=True),\n",
    "#     # DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GradientBoostingClassifier(),\n",
    "#     # GaussianNB(),\n",
    "#     # LinearDiscriminantAnalysis(),\n",
    "#     # QuadraticDiscriminantAnalysis()\n",
    "#     ]\n",
    "\n",
    "# # Logging for Visual Comparison\n",
    "# log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "# log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "# for clf in classifiers:\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     name = clf.__class__.__name__\n",
    "    \n",
    "#     print(\"=\"*30)\n",
    "#     print(name)\n",
    "    \n",
    "#     print('****Results****')\n",
    "#     train_predictions = clf.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, train_predictions)\n",
    "#     print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "#     train_predictions = clf.predict_proba(X_test)\n",
    "#     ll = log_loss(y_test, train_predictions)\n",
    "#     print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "#     log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "#     # log = log.append(log_entry)\n",
    "    \n",
    "# print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the high level results using only 5% of the data:\n",
    "\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "I will narrow in on the 3 most promising models (RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()))and rerun with 25% of the training data\n",
    "\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "Having determined that GradientBoostingClassifier is the lowest overall model using log_loss (log_loss is a cost function where we want the lowest value unlike utility functions where we want the highest), we can now Cross Validate and GridSearch to find the best hyperparameters\n",
    "Credit:  https://www.kaggle.com/code/hatone/gradientboostingclassifier-with-gridsearchcv/script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.24820910559062812\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 0.1, 'n_estimators': 10, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"log_loss\"],\n",
    "    \"learning_rate\": [0.01, 0.075, 0.1, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,8],\n",
    "    \"max_features\":[\"log2\"],\n",
    "    \"criterion\": [\"friedman_mse\"],\n",
    "    \"subsample\":[0.5, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "# Do cross fold validation using all processors. Default CV is 5 folds\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, n_jobs=-1, cv=5, scoring=\"neg_mean_squared_error\") # GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientBoostingGrid Search ran for hours, but ultimately the Grid Search for the Gradient Boost returned: \n",
    "\n",
    "-0.24820910559062812\n",
    "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'min_samples_split': 0.1, 'n_estimators': 10, 'subsample': 0.5}\n",
    "\n",
    "\n",
    ". Random Forest didn't perform significantly worse in the Classifier Showdown so trying it out to see if it is faster. Will let Gradient Boosting run overnight on the weekend.\n",
    "\n",
    "https://stackoverflow.com/questions/50993867/increasing-n-jobs-has-no-effect-on-gridsearchcv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment3.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Do cross fold validation using all processors. Default CV is 5 folds\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m clf \u001b[39m=\u001b[39m GridSearchCV(RandomForestClassifier(), parameters, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(clf\u001b[39m.\u001b[39mscore(X_train, y_train))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(clf\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "parameters = [{\n",
    "    'n_estimators': [20, 30, 40, 50, 60], \n",
    "    'max_features': [100, 200, 300, 400, 500], \n",
    "    'criterion': ['log_loss'] # Need to consider with gini would be better\n",
    "    }] \n",
    "\n",
    "# Do cross fold validation using all processors. Default CV is 5 folds\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1, cv=5, scoring=\"neg_mean_squared_error\") # GridSearchCV requires cost functions so you have turn some scoring metrics into negative numbers for it to work.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the RandomForest() did perform the GridSearch successfully. Here are the results:\n",
    "-0.0024907987262500824\n",
    "{'criterion': 'log_loss', 'max_features': 200, 'n_estimators': 60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset X_test to the values from df_test rather than the results of the split\n",
    "# X_test = df_test.values # Include ALL columns except CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be good to write code that would loop through all the columns and print out the uniques to add decisions about one-hot vs label encoding vs scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that takes one column and generates 9 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GradientBoostingClassifier.__init__() takes 1 positional argument but 2 positional arguments (and 8 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment3.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m GradientBoostingClassifier\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m clf \u001b[39m=\u001b[39m GradientBoostingClassifier(\u001b[39m'\u001b[39;49m\u001b[39mfriedman_mse\u001b[39;49m\u001b[39m'\u001b[39;49m, learning_rate \u001b[39m=\u001b[39;49m \u001b[39m0.01\u001b[39;49m, loss\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mlog_loss\u001b[39;49m\u001b[39m'\u001b[39;49m, max_depth\u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, max_features\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mlog2\u001b[39;49m\u001b[39m'\u001b[39;49m, min_samples_leaf\u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, min_samples_split\u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, n_estimators\u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, subsample\u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment3.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(f\"X_train.shape: {X_train.shape} vs X_test.shape: {X_test.shape}\")\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: GradientBoostingClassifier.__init__() takes 1 positional argument but 2 positional arguments (and 8 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier('friedman_mse', learning_rate = 0.01, loss= 'log_loss', max_depth= 3, max_features= 'log2', min_samples_leaf= 0.1, min_samples_split= 0.1, n_estimators= 10, subsample= 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"X_train.shape: {X_train.shape} vs X_test.shape: {X_test.shape}\")\n",
    "result = clf.predict(X_test)\n",
    "print(f\"result size: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result size: (99762,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = RandomForestClassifier(criterion = 'log_loss', max_features = 200, n_estimators = 60)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"X_train.shape: {X_train.shape} vs X_test.shape: {X_test.shape}\")\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "print(f\"result size: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42390982,  3.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.4579429 ,  0.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.02065567, 16.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.92027069,  0.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.20337442,  5.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.55832787,  5.        , -0.20159864, ...,  1.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1:507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      " [-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Program to save a NumPy array to a text file\n",
    "  \n",
    "# Displaying the array\n",
    "print('Array:\\n', result)\n",
    "\n",
    "result.dtype\n",
    "\n",
    "np.savetxt(\"Christiansen_Rob.txt\", result, newline=\"\\n\", fmt = '%i')\n",
    "# result.T.tofile('Christiansen_Rob.txt', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resouces Consulted for major questions:\n",
    "\n",
    "* https://www.kaggle.com/code/jeffd23/10-classifier-showdown-in-scikit-learn\n",
    "* https://stackoverflow.com/questions/38151615/specific-cross-validation-with-random-forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
