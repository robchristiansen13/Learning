{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6GaaBjFPmns"
      },
      "source": [
        "For this assignment you will be implementing your own verision of the perceptron algorithm.  Similar to the Naive Bayes assignment your Percptron class should be built to be compatible with the sklearn framework.  You can use the sklearn library and other python libraries to help with your implementation but you must implement the actual percpetron algorithm using your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required modules\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score # balanced_accuracy_score with adjusted=True is Informedness\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import preprocessing\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLRLL7L0PYJ0"
      },
      "outputs": [],
      "source": [
        "#TODO: put your perceptron implemenation here\n",
        "\n",
        "#initial imports that you may find useful\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import itertools \n",
        "\n",
        "class Perceptron(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, alpha=1, epochs=10):\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "             \n",
        "    def fit(self, X, y):\n",
        "\n",
        "        def ApplyandReweight(alpha, weights, X, y):\n",
        "\n",
        "            y_hat = np.sum(np.multiply(weights,X))\n",
        "            print(f\"y: {y} y_hat: {y_hat} = {weights} * {X}\")\n",
        "            if (y_hat >= 1 and y==1) or (y_hat < 1 and y == -1):\n",
        "                new_weights = weights\n",
        "                print(f\"Good prediction. Return same weights\")\n",
        "            else:\n",
        "                new_weights = weights + (alpha * (y - y_hat) * X)\n",
        "                print(f\"Bad prediction. new_weights = weights + (alpha * (y - y_hat) * X): {weights} + ({alpha} * ({y_hat} - {y}) * {X})\")\n",
        "            return new_weights\n",
        "        \n",
        "        # STEP 0: Convert dataframe to numpy array\n",
        "        # X = X.to_numpy()\n",
        "        # y = y.to_numpy()\n",
        "\n",
        "        # STEP 1: Initialize the weights of the parameters\n",
        "        X_rows, X_columns = X.shape\n",
        "        # print(f\"X shape: {X_rows}, {X_columns}\")\n",
        "        weights = np.random.rand(X_columns) # Create an array with the number of parameters as X\n",
        "        print(f\"Initial random weights: {weights}\")\n",
        "\n",
        "        # STEP 2: Apply the weights to the Nth instance\n",
        "        epoch = 1\n",
        "        while epoch <= self.epochs:\n",
        "            for i, row in enumerate(y):                    # Iterate through both X and y together. i is the counter, row is X[i]\n",
        "                print(f\"\\nRow[{i}]\")\n",
        "                weights = ApplyandReweight(self.alpha, weights, X[i], row)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "\n",
        "        return np.array(y_predicted) # For a long time I was returning self\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pdClma2RGpf"
      },
      "source": [
        "Split the diabetes data into and 80/20 train/test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VnlgTV_SodE"
      },
      "outputs": [],
      "source": [
        "#TODO: split the diabetes data into training/test data\n",
        "\n",
        "df = pd.read_csv('Datasets for Assignment 4/diabetes.csv')\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGgFkk-zS6iu"
      },
      "source": [
        "Using the diabetes training data with k-fold cross-validation (you choose the value for k) plot your perceptron's cross-validation accuracy and MCC as a function of the number of training epochs.  Use a learning rate of 0.10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3e2jeSrUWZ_"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#TODO: k-fold cross validation with a learning rate of 0.10\n",
        "\n",
        "pipe = make_pipeline(MinMaxScaler(), Perceptron(alpha=0.1, epochs=3))\n",
        "pipe.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLyn_YOiU5gR"
      },
      "source": [
        "Choose a normalization/standardization strategy to apply to the diabetes training data. Using k-fold cross validation and plot the cross validation accuracy and MCC as a function of the number of training epochs.  Repeat for learning rates of 0.90, 0.50, 0.10, 0.01, and 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhd_xx5iYKMa"
      },
      "outputs": [],
      "source": [
        "#TODO: k-fold cross validation on normalized/standardized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anoQ-Q0CYWZH"
      },
      "source": [
        "How does the normalization/standardization affect the accuracy and MCC scores and number of training epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9L9BztuYsz0"
      },
      "source": [
        "Answer in this text box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdPcCan-YzYP"
      },
      "source": [
        "On the scaled data, for each learning rate, how many training epochs are needed to acheive maximize the accuracy and MCC scores?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kxB29z1ZGhV"
      },
      "source": [
        "Answer in this text box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7VgxFqSZIrS"
      },
      "source": [
        "Is there a correlation between learning rates and the number of training epochs needed to acheive the optimal results?  If so, what is the correlation?  If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH9-eXOYZx78"
      },
      "source": [
        "Answer in this text box"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
