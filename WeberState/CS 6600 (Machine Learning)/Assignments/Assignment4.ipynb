{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6GaaBjFPmns"
      },
      "source": [
        "For this assignment you will be implementing your own verision of the perceptron algorithm.  Similar to the Naive Bayes assignment your Percptron class should be built to be compatible with the sklearn framework.  You can use the sklearn library and other python libraries to help with your implementation but you must implement the actual percpetron algorithm using your own code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required modules\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SLRLL7L0PYJ0"
      },
      "outputs": [],
      "source": [
        "#TODO: put your perceptron implemenation here\n",
        "\n",
        "#initial imports that you may find useful\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import itertools \n",
        "\n",
        "class perceptron(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, alpha=0.1, epochs=1):\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "                 \n",
        "    def predict(self, X):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        y_hat = np.zeros(len(X))\n",
        "\n",
        "        for i, row in enumerate(X):                      # Iterate through both X and y together. i is the counter, row is X[i]\n",
        "            # print(f\"predict.enumerate.row: {row}\")\n",
        "            a = sigmoid(np.sum(np.multiply(weights,row)))\n",
        "            # print(f\"a: {a} from sigmoid(sum({weights} product {row}))\")\n",
        "\n",
        "            if a >= 0.5:\n",
        "                # print(f\"Class: 1\")\n",
        "                y_hat[i] = 1\n",
        "            else:\n",
        "                # print(f\"Class: -1\")\n",
        "                y_hat[i] = -1\n",
        "        \n",
        "        # print(f\"y_hat in predict: {y_hat}\")\n",
        "        return np.array(y_hat)\n",
        "    \n",
        "    def train_weights(self, X, y, y_predicted, weights, alpha):\n",
        "        X_rows, X_columns = X.shape\n",
        "        new_weights = np.zeros((X_columns))\n",
        "        weights_subtotal = np.zeros((X_columns))\n",
        "        # print(f\"new_weights initial: {new_weights}\")\n",
        "\n",
        "        for i, row in enumerate(zip(y, y_predicted, X)):\n",
        "            if row[0] != int(row[1]):\n",
        "                # temp_weights = np.sum(new_weights, np.multiply(self.alpha, np.multiply(row[2], (row[0] - row[1]))), axis=0)\n",
        "                temp_weights = np.multiply(self.alpha, np.multiply(row[2], (row[0] - row[1]))) \n",
        "                # print(f\"temp_weights[{i}]: {temp_weights}\")\n",
        "                \n",
        "                weights_subtotal = np.add(weights_subtotal, temp_weights) # This will keep a running subtotal of the weights by parameter\n",
        "                # print(f\"weights_subtotal: {weights_subtotal}\\n\")\n",
        "        # print(f\"np.add(weights + weights_subtotal): {weights} + {weights_subtotal}\")\n",
        "        weights = np.add(weights, weights_subtotal)\n",
        "        # print(f\"weights: {weights}\")\n",
        "        return weights\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        # STEP 1: Initialize the weights of the parameters\n",
        "        X_rows, X_columns = X.shape\n",
        "        # print(f\"X shape: {X_rows}, {X_columns}\")\n",
        "        global weights\n",
        "        weights = np.random.rand(X_columns) # Create an array with the number of parameters as X\n",
        "        # print(f\"Initial random weights: {weights}\")\n",
        "\n",
        "        # STEP 2: Apply the weights to the Nth instance\n",
        "        \n",
        "        epoch = 1\n",
        "        while epoch <= self.epochs:\n",
        "            # print(f\"\\n\\nEpoch: {epoch}\")\n",
        "            y_hat = np.zeros(len(X))\n",
        "            y_hat = perceptron.predict(self, X)\n",
        "            # print(f\"len(y_hat): {len(y_hat)}\")\n",
        "\n",
        "            # for i, row in enumerate(zip(y, y_hat)):                    # Iterate through both X and y together. i is the counter, row is X[i]\n",
        "            weights = self.train_weights(X, y, y_hat, weights, self.alpha)\n",
        "            epoch += 1\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def classes_(self):\n",
        "        if self.estimator:\n",
        "            return self.estimator.classes_\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pdClma2RGpf"
      },
      "source": [
        "Split the diabetes data into and 80/20 train/test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-VnlgTV_SodE"
      },
      "outputs": [],
      "source": [
        "#TODO: split the diabetes data into training/test data\n",
        "\n",
        "df = pd.read_csv('Datasets for Assignment 4/diabetes.csv')\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGgFkk-zS6iu"
      },
      "source": [
        "Using the diabetes training data with k-fold cross-validation (you choose the value for k) plot your perceptron's cross-validation accuracy and MCC as a function of the number of training epochs.  Use a learning rate of 0.10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O3e2jeSrUWZ_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scores: 0.3746501399440224\n",
            "scores: 0.6872717579634813\n",
            "scores: 0.6856723977075836\n"
          ]
        }
      ],
      "source": [
        "#TODO: k-fold cross validation with a learning rate of 0.10\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, balanced_accuracy_score # balanced_accuracy_score with adjusted=True is Informedness\n",
        "\n",
        "epoch_range = [1, 10, 100]\n",
        "\n",
        "for i in epoch_range:\n",
        "    # Create the K-fold cross-validator\n",
        "    pipe = make_pipeline(StandardScaler(), perceptron(alpha=0.1, epochs=i))\n",
        "    # pipe.fit(X_train, y_train)\n",
        "    # print(f\"pipe.score for {i} epochs: {pipe.score(X_train, y_train)}\")\n",
        "\n",
        "    cv = KFold(n_splits=5, shuffle=True)\n",
        "    scores = cross_val_score(pipe, X_train, y_train, cv=cv)\n",
        "    print(f\"scores: {scores.mean()}\")\n",
        "\n",
        "    # y_pred = pipe.predict(X_train)\n",
        "    # accuracy = accuracy_score(y_train, y_pred)\n",
        "    # MCC = matthews_corrcoef(y_train, y_pred)\n",
        "\n",
        "    # score = cross_val_score(pipe, X_train, y_train, cv=10, scoring=\"balanced_accuracy\")\n",
        "    # print(f\"Accuracy scores for epoch_range {i}: {score.mean()}\")\n",
        "\n",
        "    # score = cross_val_score(pipe, X_train, y_train, cv=10, scoring=\"matthews_corrcoef\")\n",
        "    # print(f\"Accuracy scores for epoch_range {i}: {score.mean()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLyn_YOiU5gR"
      },
      "source": [
        "Choose a normalization/standardization strategy to apply to the diabetes training data. Using k-fold cross validation and plot the cross validation accuracy and MCC as a function of the number of training epochs.  Repeat for learning rates of 0.90, 0.50, 0.10, 0.01, and 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xhd_xx5iYKMa"
      },
      "outputs": [],
      "source": [
        "#TODO: k-fold cross validation on normalized/standardized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anoQ-Q0CYWZH"
      },
      "source": [
        "How does the normalization/standardization affect the accuracy and MCC scores and number of training epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9L9BztuYsz0"
      },
      "source": [
        "Answer in this text box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdPcCan-YzYP"
      },
      "source": [
        "On the scaled data, for each learning rate, how many training epochs are needed to acheive maximize the accuracy and MCC scores?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kxB29z1ZGhV"
      },
      "source": [
        "Answer in this text box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7VgxFqSZIrS"
      },
      "source": [
        "Is there a correlation between learning rates and the number of training epochs needed to acheive the optimal results?  If so, what is the correlation?  If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH9-eXOYZx78"
      },
      "source": [
        "Answer in this text box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helpful resources:\n",
        "* https://towardsdatascience.com/how-to-build-a-custom-estimator-for-scikit-learn-fddc0cb9e16e\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
