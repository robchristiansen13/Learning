{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resources Consulted:\n",
        "* https://sklearn-template.readthedocs.io/en/latest/user_guide.html\n",
        "* https://saturncloud.io/blog/how-to-keep-column-names-when-converting-from-pandas-to-numpy\n",
        "* https://github.com/ApoorvRusia/Naive-Bayes-classification-on-Iris-dataset/blob/master/Naiye%20Bayes%20classification%20application.ipynb\n",
        "* https://datascience.stackexchange.com/questions/18904/how-do-i-convert-a-pandas-dataframe-to-a-1d-array\n",
        "* https://stackoverflow.com/questions/35996970/typeerror-fit-missing-1-required-positional-argument-y\n",
        "* https://machinelearningmastery.com/bayes-theorem-for-machine-learning/\n",
        "* https://news.ycombinator.com/item?id=21151032\n",
        "* https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics\n",
        "* **https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html**\n",
        "\n",
        "Notes for study from machinelearningmastery link above:\n",
        "\n",
        "The result P(A|B) is referred to as the posterior probability and P(A) is referred to as the prior probability.\n",
        "\n",
        "P(A|B): Posterior probability.\n",
        "P(A): Prior probability.\n",
        "Sometimes P(B|A) is referred to as the likelihood and P(B) is referred to as the evidence.\n",
        "\n",
        "P(B|A): Likelihood.\n",
        "P(B): Evidence.\n",
        "This allows Bayes Theorem to be restated as:\n",
        "\n",
        "Posterior = Likelihood * Prior / Evidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1K_dSlP_UmU"
      },
      "source": [
        "For this assignment you will need to use the sklearn framework to implement a custom Naive Bayes classifier.  The classifier only needs to handle binary data (both the attributes and the classes).  The attributes will always have a value of 0 or 1.  The class labels will always have a value of 1 or -1.  You can use libraries to help with the data processing, calculations, etc, but you must implement your own NaÃ¯ve Bayes algorithm.  Do not use an existing implementation.  One important implementation detail is that you should convert the probabilities to log probabilities to avoid the number becoming to small to represent as a floating point number.  For example instead of computing P(x|c)P(c) compute log(P(x|c)+log(P(c)). Provide your implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gdxr-tZ4_JHf"
      },
      "outputs": [],
      "source": [
        "#initial imports that you may find useful\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#additional imports\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tFG6O-SBCOyH"
      },
      "outputs": [],
      "source": [
        "# #Your Naive Bayes Implementation goes here.\n",
        "# #Adjust this as you see fit\n",
        "\n",
        "class BinaryNBClassifier(BaseEstimator, ClassifierMixin):        \n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "             \n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        # Initiate variables\n",
        "        word_in_spam = []\n",
        "        word_in_ham = []\n",
        "        word_in_event = []\n",
        "\n",
        "        #  STEP 1: Determine how often the labeled class appears (e.g. spam). This variable is called p_of_b\n",
        "        total_b = 0\n",
        "        \n",
        "        self.row_counter = 0\n",
        "        for row in y:\n",
        "            if y[self.row_counter][0] == 1: # Note: Because the not spam values are recorded as -1 rather than 0 we can't simply add up y using np.sum(y)\n",
        "                total_b += 1\n",
        "            self.row_counter += 1\n",
        "        \n",
        "        #   We will need the following two values later in the predict() function so we need to make sure they are in scope\n",
        "        global p_of_b\n",
        "        global p_of_not_b\n",
        "\n",
        "        p_of_b = total_b / len(y) # This is how often the B event happens in the universe of labeled classifications\n",
        "        p_of_not_b = 1 - p_of_b\n",
        "\n",
        "        # STEP 2: Calculate the total number of A events when B is true as well when B is not true. \n",
        "        # For example, in spam num_of_A_events is equal to the number of A is true words in all the spam labeled messages\n",
        "        # To accomplish this we basically loop through all the 'B is true' rows and add all the '1' values at each row. \n",
        "        self.num_of_A_events_when_B = 0\n",
        "        self.num_of_A_events_when_not_B = 0\n",
        "\n",
        "\n",
        "        self.row_counter = 0\n",
        "        for row in X:\n",
        "            if y[self.row_counter] == 1:\n",
        "                self.num_of_A_events_when_B = self.num_of_A_events_when_B + int(np.sum(X[self.row_counter])) # The logic here is the numpy.sum operator will add all the 1s without having to loop\n",
        "                # print(f\"Row {self.row_counter} num_of_A_events_when_B running total is {self.num_of_A_events_when_B}\")\n",
        "            elif y[self.row_counter] != 1:\n",
        "                # print(f\"Count for B is not true for row {self.row_counter}: {np.sum(X[self.row_counter])}\")                \n",
        "                self.num_of_A_events_when_not_B = self.num_of_A_events_when_not_B + np.sum(X[self.row_counter])\n",
        "                # print(f\"Row {self.row_counter} num_of_A_events_when_not_B running total is {self.num_of_A_events_when_not_B}\")\n",
        "            self.row_counter += 1\n",
        "\n",
        "        p_of_a_given_b = self.num_of_A_events_when_B / total_b\n",
        "        p_of_a_given_not_b = self.num_of_A_events_when_not_B / total_b\n",
        "\n",
        "        # STEP 3: Calculate the total number of features in A x number of events (this is like the count of 'cells')\n",
        "        self.num_of_A_features = np.size(X)\n",
        "\n",
        "        # STEP 4: With the constants in place we can look at the probability of the features when B is true/not true\n",
        "        \n",
        "        #   Prepopulate a single dimension array of length \"feature_count\" with 0s\n",
        "        self.count_of_feature_A_when_b = np.full(len(y), 0) \n",
        "        self.count_of_feature_A_when_not_b = np.full(len(y), 0)\n",
        "        self.word_in_event_counter = np.full(len(y), 0)\n",
        "\n",
        "        global prob_of_feature_A_when_b\n",
        "        global prob_of_feature_A_when_not_b\n",
        "\n",
        "        #   For each event, go through each feature. Increment the appropriate counter based on whether the word appears when B is true / not true\n",
        "        self.row_counter = 0\n",
        "        self.column_counter = 0\n",
        "        self.evaluate_A_feature = int\n",
        "        self.evaluate_B_feature = int\n",
        "\n",
        "        for row in X:\n",
        "            for value in row:\n",
        "                # The following rows setup the condition to search for\n",
        "                self.evaluate_A_feature = X[self.row_counter][self.column_counter]\n",
        "                self.evaluate_B_feature = y[self.row_counter][0]\n",
        "\n",
        "                if (self.evaluate_A_feature == 1 and self.evaluate_B_feature == 1):\n",
        "                    self.count_of_feature_A_when_b[self.column_counter] += 1\n",
        "                elif (self.evaluate_A_feature == 1 and self.evaluate_B_feature != 1):                    \n",
        "                    self.count_of_feature_A_when_not_b[self.column_counter] += 1\n",
        "                if (self.evaluate_A_feature == 1):\n",
        "                    self.word_in_event_counter[self.column_counter] += 1\n",
        "                self.column_counter += 1                \n",
        "            self.column_counter = 0\n",
        "            self.row_counter += 1\n",
        "\n",
        "        #   Having calculated the count of each identified feature, we can now calculate the probabilities of each feature as it appears in B, not B, and total\n",
        "        prob_of_feature_A_when_b = np.divide(self.count_of_feature_A_when_b, self.word_in_event_counter) # The number of times the A feature occurs given B / Number of events marked B\n",
        "        print(f\"prob_of_feature_A_when_b: {prob_of_feature_A_when_b}\")\n",
        "        prob_of_feature_A_when_not_b = np.divide(self.count_of_feature_A_when_not_b, self.word_in_event_counter) # The number of times the A feature occurs given B / Number of events marked B\n",
        "        print(f\"prob_of_feature_A_when_not_b: {prob_of_feature_A_when_not_b}\")\n",
        "\n",
        "        # # END: At this point, we have all the calculations required for what is necessary in the predict method\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "\n",
        "        import math\n",
        "\n",
        "        # The predict function accepts N events with M features to make a classification using the Naive Bayes implementation\n",
        "        # We want to multiply the existence of a feature (or lack thereof) by the probability that feature appears in the B (or not B) labeled training set\n",
        "\n",
        "        num_rows, num_cols = X.shape\n",
        "        y_predicted = np.full(num_rows, 0) \n",
        "\n",
        "        self.row_counter = 0\n",
        "        for row in X:\n",
        "            print(f\"X: {row}\")\n",
        "\n",
        "            # Using the numpy multiply operator we can multiple each 'cell' by the corresponding 'cell'\n",
        "            prob_of_feature_X_as_B_fit_weighted = np.multiply(row, prob_of_feature_A_when_b)\n",
        "            # print(f\"Predict: prob_of_feature_X_as_B_fit_weighted: {prob_of_feature_X_as_B_fit_weighted}\")\n",
        "            prob_of_feature_X_as_not_B_fit_weighted = np.multiply(row, prob_of_feature_A_when_not_b)\n",
        "            # print(f\"Predict: prob_of_feature_X_as_not_B_fit_weighted: {prob_of_feature_X_as_not_B_fit_weighted}\\n\")\n",
        "\n",
        "            # With the probabilities of each feature we can now add the log() scores together\n",
        "            #   Note:   It's common for the probabilities for some of the features to come back zero\n",
        "            #           However the np.log function breaks when trying to take the log(0) since there's no exponent that will get the base value to zero\n",
        "            #           Since we no longer care about the order of the values, we can np.sort(), then np.trim_zeros to get rid of leading or trailing zeros\n",
        "            #           before we take the log()        \n",
        "   \n",
        "            prob_of_feature_X_as_B_fit_weighted_log = np.sum(np.log(np.trim_zeros(np.sort(prob_of_feature_X_as_B_fit_weighted))))\n",
        "            prob_of_feature_X_as_not_B_fit_weighted_log = np.sum(np.log(np.trim_zeros(np.sort(prob_of_feature_X_as_not_B_fit_weighted))))\n",
        "\n",
        "            # print(f\"prob_of_feature_X_as_B_fit_weighted_log: {prob_of_feature_X_as_B_fit_weighted_log}\")\n",
        "            # print(f\"prob_of_feature_X_as_not_B_fit_weighted_log: {prob_of_feature_X_as_not_B_fit_weighted_log}\")\n",
        "\n",
        "            prob_of_B_given_A = 10**(prob_of_feature_X_as_B_fit_weighted_log + math.log10(p_of_b))\n",
        "            prob_of_not_B_given_A =  10**(prob_of_feature_X_as_not_B_fit_weighted_log + math.log10(p_of_not_b)) # ** is the operator for raising a value to that power\n",
        "\n",
        "            # print(f\"prob_of_B_given_A: {prob_of_B_given_A} vs prob_of_not_B_given_A: {prob_of_not_B_given_A}\")\n",
        "\n",
        "            if prob_of_B_given_A >= prob_of_not_B_given_A: \n",
        "                y_predicted[self.row_counter] = 1\n",
        "            else:\n",
        "                y_predicted[self.row_counter] = -1\n",
        "\n",
        "            self.row_counter += 1\n",
        "\n",
        "        print(f\"y_predicted: {y_predicted}\")\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through files in directory:\n",
        "\n",
        "# import required module\n",
        "import os\n",
        "# assign directory\n",
        "directory = 'Datasets for Assignment 2'\n",
        " \n",
        "# iterate over files in directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        # print(f)\n",
        "        df = pd.read_csv(f,)\n",
        "        df.info\n",
        "        \n",
        "        # Prepare the data\n",
        "\n",
        "        # The values for the Events are up to the last column\n",
        "        X = df.iloc[:,:-1].values # The values are everything but the last column\n",
        "        \n",
        "        # The values for the Classification are in the last column\n",
        "        y = df.iloc[:,-1:].values\n",
        "\n",
        "        # Splitting the dataset into the Training set and Test set\n",
        "        # from sklearn.model_selection import train_test_split\n",
        "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)\n",
        "\n",
        "        # However for this particular exercise, the instruction is to use all the samples for training and testing\n",
        "        X_train = X\n",
        "        y_train = y\n",
        "\n",
        "        X_test = X\n",
        "        y_test = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 9 is out of bounds for axis 0 with size 9",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Call pipelines\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m NBClassifier_pipe_start \u001b[39m=\u001b[39m  time\u001b[39m.\u001b[39mtime() \u001b[39m# Returns Unix epoch time\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m NBClassifier_pipe\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m NBClassifier_pipe\u001b[39m.\u001b[39mpredict(y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m NBClassifier_pipe_end \u001b[39m=\u001b[39m  time\u001b[39m.\u001b[39mtime() \u001b[39m# Returns Unix epoch time\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_of_feature_A_when_b[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumn_counter] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39melif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_A_feature \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_B_feature \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m):                    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount_of_feature_A_when_not_b[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumn_counter] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_A_feature \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X43sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_in_event_counter[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumn_counter] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 9"
          ]
        }
      ],
      "source": [
        "# Define pipelines\n",
        "\n",
        "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "import time\n",
        "\n",
        "NBClassifier_pipe = Pipeline([\n",
        "# ('scaler', StandardScaler()), # Not necessary for this exercise\n",
        "# ('selector', VarianceThreshold()), # Not necessary for this exercise\n",
        "('classifier', BinaryNBClassifier())\n",
        "])\n",
        "\n",
        "CategoricalNB_pipe = Pipeline([\n",
        "# ('scaler', StandardScaler()), # Not necessary for this exercise\n",
        "# ('selector', VarianceThreshold()), # Not necessary for this exercise\n",
        "('classifier', CategoricalNB())\n",
        "])\n",
        "\n",
        "# Call pipelines\n",
        "NBClassifier_pipe_start =  time.time() # Returns Unix epoch time\n",
        "NBClassifier_pipe.fit(X_train, y_train)\n",
        "NBClassifier_pipe.predict(y_train)\n",
        "NBClassifier_pipe_end =  time.time() # Returns Unix epoch time\n",
        "NBClassifier_pipe_seconds_elapsed = NBClassifier_pipe_end - NBClassifier_pipe_start\n",
        "\n",
        "CategoricalNB_pipe_start =  time.time() # Returns Unix epoch time\n",
        "CategoricalNB_pipe.fit(X_train, y_train)\n",
        "CategoricalNB_pipe.predict(y_train)\n",
        "CategoricalNB_pipe_end =  time.time() # Returns Unix epoch time\n",
        "CategoricalNB_pipe_seconds_elapsed = CategoricalNB_pipe_end - CategoricalNB_pipe_start\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1  1  1  1  1 -1 -1 -1 -1 -1]\n",
            "[ 1  1  1  1  1 -1  1 -1 -1 -1]\n"
          ]
        }
      ],
      "source": [
        "# Predicting the set results\n",
        "y_pred = cclassifier.predict(X_train)\n",
        "print(y_train)\n",
        "print(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1,  1],\n",
              "       [ 1,  1],\n",
              "       [ 1,  1],\n",
              "       [ 1,  1],\n",
              "       [ 1,  1],\n",
              "       [-1, -1],\n",
              "       [-1,  1],\n",
              "       [-1, -1],\n",
              "       [-1, -1],\n",
              "       [-1, -1]])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#lets see the actual and predicted value side by side\n",
        "y_compare = np.vstack((y_train,y_pred)).T\n",
        "#actual value on the left side and predicted value on the right hand side\n",
        "#printing the top 5 values\n",
        "y_compare[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4 1]\n",
            " [0 5]]\n"
          ]
        }
      ],
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct predictions:  9\n",
            "False predictions 1\n",
            "\n",
            "\n",
            "Accuracy of the Categorical Clasification is:  0.9\n"
          ]
        }
      ],
      "source": [
        "#finding accuracy from the confusion matrix.\n",
        "a = cm.shape\n",
        "corrPred = 0\n",
        "falsePred = 0\n",
        "\n",
        "for row in range(a[0]):\n",
        "    for c in range(a[1]):\n",
        "        if row == c:\n",
        "            corrPred +=cm[row,c]\n",
        "        else:\n",
        "            falsePred += cm[row,c]\n",
        "print('Correct predictions: ', corrPred)\n",
        "print('False predictions', falsePred)\n",
        "print ('\\n\\nAccuracy of the Categorical Clasification is: ', corrPred/(cm.sum()))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv24NS1RCpjT"
      },
      "source": [
        "Now you will train and test your Binary Naive Bayes classifier on a few different datasets.  The datasets can be downloaded from canvas.  They are linked in the assignment description.  For this part of the assignment we will not be splitting the data into training, validation and test data sets.  Instead you should use the entire dataset for training and the entire dataset for testing.  You will need to complete the following table (you can just output the results in this format you don't need to copy them into the text field).\n",
        "\n",
        "|dataset|# of instances|# of features | Your NB Training Time | Your NB Test Time | Your NB Accuracy | sklearn CategoricalNB Training Time | sklearn Categorical NB Test Time | sklearn CategoricalNB Accuracy|\n",
        "|-----------|------------|-------------|------------------|-------------------|-------------------------|---------------------------------|------------------------|----------------------------------|\n",
        "test1_1 |\n",
        "test1_2 |\n",
        "test1_4 |\n",
        "test1_5 |\n",
        "test2_1 |\n",
        "test2_2 |\n",
        "test2_4 |\n",
        "test2_5 |\n",
        "test4_1 |\n",
        "test4_2 |\n",
        "test4_4 |\n",
        "test4_5 |\n",
        "test5_1 |\n",
        "test5_2 |\n",
        "test5_4 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n6x9Ol9tE0OX"
      },
      "outputs": [],
      "source": [
        "#Train and test your BinaryNBClassifier and the sklearn CategoricalNBClassifier on the datasets from canvas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzzilfSNFGvY"
      },
      "source": [
        "The next step for this assignment is split the vote dataset (also found on canvas) into a *train/test split* (use 20% of the data for testing).  Train both algorithms on the training data using *cross-fold validation* and then report the accuracy, f1-score, mcc and informedness results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vi8VaK4V1z_E"
      },
      "outputs": [],
      "source": [
        "#Split the vote dataset\n",
        "#Use cross-validatation to compare BinaryNBClassifier against CategoricalNBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFP5ttuS2AtP"
      },
      "source": [
        "Finally, choose the algorithm that performed the best on the cross-validation, train it on all the training data and test on the test data.  Report the accuracy, f1-score, mcc and informedness results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UquA9oi22X-Q"
      },
      "outputs": [],
      "source": [
        "#Final Generalization test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell remains as a reminder of a LOT of work I did to try to maintain the words with their 'labels'. \n",
        "\n",
        "#Spliting the dataset in independent and dependent variables\n",
        "# X = df.iloc[:,:-1].to_dict('list') # The idea here is to capture all the columns except the last one as X\n",
        "# y = df['Class'].to_dict('records')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
