{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gdxr-tZ4_JHf"
      },
      "outputs": [],
      "source": [
        "#initial imports that you may find useful\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#additional imports\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1K_dSlP_UmU"
      },
      "source": [
        "For this assignment you will need to use the sklearn framework to implement a custom Naive Bayes classifier.  The classifier only needs to handle binary data (both the attributes and the classes).  The attributes will always have a value of 0 or 1.  The class labels will always have a value of 1 or -1.  You can use libraries to help with the data processing, calculations, etc, but you must implement your own Naïve Bayes algorithm.  Do not use an existing implementation.  One important implementation detail is that you should convert the probabilities to log probabilities to avoid the number becoming to small to represent as a floating point number.  For example instead of computing P(x|c)P(c) compute log(P(x|c)+log(P(c)). Provide your implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "tFG6O-SBCOyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prob_of_feature_A_when_b: [0.42857143 1.         0.25       0.4        0.5        0.71428571\n",
            " 0.75       0.5        0.5        0.55555556]\n",
            "prob_of_feature_A_when_not_b: [0.57142857 0.         0.75       0.6        0.5        0.28571429\n",
            " 0.25       0.5        0.5        0.44444444]\n",
            "X: [1 0 0 0 1 1 0 1 1 1]\n",
            "X: [1 1 1 1 0 1 0 1 0 1]\n",
            "X: [0 1 0 0 1 1 1 1 0 1]\n",
            "X: [1 1 0 0 0 1 1 0 1 1]\n",
            "X: [0 0 0 1 0 1 1 0 0 1]\n",
            "X: [1 0 1 1 0 1 0 1 1 1]\n",
            "X: [0 0 0 1 0 1 1 1 1 1]\n",
            "X: [1 0 1 0 1 0 0 1 0 0]\n",
            "X: [1 0 0 1 1 0 0 0 0 1]\n",
            "X: [1 0 1 0 0 0 0 0 0 1]\n",
            "y_predicted: [ 1 -1  1  1  1 -1  1 -1 -1 -1]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BinaryNBClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BinaryNBClassifier</label><div class=\"sk-toggleable__content\"><pre>BinaryNBClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "BinaryNBClassifier()"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# #Your Naive Bayes Implementation goes here.\n",
        "# #Adjust this as you see fit\n",
        "\n",
        "class BinaryNBClassifier(BaseEstimator, ClassifierMixin):        \n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "             \n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        # Initiate variables\n",
        "        word_in_spam = []\n",
        "        word_in_ham = []\n",
        "        word_in_event = []\n",
        "\n",
        "        #  STEP 1: Determine how often the labeled class appears (e.g. spam). This variable is called p_of_b\n",
        "        total_b = 0\n",
        "        \n",
        "        self.row_counter = 0\n",
        "        for row in y:\n",
        "            if y[self.row_counter][0] == 1: # Note: Because the not spam values are recorded as -1 rather than 0 we can't simply add up y using np.sum(y)\n",
        "                total_b += 1\n",
        "            self.row_counter += 1\n",
        "        \n",
        "        #   We will need the following two values later in the predict() function so we need to make sure they are in scope\n",
        "        global p_of_b\n",
        "        global p_of_not_b\n",
        "\n",
        "        p_of_b = total_b / len(y) # This is how often the B event happens in the universe of labeled classifications\n",
        "        p_of_not_b = 1 - p_of_b\n",
        "\n",
        "        # STEP 2: Calculate the total number of A events when B is true as well when B is not true. \n",
        "        # For example, in spam num_of_A_events is equal to the number of A is true words in all the spam labeled messages\n",
        "        # To accomplish this we basically loop through all the 'B is true' rows and add all the '1' values at each row. \n",
        "        self.num_of_A_events_when_B = 0\n",
        "        self.num_of_A_events_when_not_B = 0\n",
        "\n",
        "\n",
        "        self.row_counter = 0\n",
        "        for row in X:\n",
        "            if y[self.row_counter] == 1:\n",
        "                self.num_of_A_events_when_B = self.num_of_A_events_when_B + int(np.sum(X[self.row_counter])) # The logic here is the numpy.sum operator will add all the 1s without having to loop\n",
        "                # print(f\"Row {self.row_counter} num_of_A_events_when_B running total is {self.num_of_A_events_when_B}\")\n",
        "            elif y[self.row_counter] != 1:\n",
        "                # print(f\"Count for B is not true for row {self.row_counter}: {np.sum(X[self.row_counter])}\")                \n",
        "                self.num_of_A_events_when_not_B = self.num_of_A_events_when_not_B + np.sum(X[self.row_counter])\n",
        "                # print(f\"Row {self.row_counter} num_of_A_events_when_not_B running total is {self.num_of_A_events_when_not_B}\")\n",
        "            self.row_counter += 1\n",
        "\n",
        "        p_of_a_given_b = self.num_of_A_events_when_B / total_b\n",
        "        p_of_a_given_not_b = self.num_of_A_events_when_not_B / total_b\n",
        "\n",
        "        # STEP 3: Calculate the total number of features in A x number of events (this is like the count of 'cells')\n",
        "        self.num_of_A_features = np.size(X)\n",
        "\n",
        "        # STEP 4: With the constants in place we can look at the probability of the features when B is true/not true\n",
        "        \n",
        "        #   Prepopulate a single dimension array of length \"feature_count\" with 0s\n",
        "        self.count_of_feature_A_when_b = np.full(len(y), 0) \n",
        "        self.count_of_feature_A_when_not_b = np.full(len(y), 0)\n",
        "        self.word_in_event_counter = np.full(len(y), 0)\n",
        "\n",
        "        global prob_of_feature_A_when_b\n",
        "        global prob_of_feature_A_when_not_b\n",
        "\n",
        "        #   For each event, go through each feature. Increment the appropriate counter based on whether the word appears when B is true / not true\n",
        "        self.row_counter = 0\n",
        "        self.column_counter = 0\n",
        "        self.evaluate_A_feature = int\n",
        "        self.evaluate_B_feature = int\n",
        "\n",
        "        for row in X:\n",
        "            for value in row:\n",
        "                # The following rows setup the condition to search for\n",
        "                self.evaluate_A_feature = X[self.row_counter][self.column_counter]\n",
        "                self.evaluate_B_feature = y[self.row_counter][0]\n",
        "\n",
        "                if (self.evaluate_A_feature == 1 and self.evaluate_B_feature == 1):\n",
        "                    self.count_of_feature_A_when_b[self.column_counter] += 1\n",
        "                elif (self.evaluate_A_feature == 1 and self.evaluate_B_feature != 1):                    \n",
        "                    self.count_of_feature_A_when_not_b[self.column_counter] += 1\n",
        "                if (self.evaluate_A_feature == 1):\n",
        "                    self.word_in_event_counter[self.column_counter] += 1\n",
        "                self.column_counter += 1                \n",
        "            self.column_counter = 0\n",
        "            self.row_counter += 1\n",
        "\n",
        "        #   Having calculated the count of each identified feature, we can now calculate the probabilities of each feature as it appears in B, not B, and total\n",
        "        prob_of_feature_A_when_b = np.divide(self.count_of_feature_A_when_b, self.word_in_event_counter) # The number of times the A feature occurs given B / Number of events marked B\n",
        "        print(f\"prob_of_feature_A_when_b: {prob_of_feature_A_when_b}\")\n",
        "        prob_of_feature_A_when_not_b = np.divide(self.count_of_feature_A_when_not_b, self.word_in_event_counter) # The number of times the A feature occurs given B / Number of events marked B\n",
        "        print(f\"prob_of_feature_A_when_not_b: {prob_of_feature_A_when_not_b}\")\n",
        "\n",
        "        # # END: At this point, we have all the calculations required for what is necessary in the predict method\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "\n",
        "        import math\n",
        "\n",
        "        # The predict function accepts N events with M features to make a classification using the Naive Bayes implementation\n",
        "        # We want to multiply the existence of a feature (or lack thereof) by the probability that feature appears in the B (or not B) labeled training set\n",
        "\n",
        "        num_rows, num_cols = X.shape\n",
        "        y_predicted = np.full(num_rows, 0) \n",
        "\n",
        "        self.row_counter = 0\n",
        "        for row in X:\n",
        "            print(f\"X: {row}\")\n",
        "\n",
        "            # Using the numpy multiply operator we can multiple each 'cell' by the corresponding 'cell'\n",
        "            prob_of_feature_X_as_B_fit_weighted = np.multiply(row, prob_of_feature_A_when_b)\n",
        "            # print(f\"Predict: prob_of_feature_X_as_B_fit_weighted: {prob_of_feature_X_as_B_fit_weighted}\")\n",
        "            prob_of_feature_X_as_not_B_fit_weighted = np.multiply(row, prob_of_feature_A_when_not_b)\n",
        "            # print(f\"Predict: prob_of_feature_X_as_not_B_fit_weighted: {prob_of_feature_X_as_not_B_fit_weighted}\\n\")\n",
        "\n",
        "            # With the probabilities of each feature we can now add the log() scores together\n",
        "            #   Note:   It's common for the probabilities for some of the features to come back zero\n",
        "            #           However the np.log function breaks when trying to take the log(0) since there's no exponent that will get the base value to zero\n",
        "            #           Since we no longer care about the order of the values, we can np.sort(), then np.trim_zeros to get rid of leading or trailing zeros\n",
        "            #           before we take the log()        \n",
        "   \n",
        "            prob_of_feature_X_as_B_fit_weighted_log = np.sum(np.log(np.trim_zeros(np.sort(prob_of_feature_X_as_B_fit_weighted))))\n",
        "            prob_of_feature_X_as_not_B_fit_weighted_log = np.sum(np.log(np.trim_zeros(np.sort(prob_of_feature_X_as_not_B_fit_weighted))))\n",
        "\n",
        "            # print(f\"prob_of_feature_X_as_B_fit_weighted_log: {prob_of_feature_X_as_B_fit_weighted_log}\")\n",
        "            # print(f\"prob_of_feature_X_as_not_B_fit_weighted_log: {prob_of_feature_X_as_not_B_fit_weighted_log}\")\n",
        "\n",
        "            prob_of_B_given_A = 10**(prob_of_feature_X_as_B_fit_weighted_log + math.log10(p_of_b))\n",
        "            prob_of_not_B_given_A =  10**(prob_of_feature_X_as_not_B_fit_weighted_log + math.log10(p_of_not_b)) # ** is the operator for raising a value to that power\n",
        "\n",
        "            # print(f\"prob_of_B_given_A: {prob_of_B_given_A} vs prob_of_not_B_given_A: {prob_of_not_B_given_A}\")\n",
        "\n",
        "            if prob_of_B_given_A >= prob_of_not_B_given_A: \n",
        "                y_predicted[self.row_counter] = 1\n",
        "            else:\n",
        "                y_predicted[self.row_counter] = -1\n",
        "\n",
        "            self.row_counter += 1\n",
        "\n",
        "        print(f\"y_predicted: {y_predicted}\")\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "            # self.feature_counter = 0\n",
        "            # for cell in X:\n",
        "            #     prob_of_feature_X_fit_weighted = row[self.feature_counter] * prob_of_feature_A_when_b\n",
        "                # log_of_prob_of_feature_X_fit_weighted_= log(prob_of_feature_X_fit_weighted)\n",
        "                # print(f\"Cell: {row[self.feature_counter]} has prob {prob_of_feature_X_fit_weighted}, as a log: {log()}\") # Look at the first feature of the event\n",
        "                # rolling_total_log = rolling_total_log + log(prob_of_feature_X_fit_weighted)                \n",
        "\n",
        "\n",
        "        #         p_a = prob_of_feature_A # p_a is the probability of B for the total number of B\n",
        "        #         p_b_given_a = prob_of_word_in_spam[self.word_counter] #p_b_given_a is the probability of a given the message is spam \n",
        "        #         p_b_given_not_a = prob_of_word_in_ham[self.word_counter] # p_b_given_not_a is the probability of a given the message is ham\n",
        "        #         p_a_given_b_array[self.word_counter] = self.bayes_theorem(p_a, p_b_given_a, p_b_given_not_a) # calculate P(A|B)\n",
        "        #         rolling_total = rolling_total + np.log(p_a_given_b_array[self.word_counter])\n",
        "        #         print(f\"Rolling total: {rolling_total}\\n\")\n",
        "            #     self.feature_counter += 1\n",
        "            # self.row_counter += 1\n",
        "            \n",
        "        # # p_a_given_b = np.exp(np.log(p_of_b)+ rolling_total)\n",
        "        # print(f\"p_a_given_b: {p_a_given_b}\")        \n",
        "\n",
        "            # calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
        "    def bayes_theorem(self, p_a, p_b, p_b_given_a, p_b_given_not_a):\n",
        "\n",
        "        # calculate P(not A)\n",
        "        not_a = 1 - p_a\n",
        "        # calculate P(B)\n",
        "        p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
        "        # calculate P(A|B)\n",
        "        p_a_given_b = (p_b_given_a * p_a) / p_b\n",
        "        print(f\"Bayes rule p_a_given_b for event: {p_a_given_b} = ({p_b_given_a} * {p_a})/ {p_b}\")\n",
        "        return p_a_given_b\n",
        "\n",
        "        \n",
        "\n",
        "NBClassifier = BinaryNBClassifier()\n",
        "NBClassifier.fit(X_train, y_train)\n",
        "NBClassifier.predict(X_train)\n",
        "# NBClassifier.test_eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resources Consulted:\n",
        "* https://sklearn-template.readthedocs.io/en/latest/user_guide.html\n",
        "* https://saturncloud.io/blog/how-to-keep-column-names-when-converting-from-pandas-to-numpy\n",
        "* https://github.com/ApoorvRusia/Naive-Bayes-classification-on-Iris-dataset/blob/master/Naiye%20Bayes%20classification%20application.ipynb\n",
        "* https://datascience.stackexchange.com/questions/18904/how-do-i-convert-a-pandas-dataframe-to-a-1d-array\n",
        "* https://stackoverflow.com/questions/35996970/typeerror-fit-missing-1-required-positional-argument-y\n",
        "* https://machinelearningmastery.com/bayes-theorem-for-machine-learning/\n",
        "* https://news.ycombinator.com/item?id=21151032\n",
        "* https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics\n",
        "* **https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html**\n",
        "\n",
        "Notes for study from machinelearningmastery link above:\n",
        "The result P(A|B) is referred to as the posterior probability and P(A) is referred to as the prior probability.\n",
        "\n",
        "P(A|B): Posterior probability.\n",
        "P(A): Prior probability.\n",
        "Sometimes P(B|A) is referred to as the likelihood and P(B) is referred to as the evidence.\n",
        "\n",
        "P(B|A): Likelihood.\n",
        "P(B): Evidence.\n",
        "This allows Bayes Theorem to be restated as:\n",
        "\n",
        "Posterior = Likelihood * Prior / Evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of    0  1  2  3  4  5  6  7  8  9  Class\n",
              "0  1  0  0  0  1  1  0  1  1  1      1\n",
              "1  1  1  1  1  0  1  0  1  0  1      1\n",
              "2  0  1  0  0  1  1  1  1  0  1      1\n",
              "3  1  1  0  0  0  1  1  0  1  1      1\n",
              "4  0  0  0  1  0  1  1  0  0  1      1\n",
              "5  1  0  1  1  0  1  0  1  1  1     -1\n",
              "6  0  0  0  1  0  1  1  1  1  1     -1\n",
              "7  1  0  1  0  1  0  0  1  0  0     -1\n",
              "8  1  0  0  1  1  0  0  0  0  1     -1\n",
              "9  1  0  1  0  0  0  0  0  0  1     -1>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('Datasets for Assignment 2/test1_1.csv',)\n",
        "df.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
              "       [1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1, 1, 1, 0, 1],\n",
              "       [1, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 0, 1, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 0, 1, 1, 1, 1, 1],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
              "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The values for the Events are up to the last column\n",
        "\n",
        "X = df.iloc[:,:-1].values # The values are everything but the last column\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1],\n",
              "       [ 1],\n",
              "       [ 1],\n",
              "       [ 1],\n",
              "       [ 1],\n",
              "       [-1],\n",
              "       [-1],\n",
              "       [-1],\n",
              "       [-1],\n",
              "       [-1]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The values for the Classification are in the last column\n",
        "y = df.iloc[:,-1:].values\n",
        "y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell remains as a reminder of a LOT of work I did to try to maintain the words with their 'labels'. \n",
        "# Ultimately, it was fruitless because numpy handles arrays, not dictionaries.\n",
        "\n",
        "#Spliting the dataset in independent and dependent variables\n",
        "# X = df.iloc[:,:-1].to_dict('list') # The idea here is to capture all the columns except the last one as X\n",
        "# y = df['Class'].to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1.0, random_state = 42)\n",
        "\n",
        "# However for this particular exercise, the instruction is to use all the samples for training and testing\n",
        "\n",
        "X_train = X\n",
        "X_test = X\n",
        "y_train = y\n",
        "y_test = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'BinaryNBClassifier' object has no attribute 'num_of_A_events_when_B'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m NBClassifier \u001b[39m=\u001b[39m BinaryNBClassifier()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m NBClassifier\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m X:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m y[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow_counter] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_of_A_events_when_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_of_A_events_when_B \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39msum(X[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow_counter])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRow \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow_counter\u001b[39m}\u001b[39;00m\u001b[39m num_of_A_events_when_B running total is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_of_A_events_when_B\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39melif\u001b[39;00m y[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrow_counter] \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BinaryNBClassifier' object has no attribute 'num_of_A_events_when_B'"
          ]
        }
      ],
      "source": [
        "NBClassifier = BinaryNBClassifier()\n",
        "NBClassifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CategoricalNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CategoricalNB</label><div class=\"sk-toggleable__content\"><pre>CategoricalNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CategoricalNB()"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Train and test your BinaryNBClassifier and the sklearn CategoricalNBClassifier on the datasets from canvas\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "cclassifier = CategoricalNB()\n",
        "cclassifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1  1  1  1  1 -1  1 -1 -1 -1]\n"
          ]
        }
      ],
      "source": [
        "# Predicting the set results\n",
        "y_pred = cclassifier.predict(X_train)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 10",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#lets see the actual and predicted value side by side\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_compare \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack((y_train,y_pred))\u001b[39m.\u001b[39mT\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#actual value on the left side and predicted value on the right hand side\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#printing the top 5 values\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_compare[:]\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/core/shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    295\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 10"
          ]
        }
      ],
      "source": [
        "#lets see the actual and predicted value side by side\n",
        "y_compare = np.vstack((y_train,y_pred)).T\n",
        "#actual value on the left side and predicted value on the right hand side\n",
        "#printing the top 5 values\n",
        "y_compare[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4 1]\n",
            " [0 5]]\n"
          ]
        }
      ],
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_train, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct predictions:  9\n",
            "False predictions 1\n",
            "\n",
            "\n",
            "Accuracy of the Categorical Clasification is:  0.9\n"
          ]
        }
      ],
      "source": [
        "#finding accuracy from the confusion matrix.\n",
        "a = cm.shape\n",
        "corrPred = 0\n",
        "falsePred = 0\n",
        "\n",
        "for row in range(a[0]):\n",
        "    for c in range(a[1]):\n",
        "        if row == c:\n",
        "            corrPred +=cm[row,c]\n",
        "        else:\n",
        "            falsePred += cm[row,c]\n",
        "print('Correct predictions: ', corrPred)\n",
        "print('False predictions', falsePred)\n",
        "print ('\\n\\nAccuracy of the Categorical Clasification is: ', corrPred/(cm.sum()))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv24NS1RCpjT"
      },
      "source": [
        "Now you will train and test your Binary Naive Bayes classifier on a few different datasets.  The datasets can be downloaded from canvas.  They are linked in the assignment description.  For this part of the assignment we will not be splitting the data into training, validation and test data sets.  Instead you should use the entire dataset for training and the entire dataset for testing.  You will need to complete the following table (you can just output the results in this format you don't need to copy them into the text field).\n",
        "\n",
        "|dataset|# of instances|# of features | Your NB Training Time | Your NB Test Time | Your NB Accuracy | sklearn CategoricalNB Training Time | sklearn Categorical NB Test Time | sklearn CategoricalNB Accuracy|\n",
        "|-----------|------------|-------------|------------------|-------------------|-------------------------|---------------------------------|------------------------|----------------------------------|\n",
        "test1_1 |\n",
        "test1_2 |\n",
        "test1_4 |\n",
        "test1_5 |\n",
        "test2_1 |\n",
        "test2_2 |\n",
        "test2_4 |\n",
        "test2_5 |\n",
        "test4_1 |\n",
        "test4_2 |\n",
        "test4_4 |\n",
        "test4_5 |\n",
        "test5_1 |\n",
        "test5_2 |\n",
        "test5_4 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n6x9Ol9tE0OX"
      },
      "outputs": [],
      "source": [
        "#Train and test your BinaryNBClassifier and the sklearn CategoricalNBClassifier on the datasets from canvas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzzilfSNFGvY"
      },
      "source": [
        "The next step for this assignment is split the vote dataset (also found on canvas) into a *train/test split* (use 20% of the data for testing).  Train both algorithms on the training data using *cross-fold validation* and then report the accuracy, f1-score, mcc and informedness results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vi8VaK4V1z_E"
      },
      "outputs": [],
      "source": [
        "#Split the vote dataset\n",
        "#Use cross-validatation to compare BinaryNBClassifier against CategoricalNBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFP5ttuS2AtP"
      },
      "source": [
        "Finally, choose the algorithm that performed the best on the cross-validation, train it on all the training data and test on the test data.  Report the accuracy, f1-score, mcc and informedness results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UquA9oi22X-Q"
      },
      "outputs": [],
      "source": [
        "#Final Generalization test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
