{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gdxr-tZ4_JHf"
      },
      "outputs": [],
      "source": [
        "#initial imports that you may find useful\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1K_dSlP_UmU"
      },
      "source": [
        "For this assignment you will need to use the sklearn framework to implement a custom Naive Bayes classifier.  The classifier only needs to handle binary data (both the attributes and the classes).  The attributes will always have a value of 0 or 1.  The class labels will always have a value of 1 or -1.  You can use libraries to help with the data processing, calculations, etc, but you must implement your own Na√Øve Bayes algorithm.  Do not use an existing implementation.  One important implementation detail is that you should convert the probabilities to log probabilities to avoid the number becoming to small to represent as a floating point number.  For example instead of computing P(x|c)P(c) compute log(P(x|c)+log(P(c)). Provide your implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tFG6O-SBCOyH"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'split_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6600 (Machine Learning)/Assignments/Assignment2.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#W2sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# random.seed(0)      # just so you get the same answers as me\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#W2sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m train_data, test_data \u001b[39m=\u001b[39m split_data(data, \u001b[39m0.75\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m classifier \u001b[39m=\u001b[39m NaiveBayesClassifier()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206600%20%28Machine%20Learning%29/Assignments/Assignment2.ipynb#W2sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m classifier\u001b[39m.\u001b[39mtrain(train_data)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'split_data' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#Your Naive Bayes Implementation goes here.\n",
        "#Adjust this as you see fit\n",
        "\n",
        "class BinaryNBClassifier(BaseEstimator, ClassifierMixin):        \n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        return self\n",
        "\n",
        "    def train(self, training_set):\n",
        "\n",
        "        # count spam and non-spam messages\n",
        "        num_spams = len([is_spam\n",
        "                         for message, is_spam in training_set\n",
        "                         if is_spam])\n",
        "        num_non_spams = len(training_set) - num_spams\n",
        "\n",
        "        # run training data through our \"pipeline\"\n",
        "        word_counts = count_words(training_set)\n",
        "        self.word_probs = word_probabilities(word_counts,\n",
        "                                             num_spams,\n",
        "                                             num_non_spams,\n",
        "                                             self.k)\n",
        "\n",
        "    def classify(self, message):\n",
        "        return spam_probability(self.word_probs, message)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.random.randint(0, self.classes_.size,\n",
        "                                 size=X.shape[0])\n",
        "    \n",
        "    pass\n",
        "    \n",
        "def tokenize(message):\n",
        "    message = message.lower()                       # convert to lowercase\n",
        "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
        "    return set(all_words)                           # remove duplicates\n",
        "\n",
        "\n",
        "def count_words(training_set):\n",
        "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
        "    counts = defaultdict(lambda: [0, 0])\n",
        "    for message, is_spam in training_set:\n",
        "        for word in tokenize(message):\n",
        "            counts[word][0 if is_spam else 1] += 1\n",
        "    return counts\n",
        "\n",
        "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
        "    \"\"\"turn the word_counts into a list of triplets\n",
        "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
        "    return [(w,\n",
        "            (spam + k) / (total_spams + 2 * k),\n",
        "            (non_spam + k) / (total_non_spams + 2 * k))\n",
        "            for w, (spam, non_spam) in counts.iteritems()]\n",
        "\n",
        "def spam_probability(word_probs, message):\n",
        "    message_words = tokenize(message)\n",
        "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
        "\n",
        "    # iterate through each word in our vocabulary\n",
        "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
        "\n",
        "        # if *word* appears in the message,\n",
        "        # add the log probability of seeing it\n",
        "        if word in message_words:\n",
        "            log_prob_if_spam += math.log(prob_if_spam)\n",
        "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
        "\n",
        "        # if *word* doesn't appear in the message\n",
        "        # add the log probability of _not_ seeing it\n",
        "        # which is log(1 - probability of seeing it)\n",
        "        else:\n",
        "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
        "\n",
        "    prob_if_spam = math.exp(log_prob_if_spam)\n",
        "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
        "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
        "\n",
        "\n",
        "\n",
        "seed = np.random\n",
        "# random.seed(0)      # just so you get the same answers as me\n",
        "train_data, test_data = split_data(data, 0.75)\n",
        "\n",
        "classifier = NaiveBayesClassifier()\n",
        "classifier.train(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helpful Resources: \n",
        "* https://sklearn-template.readthedocs.io/en/latest/user_guide.html\n",
        "* https://saturncloud.io/blog/how-to-keep-column-names-when-converting-from-pandas-to-numpy\n",
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv24NS1RCpjT"
      },
      "source": [
        "Now you will train and test your Binary Naive Bayes classifier on a few different datasets.  The datasets can be downloaded from canvas.  They are linked in the assignment description.  For this part of the assignment we will not be splitting the data into training, validation and test data sets.  Instead you should use the entire dataset for training and the entire dataset for testing.  You will need to complete the following table (you can just output the results in this format you don't need to copy them into the text field).\n",
        "\n",
        "|dataset|# of instances|# of features | Your NB Training Time | Your NB Test Time | Your NB Accuracy | sklearn CategoricalNB Training Time | sklearn Categorical NB Test Time | sklearn CategoricalNB Accuracy|\n",
        "|-----------|------------|-------------|------------------|-------------------|-------------------------|---------------------------------|------------------------|----------------------------------|\n",
        "test1_1 |\n",
        "test1_2 |\n",
        "test1_4 |\n",
        "test1_5 |\n",
        "test2_1 |\n",
        "test2_2 |\n",
        "test2_4 |\n",
        "test2_5 |\n",
        "test4_1 |\n",
        "test4_2 |\n",
        "test4_4 |\n",
        "test4_5 |\n",
        "test5_1 |\n",
        "test5_2 |\n",
        "test5_4 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n6x9Ol9tE0OX"
      },
      "outputs": [],
      "source": [
        "#Train and test your BinaryNBClassifier and the sklearn CategoricalNBClassifier on the datasets from canvas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzzilfSNFGvY"
      },
      "source": [
        "The next step for this assignment is split the vote dataset (also found on canvas) into a *train/test split* (use 20% of the data for testing).  Train both algorithms on the training data using *cross-fold validation* and then report the accuracy, f1-score, mcc and informedness results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vi8VaK4V1z_E"
      },
      "outputs": [],
      "source": [
        "#Split the vote dataset\n",
        "#Use cross-validatation to compare BinaryNBClassifier against CategoricalNBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFP5ttuS2AtP"
      },
      "source": [
        "Finally, choose the algorithm that performed the best on the cross-validation, train it on all the training data and test on the test data.  Report the accuracy, f1-score, mcc and informedness results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UquA9oi22X-Q"
      },
      "outputs": [],
      "source": [
        "#Final Generalization test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
