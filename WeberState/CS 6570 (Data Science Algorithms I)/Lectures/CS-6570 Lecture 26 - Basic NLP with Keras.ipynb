{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a351f259-11d5-4140-82dd-5edd6064406b",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 26 - NLP with Keras\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3647c8-dac2-4dee-9e60-548eccd78d3b",
   "metadata": {},
   "source": [
    "Today's lecture will build off our previous one. We'll explore in greater depth techniques for representing and building models from sets of words, and then talk briefly about building more powerful models from sequences of words. We'll also see how we can use Keras and Tensorflow in the construction of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cae6c-d27b-42bd-be47-7a16826e1135",
   "metadata": {},
   "source": [
    "How a machine learning model should represent *individual words* is relatively straightforward: they're categorical features. The more difficult question is how to encode the way words are woven into sentences. In other words, *word order*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a0d98-1aa3-4e46-9f8e-3e37613ffef9",
   "metadata": {},
   "source": [
    "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don't have a natural, canonical order. Different languages order words in very different ways, or even within the same language there can be very different orderings of the same words with essentially the same meaning. Order is clearly important, but its relationship to meaning isn't straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54de2-a24f-448a-9eb4-749e57bb29a8",
   "metadata": {},
   "source": [
    "Historically, most early applications of machine learning to NLP just involved models that didn't take word order into account. These are known as bag-of-words models. Interest in models that incorporate word order - sequence models - only started rising in 2015 with the rebirth of recurrent neural networks (RNNs) and then in 2017 with the advent of transformers - transformers are now the only game in town."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906a052-4927-4288-b46a-8a71bf75aba7",
   "metadata": {},
   "source": [
    "But first, let's import our favorite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0cdf17-afb1-4840-87a6-911a8f086fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e46009-e6e8-4d4f-92b4-205908e40d0e",
   "metadata": {},
   "source": [
    "We'll also want the following text vectorization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb512be8-0f3d-4d76-a252-2c2af80ae3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2068ead-5d10-4396-ada9-0b9e74d1e916",
   "metadata": {},
   "source": [
    "**Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe27f96-ec4e-4724-80de-44e9ffab0968",
   "metadata": {},
   "source": [
    "We'll start with some bag-of-words models. In particular, we'll explore some bag-of-words models using the IMDB review dataset you may recognize from Assignment 6. The modeling goal here is to classify reviews as either positive or negative. The reviews are very obviously one or the other.\n",
    "\n",
    "This dataset is available from Keras, and can be downloaded from Keras as in Assignment 6. However, we'll want a more raw form of the data for the purposes of this lecture, and so we'll download the data directly from the source. The code below should download it for you, but you should only need to run it once. So, it's commented out on this notebook, but if it's your first time through, you'll want to remove the comment lines at the beginning and end and then run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44098736-1fb9-4548-a48c-bdddec374304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b008168-2d0c-48d5-8cd8-f6b7436b503f",
   "metadata": {},
   "source": [
    "Now we'll read in the data to create a training dataset, a validation dataset, and a test dataset. We'll use the Keras utility *text_dataset_from_directory* to do so. Don't worry about the specifics of this utility - that's not important for our discussion. Just know we're creating a training, validation, and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99b324d-c079-46ee-a0b5-6c20edeb6359",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory aclImdb/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6570 (Data Science Algorithms I)/Lectures/CS-6570 Lecture 26 - Basic NLP with Keras.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_ds \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mtext_dataset_from_directory(\u001b[39m\"\u001b[39;49m\u001b[39maclImdb/train\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m val_ds \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mtext_dataset_from_directory(\u001b[39m\"\u001b[39m\u001b[39maclImdb/val\u001b[39m\u001b[39m\"\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_ds \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mtext_dataset_from_directory(\u001b[39m\"\u001b[39m\u001b[39maclImdb/test\u001b[39m\u001b[39m\"\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/text_dataset.py:161\u001b[0m, in \u001b[0;36mtext_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1e6\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m file_paths, labels, class_names \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39;49mindex_directory(\n\u001b[1;32m    162\u001b[0m     directory,\n\u001b[1;32m    163\u001b[0m     labels,\n\u001b[1;32m    164\u001b[0m     formats\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m    165\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m    166\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    167\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    168\u001b[0m     follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m label_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(class_names) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mWhen passing `label_mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`, there must be exactly 2 \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclass_names. Received: class_names=\u001b[39m\u001b[39m{\u001b[39;00mclass_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m     subdirs \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[1;32m    543\u001b[0m         \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    544\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m subdir\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[39mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find directory \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[39m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[39m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m _pywrap_file_io\u001b[39m.\u001b[39mGetChildren(compat\u001b[39m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory aclImdb/train"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe460a-2592-49a5-aa44-6afbc6c76424",
   "metadata": {},
   "source": [
    "We can check out what our training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f332f5be-f41e-4880-b4e8-bb5bcaac6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'I have never seen a movie as bad as this. It is meant to be a \"fun\" movie, but the only joke is at the start, and it is NOT funny. If you like this sort of movie, then you may just be able to give it a vote of 2. If it had the necessary votes, it would truly belong on the bottom 100.<br /><br />', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053a38a-0b86-4c37-941b-ddfae492a1e4",
   "metadata": {},
   "source": [
    "We see there are 32 batches in both the inputs and targets. The inputs are strings, and the targets are numbers (the numbers 0 and 1, to be specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde8fb3-6151-4291-a763-d8283f2ec959",
   "metadata": {},
   "source": [
    "With a bag-of-words model you can represent an entire text as a single vector. Each entry in the vector indicates the presence of a given word within the text. This provides a single vector with $0$s almost everywhere and some $1$s for those words that are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588f155-0f2a-4ae0-b222-3f92f4ebf2e8",
   "metadata": {},
   "source": [
    "First, we'll prepare our dataset so it only yields raw text inputs (no labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0415053-9e19-4568-8614-3ae45410fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ccbe28-8f6d-4171-a642-29f6e82c6377",
   "metadata": {},
   "source": [
    "Next, we'll limit the vocabulary to only the 20,000 most common words. Then we'll use the *adapt* method to index the vocabulary. Then, we'll create our processed training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8604ef-4f02-4cf2-8074-b23c4c2439fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(max_tokens = 20000, output_mode=\"multi_hot\")\n",
    "text_vectorization.adapt(text_only_train_ds) # Parses all the text in the dataset and builds the 20,000 words and the mapping we'll apply to it\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e6703-8504-4a25-b68b-d5c31e296e86",
   "metadata": {},
   "source": [
    "We can now take a look at what our transformed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f73b9e7-4214-49be-a718-42274044faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc62a4a-21d6-4b91-aa8d-7aeb49340cf8",
   "metadata": {},
   "source": [
    "Each review is now a 20,000-dimension vector of $1$s and $0$s. Mostly $0$s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdaf8f1-12b9-49fd-9600-67bbd358b1fd",
   "metadata": {},
   "source": [
    "Now, we'll create a simple, single-layer dense neural network for making our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a869d6-6edd-4eb8-a8d4-ec230c7ef814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens = 20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = keras.layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb788500-3569-4936-b5bc-01da64346a00",
   "metadata": {},
   "source": [
    "Let's run our model on our data, and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd21a094-0e6c-4704-bd0e-aba39caa8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3352 - accuracy: 0.8647 - val_loss: 0.2655 - val_accuracy: 0.8906\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1952 - accuracy: 0.9262 - val_loss: 0.2768 - val_accuracy: 0.8880\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1520 - accuracy: 0.9445 - val_loss: 0.3003 - val_accuracy: 0.8874\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.3264 - val_accuracy: 0.8852\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1045 - accuracy: 0.9640 - val_loss: 0.3562 - val_accuracy: 0.8828\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0880 - accuracy: 0.9703 - val_loss: 0.3869 - val_accuracy: 0.8794\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0743 - accuracy: 0.9744 - val_loss: 0.4185 - val_accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0628 - accuracy: 0.9789 - val_loss: 0.4519 - val_accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0528 - accuracy: 0.9829 - val_loss: 0.4872 - val_accuracy: 0.8722\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0441 - accuracy: 0.9858 - val_loss: 0.5261 - val_accuracy: 0.8700\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5721 - accuracy: 0.8587\n",
      "Test acc: 0.859\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(), #We call cache() to put the data in memory, so we only need to do the preprossing once, during the first epoch.\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab345e23-c6bf-4054-a5d3-67ae62d5ece2",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf72962-aa16-4c4a-ab2f-6c79c4405d0b",
   "metadata": {},
   "source": [
    "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words. For example, the term \"United States\" conveys a concept that is quite distinct from the meaning of the words \"states\" or \"united\" taken separately. For this reason, we can probably do a little better if we have some local order information. We do this using \"N-grams\", which are sequential combinations of $N$ words. Frequently $N = 2$, and these are called \"bigrams\".\n",
    "\n",
    "We can use N-grams just as easily as we used individual words. We just add it as an argument to the *TextVectorization* call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1e7de2-5598-4ce9-b8ff-9c89461b5bc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_only_train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/robchristiansen/Documents/Code/Learning/WeberState/CS 6570 (Data Science Algorithms I)/Lectures/CS-6570 Lecture 26 - Basic NLP with Keras.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_vectorization \u001b[39m=\u001b[39m TextVectorization(ngrams\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m20000\u001b[39m, output_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulti_hot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text_vectorization\u001b[39m.\u001b[39madapt(text_only_train_ds)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m binary_2gram_train_ds \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (text_vectorization(x), y))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robchristiansen/Documents/Code/Learning/WeberState/CS%206570%20%28Data%20Science%20Algorithms%20I%29/Lectures/CS-6570%20Lecture%2026%20-%20Basic%20NLP%20with%20Keras.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m binary_2gram_val_ds \u001b[39m=\u001b[39m val_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (text_vectorization(x), y))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_only_train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"multi_hot\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a1875-83c2-4bea-a5bf-31ec6d39aad5",
   "metadata": {},
   "source": [
    "Do bigrams do better than individual words? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58fceff7-8d37-4c8b-b734-077b5436dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3077 - accuracy: 0.8721 - val_loss: 0.2562 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1499 - accuracy: 0.9463 - val_loss: 0.2769 - val_accuracy: 0.8964\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.3172 - val_accuracy: 0.8916\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9794 - val_loss: 0.3667 - val_accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0389 - accuracy: 0.9883 - val_loss: 0.4258 - val_accuracy: 0.8856\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0234 - accuracy: 0.9929 - val_loss: 0.4888 - val_accuracy: 0.8848\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.5588 - val_accuracy: 0.8828\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.6285 - val_accuracy: 0.8818\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.7065 - val_accuracy: 0.8802\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.7837 - val_accuracy: 0.8798\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8059 - accuracy: 0.8784\n",
      "Test acc: 0.878\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02b277-7658-4dac-a121-57bc953c1169",
   "metadata": {},
   "source": [
    "A bit better than the test accuracy we saw for single words. I'll take it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366169c-1304-4d7a-a60f-67599b7f5981",
   "metadata": {},
   "source": [
    "You can also add a bit more information to a bag-of-words representation by counting how many times each word or N-gram occurs. You can do this with \"output_mode=count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98fe62ae-9bb0-48ec-b83d-20604a2b2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams = 2, max_tokens=20000, output_mode=\"count\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "count_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "count_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "count_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f78e64e-9e27-43ad-9fa2-ca952fa65825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3491 - accuracy: 0.8596 - val_loss: 0.3358 - val_accuracy: 0.8760\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1869 - accuracy: 0.9300 - val_loss: 0.3081 - val_accuracy: 0.8884\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1286 - accuracy: 0.9543 - val_loss: 0.3162 - val_accuracy: 0.8906\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0906 - accuracy: 0.9707 - val_loss: 0.3443 - val_accuracy: 0.8892\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9788 - val_loss: 0.3789 - val_accuracy: 0.8916\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.4122 - val_accuracy: 0.8876\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.4598 - val_accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0188 - accuracy: 0.9944 - val_loss: 0.5048 - val_accuracy: 0.8846\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.5665 - val_accuracy: 0.8852\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.6180 - val_accuracy: 0.8834\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6206 - accuracy: 0.8792\n",
      "Test acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(count_2gram_train_ds.cache(),\n",
    "          validation_data=count_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"count_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(count_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c9e9f-06fa-4392-a629-7e714f26488c",
   "metadata": {},
   "source": [
    "Not much improvement. However, for larger corpuses of text this wouldn't likely be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cb725-e865-4efd-b8ed-6415a9ed516f",
   "metadata": {},
   "source": [
    "Now, of course, some words are bound to occur more often than other no matter what the text is about. Words like \"the\", \"and\", \"is\", \"it\", and the like will almost always dominate your wordcount histograms. However, they're pretty useless features in a classification context. How can this be addressed?\n",
    "\n",
    "Well, we can normalize the frequency using the tf-idf measure we introduced in the last lecture. In fact, it's one of the options for \"output_mode\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67503b85-31f4-451e-9879-480dea075649",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams = 2, max_tokens = 20000, output_mode = \"tf_idf\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45e656b1-379d-4489-9036-54207a0fa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3764 - accuracy: 0.8468 - val_loss: 0.2632 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1731 - accuracy: 0.9377 - val_loss: 0.2851 - val_accuracy: 0.8970\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1135 - accuracy: 0.9622 - val_loss: 0.3239 - val_accuracy: 0.8962\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9775 - val_loss: 0.3863 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0470 - accuracy: 0.9857 - val_loss: 0.5015 - val_accuracy: 0.8816\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0390 - accuracy: 0.9882 - val_loss: 0.5138 - val_accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.5788 - val_accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.6504 - val_accuracy: 0.8878\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.7053 - val_accuracy: 0.8858\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.7844 - val_accuracy: 0.8872\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7955 - accuracy: 0.8726\n",
      "Test acc: 0.873\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f5a12-a86a-495a-988a-2053cda5947e",
   "metadata": {},
   "source": [
    "Again, not a huge improvement here, but we'd probably see some gain on larger corpuses of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66754a-10a1-416b-ab2c-4c8ad7a109ac",
   "metadata": {},
   "source": [
    "**Sequential Models**\n",
    "\n",
    "As you might guess, there's more to be set about how we can take advantage of sequences of words. In fact, there's *much* more to be said. Too much to do justice to in this lecture. So, I'll just mention a few things, and encourage you to take an NLP or deep learning class, where they'll probably get into these in more depth.\n",
    "\n",
    "* Recurrent neural networks (RNNs) are the type of neural network that you use for time series. Or, more broadly, for sequential data. As you might imagine, they can be useful for NLP. In fact, around 2016-2017 bidirectional RNNs (in particular, bidirectional LSTMs) were considered state of the art.\n",
    "\n",
    "* In 2017 one of the most influential papers in the history of machine learning, titled \"Attention Is All You Need\", was published, and it introduced \"transformer\" models. One of the great applications of transformer models was NLP, and today NLP is almost universally done with transformers. It's a pretty cool subject.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
