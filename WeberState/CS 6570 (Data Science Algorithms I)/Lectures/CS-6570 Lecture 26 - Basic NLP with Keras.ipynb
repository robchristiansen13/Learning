{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a351f259-11d5-4140-82dd-5edd6064406b",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 26 - NLP with Keras\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3647c8-dac2-4dee-9e60-548eccd78d3b",
   "metadata": {},
   "source": [
    "Today's lecture will build off our previous one. We'll explore in greater depth techniques for representing and building models from sets of words, and then talk briefly about building more powerful models from sequences of words. We'll also see how we can use Keras and Tensorflow in the construction of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cae6c-d27b-42bd-be47-7a16826e1135",
   "metadata": {},
   "source": [
    "How a machine learning model should represent *individual words* is relatively straightforward: they're categorical features. The more difficult question is how to encode the way words are woven into sentences. In other words, *word order*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a0d98-1aa3-4e46-9f8e-3e37613ffef9",
   "metadata": {},
   "source": [
    "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don't have a natural, canonical order. Different languages order words in very different ways, or even within the same language there can be very different orderings of the same words with essentially the same meaning. Order is clearly important, but its relationship to meaning isn't straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54de2-a24f-448a-9eb4-749e57bb29a8",
   "metadata": {},
   "source": [
    "Historically, most early applications of machine learning to NLP just involved models that didn't take word order into account. These are known as bag-of-words models. Interest in models that incorporate word order - sequence models - only started rising in 2015 with the rebirth of recurrent neural networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906a052-4927-4288-b46a-8a71bf75aba7",
   "metadata": {},
   "source": [
    "But first, let's import our favorite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0cdf17-afb1-4840-87a6-911a8f086fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 23:27:27.330005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 23:27:27.330033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 23:27:27.331458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 23:27:27.339492: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 23:27:29.009276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e46009-e6e8-4d4f-92b4-205908e40d0e",
   "metadata": {},
   "source": [
    "We'll also want the following text vectorization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb512be8-0f3d-4d76-a252-2c2af80ae3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2068ead-5d10-4396-ada9-0b9e74d1e916",
   "metadata": {},
   "source": [
    "**Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe27f96-ec4e-4724-80de-44e9ffab0968",
   "metadata": {},
   "source": [
    "We'll start with some bag-of-words models. In particular, we'll explore some bag-of-words models using the IMDB review dataset you may recognize from Assignment 6. The modeling goal here is to classify reviews as either positive or negative. The reviews are very obviously one or the other.\n",
    "\n",
    "This dataset is available from Keras, and can be downloaded from Keras as in Assignment 6. However, we'll want a more raw form of the data for the purposes of this lecture, and so we'll download the data directly from the source. The code below should download it for you, but you should only need to run it once. So, it's commented out on this notebook, but if it's your first time through, you'll want to remove the comment lines at the beginning and end and then run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44098736-1fb9-4548-a48c-bdddec374304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b008168-2d0c-48d5-8cd8-f6b7436b503f",
   "metadata": {},
   "source": [
    "Now we'll read in the data to create a training dataset, a validation dataset, and a test dataset. We'll use the Keras utility *text_dataset_from_directory* to do so. Don't worry about the specifics of this utility - that's not important for our discussion. Just know we're creating a training, validation, and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e99b324d-c079-46ee-a0b5-6c20edeb6359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe460a-2592-49a5-aa44-6afbc6c76424",
   "metadata": {},
   "source": [
    "We can check out what our training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f332f5be-f41e-4880-b4e8-bb5bcaac6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'I have never seen a movie as bad as this. It is meant to be a \"fun\" movie, but the only joke is at the start, and it is NOT funny. If you like this sort of movie, then you may just be able to give it a vote of 2. If it had the necessary votes, it would truly belong on the bottom 100.<br /><br />', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053a38a-0b86-4c37-941b-ddfae492a1e4",
   "metadata": {},
   "source": [
    "We see there are 32 batches in both the inputs and targets. The inputs are strings, and the targets are numbers (the numbers 0 and 1, to be specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde8fb3-6151-4291-a763-d8283f2ec959",
   "metadata": {},
   "source": [
    "With a bag-of-words model you can represent an entire text as a single vector. Each entry in the vector indicates the presence of a given word within the text. This provides a single vector with $0$s almost everywhere and some $1$s for those words that are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588f155-0f2a-4ae0-b222-3f92f4ebf2e8",
   "metadata": {},
   "source": [
    "First, we'll prepare our dataset so it only yields raw text inputs (no labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0415053-9e19-4568-8614-3ae45410fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ccbe28-8f6d-4171-a642-29f6e82c6377",
   "metadata": {},
   "source": [
    "Next, we'll limit the vocabulary to only the 20,000 most common words. Then we'll use the *adapt* method to index the vocabulary. Then, we'll create our processed training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8604ef-4f02-4cf2-8074-b23c4c2439fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(max_tokens = 20000, output_mode=\"multi_hot\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e6703-8504-4a25-b68b-d5c31e296e86",
   "metadata": {},
   "source": [
    "We can now take a look at what our transformed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f73b9e7-4214-49be-a718-42274044faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc62a4a-21d6-4b91-aa8d-7aeb49340cf8",
   "metadata": {},
   "source": [
    "Each review is now a 20,000-dimension vector of $1$s and $0$s. Mostly $0$s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdaf8f1-12b9-49fd-9600-67bbd358b1fd",
   "metadata": {},
   "source": [
    "Now, we'll create a simple, single-layer dense neural network for making our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a869d6-6edd-4eb8-a8d4-ec230c7ef814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens = 20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = keras.layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb788500-3569-4936-b5bc-01da64346a00",
   "metadata": {},
   "source": [
    "Let's run our model on our data, and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd21a094-0e6c-4704-bd0e-aba39caa8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3352 - accuracy: 0.8647 - val_loss: 0.2655 - val_accuracy: 0.8906\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1952 - accuracy: 0.9262 - val_loss: 0.2768 - val_accuracy: 0.8880\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1520 - accuracy: 0.9445 - val_loss: 0.3003 - val_accuracy: 0.8874\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.3264 - val_accuracy: 0.8852\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1045 - accuracy: 0.9640 - val_loss: 0.3562 - val_accuracy: 0.8828\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0880 - accuracy: 0.9703 - val_loss: 0.3869 - val_accuracy: 0.8794\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0743 - accuracy: 0.9744 - val_loss: 0.4185 - val_accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0628 - accuracy: 0.9789 - val_loss: 0.4519 - val_accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0528 - accuracy: 0.9829 - val_loss: 0.4872 - val_accuracy: 0.8722\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0441 - accuracy: 0.9858 - val_loss: 0.5261 - val_accuracy: 0.8700\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5721 - accuracy: 0.8587\n",
      "Test acc: 0.859\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(), #We call cache() to put the data in memory, so we only need to do the preprossing once, during the first epoch.\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab345e23-c6bf-4054-a5d3-67ae62d5ece2",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf72962-aa16-4c4a-ab2f-6c79c4405d0b",
   "metadata": {},
   "source": [
    "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words. For example, the term \"United States\" conveys a concept that is quite distinct from the meaning of the words \"states\" or \"united\" taken separately. For this reason, we can probably do a little better if we have some local order information. We do this using \"N-grams\", which are sequential combinations of $N$ words. Frequently $N = 2$, and these are called \"bigrams\".\n",
    "\n",
    "We can use N-grams just as easily as we used individual words. We just add it as an argument to the *TextVectorization* call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a1e7de2-5598-4ce9-b8ff-9c89461b5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"multi_hot\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a1875-83c2-4bea-a5bf-31ec6d39aad5",
   "metadata": {},
   "source": [
    "Do bigrams do better than individual words? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58fceff7-8d37-4c8b-b734-077b5436dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3077 - accuracy: 0.8721 - val_loss: 0.2562 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1499 - accuracy: 0.9463 - val_loss: 0.2769 - val_accuracy: 0.8964\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.3172 - val_accuracy: 0.8916\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0620 - accuracy: 0.9794 - val_loss: 0.3667 - val_accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0389 - accuracy: 0.9883 - val_loss: 0.4258 - val_accuracy: 0.8856\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0234 - accuracy: 0.9929 - val_loss: 0.4888 - val_accuracy: 0.8848\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.5588 - val_accuracy: 0.8828\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.6285 - val_accuracy: 0.8818\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.7065 - val_accuracy: 0.8802\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.7837 - val_accuracy: 0.8798\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8059 - accuracy: 0.8784\n",
      "Test acc: 0.878\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02b277-7658-4dac-a121-57bc953c1169",
   "metadata": {},
   "source": [
    "A bit better than the test accuracy we saw for single words. I'll take it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366169c-1304-4d7a-a60f-67599b7f5981",
   "metadata": {},
   "source": [
    "You can also add a bit more information to a bag-of-words representation by counting how many times each word or N-gram occurs. You can do this with \"output_mode=count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98fe62ae-9bb0-48ec-b83d-20604a2b2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams = 2, max_tokens=20000, output_mode=\"count\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "count_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "count_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "count_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f78e64e-9e27-43ad-9fa2-ca952fa65825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3491 - accuracy: 0.8596 - val_loss: 0.3358 - val_accuracy: 0.8760\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1869 - accuracy: 0.9300 - val_loss: 0.3081 - val_accuracy: 0.8884\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1286 - accuracy: 0.9543 - val_loss: 0.3162 - val_accuracy: 0.8906\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0906 - accuracy: 0.9707 - val_loss: 0.3443 - val_accuracy: 0.8892\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0646 - accuracy: 0.9788 - val_loss: 0.3789 - val_accuracy: 0.8916\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.4122 - val_accuracy: 0.8876\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.4598 - val_accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0188 - accuracy: 0.9944 - val_loss: 0.5048 - val_accuracy: 0.8846\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.5665 - val_accuracy: 0.8852\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.6180 - val_accuracy: 0.8834\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6206 - accuracy: 0.8792\n",
      "Test acc: 0.879\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(count_2gram_train_ds.cache(),\n",
    "          validation_data=count_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"count_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(count_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c9e9f-06fa-4392-a629-7e714f26488c",
   "metadata": {},
   "source": [
    "Not much improvement. However, for larger corpuses of text this wouldn't likely be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cb725-e865-4efd-b8ed-6415a9ed516f",
   "metadata": {},
   "source": [
    "Now, of course, some words are bound to occur more often than other no matter what the text is about. Words like \"the\", \"and\", \"is\", \"it\", and the like will almost always dominate your wordcount histograms. However, they're pretty useless features in a classification context. How can this be addressed?\n",
    "\n",
    "Well, we can normalize the frequency using the tf-idf measure we introduced in the last lecture. In fact, it's one of the options for \"output_mode\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67503b85-31f4-451e-9879-480dea075649",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams = 2, max_tokens = 20000, output_mode = \"tf_idf\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45e656b1-379d-4489-9036-54207a0fa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3764 - accuracy: 0.8468 - val_loss: 0.2632 - val_accuracy: 0.8980\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1731 - accuracy: 0.9377 - val_loss: 0.2851 - val_accuracy: 0.8970\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1135 - accuracy: 0.9622 - val_loss: 0.3239 - val_accuracy: 0.8962\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9775 - val_loss: 0.3863 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0470 - accuracy: 0.9857 - val_loss: 0.5015 - val_accuracy: 0.8816\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0390 - accuracy: 0.9882 - val_loss: 0.5138 - val_accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.5788 - val_accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.6504 - val_accuracy: 0.8878\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.7053 - val_accuracy: 0.8858\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.7844 - val_accuracy: 0.8872\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7955 - accuracy: 0.8726\n",
      "Test acc: 0.873\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10)\n",
    "#model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f5a12-a86a-495a-988a-2053cda5947e",
   "metadata": {},
   "source": [
    "Again, not a huge improvement here, but we'd probably see some gain on larger corpuses of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66754a-10a1-416b-ab2c-4c8ad7a109ac",
   "metadata": {},
   "source": [
    "**Sequential Models**\n",
    "\n",
    "As you might guess, there's more to be set about how we can take advantage of sequences of words. In fact, there's *much* more to be said. Too much to do justice to in this lecture. So, I'll just mention a few things, and encourage you to take an NLP or deep learning class, where they'll probably get into these in more depth.\n",
    "\n",
    "* Recurrent neural networks (RNNs) are the type of neural network that you use for time series. Or, more broadly, for sequential data. As you might imagine, they can be useful for NLP. In fact, around 2016-2017 bidirectional RNNs (in particular, bidirectional LSTMs) were considered state of the art.\n",
    "\n",
    "* In 2017 one of the most influential papers in the history of machine learning, titled \"Attention Is All You Need\", was published, and it introduced \"transformer\" models. One of the great applications of transformer models was NLP, and today NLP is almost universally done with transformers. It's a pretty cool subject.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
