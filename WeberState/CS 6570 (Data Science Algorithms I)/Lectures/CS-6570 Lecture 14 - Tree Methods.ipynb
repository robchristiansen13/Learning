{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e452f2",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 14 - Tree Methods\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c8927-5e1d-4866-9b2f-c372bceb0d8c",
   "metadata": {},
   "source": [
    "Today, we're going to start class with a game. Specifically, I'm going to ask one of you to pick a number between 1 and 1,000,000, and I'm going to be able to guess *exactly* which number it is through a series of 20 questions. Let's do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73094a-e753-412a-8145-600dfb930931",
   "metadata": {},
   "source": [
    "That I was able to do this depends a great deal on the nature of the questions that I asked. Specifically, I designed them to provide the greatest amount of *information gain*, given the amount of information I'd learned so far. This method was, in fact, a decision tree, which is the subject of our class today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5519d-8c9b-421d-9604-4e28bfef6366",
   "metadata": {},
   "source": [
    "Decision trees are classification methods that involve the stratification or segmenting of the predictor space into simple regions. The predicted value of an observation is then just the average value or the most common value of the training data within that observation's region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55de7f6-39cf-4601-af34-952fce897e02",
   "metadata": {},
   "source": [
    "Tree-based methods are simple and useful for interpretation, however they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy. Sometimes this is OK, but the real power of these methods comes when multiple trees are combined to yield a consensus prediction. We'll get into that later. First, let's look at the two major types of decision trees: regression trees, which are used for making numeric predictions, and classification trees, which are used for making categorical predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fff4b-f5fe-408f-a0b2-7e49572a3898",
   "metadata": {},
   "source": [
    "But, of course, first thing we want to do is import our favorite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a7b81-3ac9-4354-9db9-a48d5089b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b5e54-77a3-4c7f-ae74-2c9b49f55e32",
   "metadata": {},
   "source": [
    "**Regression Trees**\n",
    "\n",
    "To build a regression tree, roughly speaking there are two steps:\n",
    "\n",
    "1. Divide the predictor space - the set of possible values for the input variables $X_{1}, X_{2}, \\ldots, X_{p}$ into $J$ distinct and non-overlapping regions, $R_{1}, R_{2}, \\ldots, R_{J}$.\n",
    "\n",
    "2. For every observation that falls into the region $R_{J}$, make the same prediction, which is just the mean of the responses for the training observations in $R_{J}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551de6b-093b-474e-bb60-3e04bcf5c8f9",
   "metadata": {},
   "source": [
    "![Decision Tree Diagram](Decision_Tree_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b802a3-71ac-43ca-af6a-75080e53d257",
   "metadata": {},
   "source": [
    "How do we construct these regions? Well, the goal is to find boxes $R_{1}, R_{2}, \\ldots, R_{J}$ that (of course!) minimize the RSS, where the RSS is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4d7d3-01d0-40d5-b88b-d1d9eb36ffdf",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $\\displaystyle \\sum_{j = 1}^{J}\\sum_{r \\in R_{j}}(y_{r} - \\hat{y}_{R_{j}})^{2}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6321d91-6376-4d8e-a39d-530cb08f6fde",
   "metadata": {},
   "source": [
    "where $\\hat{y}_{R_{j}}$ is the mean response of the training observations within the $j$th box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c01db1-8c07-49ad-8408-60e99424e817",
   "metadata": {},
   "source": [
    "Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into $J$ boxes. For this reason, we take a *top-down, greedy* approach that is known as *recursive binary splitting*. It's top-down because it begins at the top of the tree, and then successively splits the predictor space. It's greedy because at each step of the tree-building process, the *best* split is made at that particular step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e94404-d8c7-456c-bb24-76e7f7f5461f",
   "metadata": {},
   "source": [
    "So, for example, to split the original region $R$ into two regions along the input variable $X_{j}$ we would create two regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e9ae5-c1ea-43c6-a13b-59ca645e92c7",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $R_{1}(s) = \\{X|X_{j} < s\\}$ and $R_{2}(s) = \\{X|X_{j} \\geq s\\}$,\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c118b-9ba1-48a2-b3ed-8b4ae2ddba73",
   "metadata": {},
   "source": [
    "such that $s$ minimizes the the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4f3ca-3d23-48d7-b92f-3de48f324fbd",
   "metadata": {},
   "source": [
    "<center>\n",
    "    $\\displaystyle \\sum_{x_{i} \\in R_{1}(s)}(y_{i}-\\hat{y}_{R_{1}})^{2} + \\sum_{x_{j} \\in R_{2}(s)}(y_{j} - \\hat{y}_{R_{2}})^{2}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e6d5f-8ef1-47e2-8a5c-b0d0ac94f056",
   "metadata": {},
   "source": [
    "The splitting begins with the entire region $R$, and finds the variable $X_{j}$ that provides the best split in terms of minimizing $RSS$. The algorithm then makes this split. Now there are two regions, and the algorithm finds the region and variable that provides the best split again in terms of minimizing $RSS$. It repeats this process until some stopping point is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f862a-fb51-4474-be6b-feeb34064e9e",
   "metadata": {},
   "source": [
    "This process might produce good predictions on the training data, but it is very likely to overfit, particularly if your stopping point is rather finely determined. One way to prevent this is to have a rather early stopping criteria, but in practice this tends to miss too much. Instead, it's usually more effective to build a complicated decision tree, and then *prune* it back to obtain a subtree. In other words, we take our complicated tree $T_{0}$ and try to find a simpler subtree that still does a good job predicting the training data. The number of possible subtrees is typically extremely large, so it's not really possible to test them all - we need another method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6079b-0a31-47fa-9ced-be230fe0c267",
   "metadata": {},
   "source": [
    "A common pruning method, which is similar to the Lasso regression method we studied earlier, is *cost complexity pruning* (also known as *weakest link pruning*). Under this approach, instead of considering every possible sub-tree, we consider a sequence of trees indexed by a non-negative tuning parameter $\\alpha$. For each value of $\\alpha$ there is a sub-tree $T \\subset T_{0}$ such that\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle \\sum_{m = 1}^{|T|}\\sum_{x_{i} \\in R_{m}}(y_{i} - \\hat{y}_{R_{m}})^{2} + \\alpha|T|$\n",
    "</center>\n",
    "\n",
    "is minimized. Here $|T|$ indicates the number of terminal nodes of the tree $T$. It turns out as we increase $\\alpha$, starting from $0$, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of sub-trees as a function of $\\alpha$ is easy. The value $\\alpha$ is a hyperparameter that can be found using a validation set or cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce5293-6b70-4b25-9222-52bfc764f9ac",
   "metadata": {},
   "source": [
    "**Classification Trees**\n",
    "\n",
    "A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. For a classification tree, instead of predicting the mean value within a region, the tree predicts the most commonly occurring class of training observations within a region.\n",
    "\n",
    "We grow a classification tree much like a binary tree, except we can't use RSS as the criterion for making splits. Instead, there are three common splitting criteria:\n",
    "\n",
    "* *Classification Error Rate*: \n",
    "<center>\n",
    "    $\\displaystyle E = 1-max_{c}(p_{c})$,\n",
    "</center>\n",
    "* *Gini Index*: \n",
    "<center>\n",
    "    $\\displaystyle G = \\sum_{c = 1}^{C}p_{c}(1-p_{c})$,\n",
    "</center>\n",
    "* *Entropy*: \n",
    "<center>\n",
    "    $\\displaystyle D = -\\sum_{c = 1}^{C}p_{c}\\log{p_{c}}$.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3392d-fd35-43e3-9a1b-eb17c1d2a118",
   "metadata": {},
   "source": [
    "The way we determine our split is we take the split that gives us the greatest decease in the given metric. In practice, the classification error, while useful for pruning, isn't sufficiently sensitive for tree-growing, and one of the other two measures is preferred. Between he Gini index and the entropy the splits are usually very similar, and so there's not much to be gained by experimenting to find the most effective one. Just pick one and focus on efficiently pruning the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093a6e1-ed62-4ae8-b875-1db78d82302c",
   "metadata": {},
   "source": [
    "*PYTHON BREAK*\n",
    "\n",
    "A couple times in this lecture we'll be using the [zip](https://docs.python.org/3.3/library/functions.html#zip) function. What this function does is it creates an iterator that aggregates elements from multiple iterables. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc58ea6-bd3c-4896-b1c1-b50dbd45adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ['a','b','c']\n",
    "B = [1,2,3]\n",
    "\n",
    "for letter, number in zip(A,B):\n",
    "    print(letter + str(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500023a-f4ca-4eb3-b29e-27f2b3ad98af",
   "metadata": {},
   "source": [
    "*PYTHON BREAK OVER* - Now back to your regularly scheduled programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461656b1-e07f-4557-9866-37da046b63e0",
   "metadata": {},
   "source": [
    "For a more visual comparison of the three different information criteria, let's plot the values for the probability range $[0,1]$. We'll add a scaled version of the entropy (divided by 2) to observe the Gini index is an intermediate measure between entropy and the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2661f-e56e-4403-b3b9-ff8941315050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    return p * (1 - p) + (1 - p) * (1 - (1 - p))\n",
    "def entropy(p):\n",
    "    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n",
    "def error(p):\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "ent = [entropy(p) if 0 < p else 0 for p in x]\n",
    "sc_ent = [e * 0.5 for e in ent]\n",
    "gini = [gini(i) for i in x]\n",
    "err = [error(i) for i in x]\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.subplot(111)\n",
    "for i, lab, ls, c, in zip([ent, sc_ent, gini, err], \n",
    "                          ['Entropy', 'Entropy (scaled)', \n",
    "                           'Gini impurity', 'Misclassification error'],\n",
    "                          ['-', '-', '--', '-.'],\n",
    "                          ['black', 'lightgray', 'red', 'green', 'cyan']):\n",
    "    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n",
    "ax.legend(loc='upper center',  bbox_to_anchor=(0.5, 1.15), ncol = 4)\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel('p(i=1)')\n",
    "plt.ylabel('impurity index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72f5bd-7038-4ec6-b0b4-326cb680fd56",
   "metadata": {},
   "source": [
    "Let's take a look at how we can build a decision tree using sklearn. To do this, we'll the [Wine Dataset](https://archive.ics.uci.edu/dataset/109/wine) consisting of 178 wine examples with 13 features describing their different chemical properties. \n",
    "\n",
    "we'll import the Iris dataset, which we've seen before, and which is available via sklearn. We'll assign the petal length and petal width to the feature matrix $X$, and the corresponding class labels of the flower species to the vector array $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ecae5-ca0c-461b-87a5-24d6291cac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('./../../Datasets/wine.data', header=None)\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline'] #The columns in the dataset just have numeric labels. These are the descriptions to which those labels correspond.\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fd9db-a728-4a26-bffa-17f16622f010",
   "metadata": {},
   "source": [
    "We're going to attempt to classify them according to their type of grape, of which there are three. To make this a little easier to visualize, we'll only look at two classes of wine and two properties - their alcohol, and OD280/OD315 which is a measure of protein concentration. We'll assign these to the feature matrix $X$, and the corresponding grape numbers to the vector array $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000873e1-21c0-4b67-b66b-580503c15165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 1 class\n",
    "df_wine = df_wine[df_wine['Class label'] != 1]\n",
    "\n",
    "#Note - The to_numpy() function converts it to a numpy array, which will be useful to us later when we want to apply numpy functions.\n",
    "\n",
    "y = df_wine['Class label'].to_numpy() \n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9cdaeb-80b6-45c6-8b11-6e18bbfa6886",
   "metadata": {},
   "source": [
    "If we take a look at our target variable $y$, we see it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52e781-fcc2-4013-a62e-c45029ff937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd17ad1-1c1b-41f9-8276-4bebf05734a8",
   "metadata": {},
   "source": [
    "Now, we'd like to be predicting a $0$ or a $1$ instead of a $2$ or a $3$. To change this, we could just use some Pandas or Numpy tools, but there's another option. We could use the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) function from the preprocessing library in sklearn. This takes a set of $n$ label values, and encodes them as $0, 1, 2, \\ldots n-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56370a-4792-410a-a52c-d05ffdf06df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058b107-a054-4128-a9f6-947626814a9c",
   "metadata": {},
   "source": [
    "Also, we'll want to look at how our models do on training vs test data, so let's split them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b688d-70e3-4ea8-8c5e-6b27eae381cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14da93b-cbf7-4706-95a6-69224064a2e1",
   "metadata": {},
   "source": [
    "Now, we'll build a decision tree for this dataset. The sklearn library contains a [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) within its [tree](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) library. Let's specify the decision criteria to be the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5706e79-d9d5-4893-bba5-1b1e2be4c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=43)\n",
    "tree_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de75645-c2e3-4d5f-8840-9eabd8683009",
   "metadata": {},
   "source": [
    "The sklearn tree library also provides a nice way to readily visualize a decision tree model, called [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3452c-510f-4b25-ac44-d3278249a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "tree.plot_tree(tree_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8522f-7fca-410b-9223-b71039e6167b",
   "metadata": {},
   "source": [
    "While decision trees have the great virtue of being reasonably explainable (how they arrive at their decision can be understood by a person in a way that makes sense to that person), they have the (probably greater) vice of being mediocre predictors. Most of the time, in a pinch, people would rather have a correct answer that they don't understand, than an incorrect one that they do.\n",
    "\n",
    "So, what then is the point of decision trees? Well, it turns out they can be saved through - democracy! More specifically, the power of decision trees can increase significantly using ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1729f7-e28b-47ca-8c29-c882c352ff49",
   "metadata": {},
   "source": [
    "**Ensemble Methods**\n",
    "\n",
    "![Plurality Vote](Plurality_Vote.png)\n",
    "\n",
    "Ensemble methods are methods that make their decision through a vote, and the decision of the group is either a weighted average (in the regression context) or a weighted plurality vote (in the classification context).\n",
    "\n",
    "The idea is that using the training dataset, we start by training $m$ different classifiers $(C_{1},C_{2},\\ldots,C_{m})$. Depending on the technique, the ensemble can be built from different classification algorithms. Or, we can use the same base classification algorithm, but different datasets.\n",
    "\n",
    "![Ensemble Learning](Ensemble_Learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688c2f9-7db6-4ae8-9e4f-0658fa8b9f10",
   "metadata": {},
   "source": [
    "To illustrate why ensemble methods can work better than the individual classifiers alone, let's apply some combinatorics. If we assume that we have a binary classification task, and each model has an independent error rate $\\epsilon$, then we can express the error probability of an ensemble as a probability mass function of a binomial distribution:\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle P(y \\geq k) = \\sum_{m = k}^{n} \\binom{n}{m}\\epsilon^{m}(1-\\epsilon)^{n-m}$\n",
    "</center>\n",
    "\n",
    "If, for example, $\\epsilon = .25$, and we have $11$ classifiers, then the error rate would be $0.034$. A big improvement!\n",
    "\n",
    "More generally, as the base error improves, we see an ensemble error graph that looks like this:\n",
    "\n",
    "![Ensemble Error](Ensemble_Error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45135df6-978a-40f5-9e39-450d2766ff70",
   "metadata": {},
   "source": [
    "**Bagging**\n",
    "\n",
    "The best way to successfully implement an ensemble method is to create multiple samples, train a model for each sample, and then combine them. However, it's usually either impossible or too costly to create multiple samples. So, what do we do if we've only got one sample, and we want to train a model on multiple samples? We boostrap!\n",
    "\n",
    "The idea behind bagging is we draw bootstrap samples (random samples with replacement) from the initial training dataset. We then build a model on each sample, and combine our bootstrap models using (potentially weighted) majority vote.\n",
    "\n",
    "![Bagging Process](Bagging_Process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2aed1-7d9d-4148-bcc9-fc0cb63efbe9",
   "metadata": {},
   "source": [
    "Let's see how bagging would work on our wine classification problem. A [bagging classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) exists in scikit-learn, which we can import from the ensemble submodule. We will use an unpruned decision tree as the base classifier, and create an ensemble of 500 decision trees fit on different bootstrap samples of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c612a2e-d2f2-44e8-aad6-13a5da355d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_model = BaggingClassifier(base_estimator=tree_model, n_estimators=500, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545cbd8-e8d2-444e-bc1a-3dfc1f5ff3f2",
   "metadata": {},
   "source": [
    "Now, let's see how the bagging classifier did vs the decision tree. We'll take a look at the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), which is just the percentage of classifications the model gets correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a437778-dc35-46ae-8666-1c580b26699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_train_pred = tree_model.predict(X_train)\n",
    "y_test_pred = tree_model.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f'\n",
    "      % (tree_train, tree_test))\n",
    "\n",
    "bag_model.fit(X_train, y_train)\n",
    "y_train_pred = bag_model.predict(X_train)\n",
    "y_test_pred = bag_model.predict(X_test)\n",
    "\n",
    "bag_train = accuracy_score(y_train, y_train_pred) \n",
    "bag_test = accuracy_score(y_test, y_test_pred) \n",
    "print('Bagging train/test accuracies %.3f/%.3f'\n",
    "      % (bag_train, bag_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9b794-bb57-441f-8e1d-819581539762",
   "metadata": {},
   "source": [
    "It looks like both are perfect on the training data, but the bagging model does better on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191cd2b-97c9-4eb0-ac01-6f73950134df",
   "metadata": {},
   "source": [
    "Now, let's look at a *very* popular type of model that is based upon this bagging with decision trees approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512565f1-5667-4856-b2d5-6693af8c0047",
   "metadata": {},
   "source": [
    "**Random Forests**\n",
    "\n",
    "Random forests are among the most widely used machine learning algorithms, probably due to their relatively good performance “out of the box” and ease of use. Not much data cleaning or parameter tuning is required to get reasonably strong results.\n",
    "\n",
    "A random forest is very similar to bagging with decision trees which we discussed last time. The major difference is that with a random forest instead of growing our trees using the entire feature set, at each node of the tree we take a random subset of the feature set. The number of features to choose for this random subset is a hyperparameter, but a good rule of thumb is $\\log_{2}{n} + 1$, where $n$ is the number of features.\n",
    "\n",
    "This randomization helps to decorrelate the trees, which decorrelates their errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9ee82-967d-4d95-b151-37262fb67814",
   "metadata": {},
   "source": [
    "Like its BaggingClassifier, the [ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) library of sklearn has a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier). Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89324f-ba2b-4322-8e90-88c2aeebdaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_model = RandomForestClassifier(criterion='entropy', n_estimators=25, max_features = 'log2', random_state=43)\n",
    "\n",
    "forest_model.fit(X_train,y_train)\n",
    "y_train_pred = forest_model.predict(X_train)\n",
    "y_test_pred = forest_model.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Random forest train/test accuracies %.3f/%.3f'\n",
    "      % (tree_train, tree_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3bbb6-97ee-4101-aa8a-ff1365e9424f",
   "metadata": {},
   "source": [
    "It looks like the Random Forest model did as well as the Bagging model in this case. In fact, I'd predict it classified the test data *exactly* the same as the bagging model.\n",
    "\n",
    "We can plot the decision regions for our three classifiers - decision tree, bagging, and random forest - using the code below. In particular, we use the [contourf](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html) function in matplotlib used for plotting filled contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd7b38-2287-40b5-9e0f-9e0a1929a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(1, 3, sharex='col', sharey='row', figsize=(15, 5))\n",
    "\n",
    "\n",
    "for idx, model, tt in zip([0, 1, 2],\n",
    "                        [tree_model, bag_model, forest_model],\n",
    "                        ['Decision tree', 'Bagging', 'Random forest']):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) #Unravels the xx and yy grids and then concatenates them\n",
    "    Z = Z.reshape(xx.shape) #Puts the output back into the proper grid format\n",
    "\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axarr[idx].scatter(X_train[y_train == 0, 0],\n",
    "                       X_train[y_train == 0, 1],\n",
    "                       c='blue', marker='^')\n",
    "    axarr[idx].scatter(X_train[y_train == 1, 0],\n",
    "                       X_train[y_train == 1, 1],\n",
    "                       c='green', marker='o')\n",
    "    axarr[idx].scatter(X_train[y_train == 2, 0],\n",
    "                       X_train[y_train == 2, 1],\n",
    "                       c='red', marker='x')\n",
    "    axarr[idx].set_title(tt)\n",
    "\n",
    "axarr[0].set_ylabel('Alcohol', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.text(7, -0.5,\n",
    "         s='OD280/OD315 of diluted wines',\n",
    "         ha='center',\n",
    "         va='center',\n",
    "         fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
