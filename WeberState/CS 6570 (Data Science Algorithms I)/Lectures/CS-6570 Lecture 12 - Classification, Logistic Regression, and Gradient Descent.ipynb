{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e452f2",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 12 - Classification, Logistic Regression, and Gradient Descent\n",
    "**Instructor: Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e309554",
   "metadata": {},
   "source": [
    "Today's lecture will be a shorter one, to make time to review the regression material before the exam. However, I want to give you a glimpse of the next big topic we'll be covering - classification. We'll look at this in the context on a simple dataset of fruit measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe272a",
   "metadata": {},
   "source": [
    "First, as usual, we'll start by importing our favorite libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85470b1-c02d-4d12-b26d-76a9e810fe55",
   "metadata": {},
   "source": [
    "## Classification Basics\n",
    "\n",
    "For our initial discussion of classification, we'll use a simple dataset of measurements from fruits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_table('YOUR PATH HERE')\n",
    "fruits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd7363",
   "metadata": {},
   "source": [
    "It appears there are seven data columns here and 59 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01593b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36dba82",
   "metadata": {},
   "source": [
    "There are four unique fruit names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4eb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits['fruit_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced60c96",
   "metadata": {},
   "source": [
    "Suppose we want to predict the fruit (based on fruit name) using the other inputs like mass, width, height, and so on. Could we do this with regression?\n",
    "\n",
    "Well, maybe, but it would be a bit awkward. With regression the goal is to predict a numerical response. Fruits aren't numbers. We could try and encode them numerically (as has been done with the fruit label), but viewing these as actual numbers and not labels is a bit silly. It would imply that, in some way, a lemon is four times an apple, and the difference between an apple and a mandarin is half the distance between a mandarin and a lemon. It just doesn't make a lot of sense.\n",
    "\n",
    "Also, in regression the predicted outcomes are numbers, and so the model could, for example, predict a 5.2 or a -1.8. What fruit would that be?\n",
    "\n",
    "Now, if there were only two categories in the classification, then you could attempt to use a regression model and pick a cutoff, so that anything above the cutoff goes to one category, and everything below goes to the other. This can work well enough when there are only two categories (in fact, it's essentially how linear discriminant analysis works with only two categories), but for more than two categories the approach falls apart. We need another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd73223",
   "metadata": {},
   "source": [
    "Returning to our dataset, let's take a look at how we might want to approach this classification. If we look at the scatterplot of our predictor variables against each other, what we'd hope to see is a clustering of the categories we would like to predict, in such a way that we can separate them out. Now, usually with real world data you don't get a perfect quarantine of your categories, but if we can find input variables that seem to provide some separation, that's very valuable for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7163c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['mass', 'width', 'height', 'color_score']\n",
    "X = fruits[feature_names]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "pd.plotting.scatter_matrix(X, c = y, marker = 'o', figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0393",
   "metadata": {},
   "source": [
    "When we're doing classification, we want to find a set of variables that, if possible, separate our classes. It look like we might be able to do something like that using mass and height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d98ef-74fa-461f-9ecb-ba07c881dc2c",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f0ab6-9d86-4928-ba64-9fa20d95e201",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"Log Regression.jpg\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e31658-8464-47e6-9773-19e9077d3929",
   "metadata": {},
   "source": [
    "Suppose I’m an instructor for a data science class, and I want to convince my students that they should\n",
    "study. Of course, there are plenty of excellent reasons for studying, including many that will hopefully\n",
    "be more persuasive than this one, but one matter-of-fact reason is that studying improves the liklihood of\n",
    "passing the final exam. Say I wanted to look into this and understand it a little better. I want to understand\n",
    "how many hours a student should study in order to pass the final exam, and how the probability of passing\n",
    "the exam improved with the number of study hours. How can I do this? Well, one way is to use logistic\n",
    "regression.\n",
    "\n",
    "In today’s class we will cover:\n",
    "\n",
    "- The fundamental concepts behind a logistic regression model, and how the “best” model for a set of\n",
    "data can be determined.\n",
    "\n",
    "- How to use logistic regression in Python with the sklearn library.\n",
    "\n",
    "- Methods for evaluating the quality on a logistic regression model, including understanding the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4fc2bc-62f5-4270-b859-1bbe8483a9b7",
   "metadata": {},
   "source": [
    "### Concepts and Theory\n",
    "\n",
    "Today we’re going to focus on the simplest type of logistic regression, called binary logistic regression, which\n",
    "is used when you want to predict a binary outcome like yes/no or true/false. There are more complicated\n",
    "types of logistic regression like multinomial logistic regression, where there are more than two possible categories, or ordinal logistic regression, where there are more than two categories, and these categories have some natural ordering. \n",
    "\n",
    "For example, a multinomial logistic regression could look at hand written digits, and classify which digit, from 0 to 9, is written, while an ordinal logistic regression could look at the text of Yelp reviews and attempt to predict the number of stars (1 to 5) awarded for the review. While multinomial and ordinal logistic regression are more complicated, the fundamental approach is based on the same concepts as binary logistic regression, so that’s where we’ll focus today.\n",
    "\n",
    "OK, so what are some examples of problems that might have a binary classification? Well, there are lots of them, including:\n",
    "\n",
    "- Predicting whether a candidate will win an election.\n",
    "\n",
    "- Predicting whether somebody will default on their loan.\n",
    "\n",
    "- Predicting whether a tumor is cancerous.\n",
    "\n",
    "- Predicting whether somebody will click on an advertisement.\n",
    "\n",
    "- Predicting whether somebody will eat at Wendy’s in the next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ddad6-01a8-4187-b891-9b67824452d8",
   "metadata": {},
   "source": [
    "### Probabilities and the Sigmoid Function\n",
    "\n",
    "One thing we could try for our binary classification is a linear regression model with a threshold. We could\n",
    "look at all our data and try to draw a line through it that minimizes the sum of squares error. Then, we\n",
    "could determine a threshold value (say .5) such that if our linear model is above that threshold, then we\n",
    "predict a positive outcome, and if it’s below we predict a negative outcome.\n",
    "\n",
    "<center>\n",
    "    <img src=\"Lin Regression.png\" width=\"600\">\n",
    "</center>\n",
    "    \n",
    "This model might do... OK. At least when it comes to classification. However, there’s a real problem when\n",
    "you want to interpret what the outcome means. If I’ve got a linear model that predictis how tall somebody\n",
    "will be based on their parents’ height, and it comes back with 75 inches, that means their predicted height\n",
    "is 75 inches. However, it’s not clear what an output of, say, .78 would mean in this case. Even worse, the\n",
    "linear model can potentially have predicted values less than 0 or greater than 1. This is nuts! This would be a predicted value greater, or less, than the largest, or smallest, possible actual value. It’s very unclear what that should mean.\n",
    "\n",
    "Well, what would we want the output value of our model to mean? These are all binary predictions, and\n",
    "for almost any binary prediction, you’re probably interested in more than just a yes/no, true/false, 1/0\n",
    "prediction. Most of the time, you want to not just know what the most likely outcome is, but you want to\n",
    "have some idea of exactly how likely that outcome is. So, ideally, what we want from our logistic regression\n",
    "model is a *probability*.\n",
    "\n",
    "A probability is a number between 0 and 1, and so instead of a line, we’d want a function with a range from\n",
    "0 to 1. A function that meets this requirement is the sigmoid, which is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f925472-a6a1-4ccb-a9c7-65ec3bac935f",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\\displaystyle S(x) = \\frac{1}{1+e^{-x}}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab7b60-4b6b-4a8d-9fc0-55e8266002f1",
   "metadata": {},
   "source": [
    "And looks like:\n",
    "\n",
    "<center>\n",
    "    <img src=\"Sigmoid.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a1968-d5d3-4522-a720-52ad68fdfb79",
   "metadata": {},
   "source": [
    "Now, there are very good mathematical reasons why the sigmoid function is what we use for logistic regression, but to get into that would divert us too much from the main concepts we want to cover. So, we won’t formally derive why we use the sigmoid,\n",
    "but let’s explore it a bit to see how its properties align with what we’d want for a binary prediction model.\n",
    "\n",
    "Some of the nice properties of the sigmoid include:\n",
    "\n",
    "- Its values are constrained between 0 and 1, so no matter how large (positive) the input to the function\n",
    "is, the output is always less than 1, and no matter how small (negative) the input to the function is,\n",
    "the output is always greater than 0. That’s something we want if we’re going to treat these outputs as\n",
    "probabilities.\n",
    "\n",
    "- The sigmoid function is increasing, which means that if something makes an outcome more likely,\n",
    "even more of that something makes the outcome even more likely. This makes intuitive sense, and\n",
    "aligns with what we’d want for a straightforward probability model.\n",
    "\n",
    "- The sigmoid function is smooth, which mathematically means it’s “differentiable\", but more qualitatively means it doesn’t have any weird jumps or spikes in the probability, which is again what we’d want for a straightforward model.\n",
    "\n",
    "- The sigmoid function has a region where the probabilities are very low, a region where the probabilities are very high, and a transition region where the probabilities are more in-between, which is what we’d expect intuitively for this type of classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450c90b-dd50-4bfc-a344-0b7951dc436a",
   "metadata": {},
   "source": [
    "### The Logistic Regression Model\n",
    "\n",
    "OK, so now we’ve seen the sigmoid function. How do we use it to make a model? Where are the parameters? Well, for a logistic regression model, you actually start with a linear model:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $c_{1}X_{1} + c_{2}X_{2} + · · · + c_{n}X_{n} + b$\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "and then you plug this linear model into the sigmoid function, which gives us:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle \\frac{1}{1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}$\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "If we’ve only got one independent variable, like in our model predicting the probability of passing the final\n",
    "exam based on the number of hours studied, we have a univariate logistic regression model, and the formula\n",
    "for our model (rewriting $c_{1}$ as $m$, and $X_{1}$ and $X$) is:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle \\frac{1}{1 + e^{−(mX+b)}}$\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now, as in linear regression, the task is to solve for the model that does the best job fitting our data. In\n",
    "linear regression, this was the model that minimized the sum of squares between our predictions and our\n",
    "data. However, in logistic regression, this sum of squares approach doesn’t make much sense. So, what do\n",
    "we do?\n",
    "\n",
    "Before we get into the answer, let’s step back and look at a different question. Suppose we have a\n",
    "biased coin, which if you flip ends up heads with probability $p$, and tails with probability $(1 − p)$, and it is\n",
    "not necessarily the case that $p = .5$. Suppose you flip the coin 10 times, and you get the following sequence\n",
    "of heads and tails:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $H, H, T, H, T, H, H, H, T, H.$\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now, for any value $0 < p < 1$, that sequence is technically possible. However, if the probability of heads\n",
    "is $1\\%$, so $p = .01$, then it’s very unlikely that you’d get 7 heads in 10 flips. On the other hand, if it were a\n",
    "fair coin, so $p = .5$, then 7 heads in 10 flips is more likely. A natural question then is - for what value of\n",
    "$p$ is the given sequence most likely? Formulating this mathematically, $p$ is the probability any given flip is\n",
    "heads, $(1 − p)$ is the probability any given flip is tails, and the probability of any sequence of flips is just the\n",
    "product of the probabilities of the individual flips, so:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $P(H, H, T, H, T, H, H, H, T, H) = pp(1 − p)p(1 − p)ppp(1 − p)p = p^{7}(1 − p)^{3}$\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "This is the probability that, with ten flips of the coin, you’d see our sequence of heads and tails, given the\n",
    "chance of heads is $p$. We call this the likelihood of our sequence, and the value of p that maximizes it is the\n",
    "*maximum likelihood estimate*. In this example it’s a calculus exercise to show that the likelihood is maximized\n",
    "at $p = .7$. In other words, if your coin shows up as heads $70\\%$ of the time, your best guess is that its\n",
    "probability of heads is $70\\%$. Not exactly shocking.\n",
    "\n",
    "Returning to logistic regression, suppose we’re looking at the first problem we discussed, predicting\n",
    "whether a student will pass the final exam based on the hours they study, and we have the following\n",
    "four data points:\n",
    "\n",
    "| Student   | Hours of Study | Exam grade |\n",
    "| :---:     |    :----:   | :---: |\n",
    "| Student 1 | 3       | F |\n",
    "| Student 2 | 2       | P |\n",
    "| Student 3 | 0       | F |\n",
    "| Student 4 | 4       | P |\n",
    "\n",
    "\n",
    "Our equation for the probability of passing is:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle S(x, m, b) = \\frac{1}{1 + e^{-(mX+b)}}$,\n",
    "</center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "where $X$ is the number of hours studied, and so the probability of the above four data points, the likelihood\n",
    "of our data, will be given by:\n",
    "\n",
    "&nbsp;\n",
    "<center>\n",
    "    $P(m, b) = (1 − S(3, m, b))S(2, m, b)(1 − S(0, m, b))S(4, m, b) = $\n",
    "</center>\n",
    "&nbsp;\n",
    "<center>\n",
    "    $\\displaystyle \\left(1 −\\frac{1}{1+e^{-(3m+b)}}\\right)\\left(\\frac{1}{1+e^{-(2m+b)}}\\right)\\left(1-\\frac{1}{1+e^{-(0m+b)}}\\right)\\left(1+\\frac{1}{e^{-(4m+b)}}\\right)$\n",
    "</center>\n",
    "&nbsp;\n",
    "\n",
    "Again, with linear regression, our goals was to find the values of m and b that minimized the sum of square\n",
    "error. With logistic regression, our goal is to find the values of m and b that maximize the likelihood, which\n",
    "for our four data points would be the function above. With more data points, it would be exactly the same\n",
    "setup, just with more terms in our product.\n",
    "\n",
    "Generally speaking, finding the values of the coefficients (in this case m and b) that maximize this likelihood function is hard. In our biased coin example, finding the exact value of p that gave us the maximum likelihood was possible. Unfortunately, that’s very rarely the case. Usually there’s no easy way to exactly calculate the maximum likelihood. \n",
    "\n",
    "Note that this is a big difference between logistic and linear regression. With linear regression, there are exact, optimized formulas that will tell you the coefficients that give you the best fit line. Not so with logistic regression.\n",
    "\n",
    "If we can’t solve our equation exactly, what do we do? We use numerical methods to get close to the\n",
    "exact solution. Precisely how this is done introduces a very important concept in data science and machine\n",
    "learning. This very important concept is called gradient descent, which we'll discuss at the end of class today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a820b-3b65-4df1-8472-ae7af6693d56",
   "metadata": {},
   "source": [
    "Alright, let's now dive into some code for doing logistic regression. First, we'll import some of the functions we'll need from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f636b8-6904-4a36-a1e8-ece04a152e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions from sklearn we'll need\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Import the plotting function we'll use for data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd #Data manipulation and analysis library\n",
    "import numpy as np #Numerical analysis library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921bcfe-b500-4722-b19d-eea013970be5",
   "metadata": {},
   "source": [
    "First, we'll use the [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function from sklearn to create a synthetic dataset that we can model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250b39c-8c25-401f-960e-9db07006c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_classification(\n",
    "\n",
    "n_samples=300, #The number of samples in our dataset.\n",
    "n_features=1, #The number of independent variables.\n",
    "n_classes=2, #The number of possible outputs.\n",
    "n_clusters_per_class=1, #The number of distinct data clusters in each class.\n",
    "flip_y=0.05, #The fraction of samples whose values are assigned randomly.\n",
    "n_informative=1, #The number of informative (independent) features.\n",
    "n_redundant=0, #The number of features that can be derived.\n",
    "n_repeated=0 #The number of repeated features.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260af13-6ac4-464e-8e40-9df675bbc0ec",
   "metadata": {},
   "source": [
    "We can then take a look at our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5128d-1466-48ce-9ec0-bbab9bf0ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=y, cmap='rainbow')\n",
    "plt.title('Scatter Plot of Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd8278-6e59-465b-b0a4-04b6f4c5acef",
   "metadata": {},
   "source": [
    "OK. We should be able to build a pretty decent regression model for this dataset. Let's split it into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31a641-c2c7-4d86-a2c8-3a686731f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits our data in training data and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70e134-6aea-423c-914d-3f580769a09b",
   "metadata": {},
   "source": [
    "Now, let's build our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429fb1c-4394-41ab-b15a-9bad50625b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train the logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebc2db-e93c-48d2-b0b8-d23c6102a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The coefficients for our logistic regression model\n",
    "print(log_reg.coef_)\n",
    "print(log_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237936d-ff94-45f3-84fe-9e99ec73810f",
   "metadata": {},
   "source": [
    "We can check the prediction for the regression model for a given input - say 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969001a-3052-4987-a999-18211506ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_reg.predict([[0.7]])) #Our model's prediction for the given input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85a088-43a0-468f-b5c8-92a56ba4ce3a",
   "metadata": {},
   "source": [
    "Let's graph our sigmoid function along with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63897587-1df9-4315-950d-82b7d8849749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will graph our sigmoid function along with our data.\n",
    "# Sigmoid function\n",
    "#\n",
    "def sigmoid(z,m,b):\n",
    "    return 1 / (1 + np.exp(-(m*z+b)))\n",
    "# Creating sample Z points\n",
    "#\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "# Invoking Sigmoid function on all Z points\n",
    "#\n",
    "phi_z = sigmoid(z,log_reg.coef_[0][0],log_reg.intercept_[0])\n",
    "# Plotting the Sigmoid function\n",
    "#\n",
    "plt.plot(z, phi_z)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi(z)$')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.scatter(x, y, c=y, cmap='rainbow')\n",
    "plt.title('Scatter Plot of Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d19779-d79f-403f-9dc8-f7057a1faeda",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131fa2e-0132-4484-84c5-6f8bfc9de7e7",
   "metadata": {},
   "source": [
    "### The Confusion Matrix\n",
    "\n",
    "Once we have a logistic regression model, or really any type of classifier, an obvious question to ask is how\n",
    "well it works. Very few models are perfect, and almost every one will get some classifications wrong. A\n",
    "confusion matrix is a tabular way to record the successes and failures of a model, and provides a quick\n",
    "overview of the model’s performance.\n",
    "\n",
    "The rows of a confusion matrix represent the actual category classifications for our data, while the columns\n",
    "represent the category classifications from our model. The entries represent the number of data elements\n",
    "with the given actual and model classifications. For example, suppose we have a binary logistic regression\n",
    "model predicting whether a tumor is malignant or benign, and it has the following confusion matrix:\n",
    "\n",
    "| Tumor Diagnosis | Malignant | Benign |\n",
    "| :---:     |    :----:   | :---: |\n",
    "| Malignant | 29      | 2 |\n",
    "| Benign    | 4       | 66 |\n",
    "\n",
    "The first row of data represents the tumors that were actually malignant, and the second row of data rep-\n",
    "resents the tumors that were actually benign. The first column of data represents the tumors the model classified as malignant, while the second column of data represents the tumors the model classified as benign.\n",
    "\n",
    "From this we can see that there were 33 tumors classified as malignant, and of these 33, 29 were actually\n",
    "malignant, while 4 were incorrectly classified, and were in reality benign. When a model marks a negative\n",
    "(in this case benign) as a positive (in this case malignant) this is called a false positive or a type I error. We\n",
    "can also see there were 68 tumors classified as benign, and of these 68, 66 were actually benign, while 2\n",
    "were incorrectly classified, and were in reality malignant. When a model marks a positive (in this case\n",
    "malignant) as a negative (in this case benign) this is called a false negative or a type II error.\n",
    "\n",
    "Now, it’s not always the case that the cost associated with a false positive is the same as that associated\n",
    "with a false negative. For example, if when the model says malignant the patient goes in for more tests,\n",
    "then the cost of a false positive is relatively small compared to the cost of a false negative, which would\n",
    "mean the patient doesn’t receive treatment when they should. In this case instead of setting the threshold\n",
    "for a positive diagnosis at p > .5, you might set it at p > .05, which will significantly increase the number\n",
    "of positive diagnoses, and increase the type I error (false positives) while decreasing the type II error (false\n",
    "negatives). This might be fine given the nature of the problem. On the other hand, if when the model says\n",
    "malignant the patient goes in for invasive surgery, then the cost of a false positive is significant, and we\n",
    "wouldn’t bring the threshold down from p = .5 as much, if at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8443e7-98b2-4d6e-9a40-07bdd33e08ce",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall, and Specificity\n",
    "\n",
    "There are a few terms associated with confusion matrices that any practicing data scientist needs to know.\n",
    "These are:\n",
    "\n",
    "- Accuracy - This one is pretty straightforward. The accuracy of a model is how often it’s right, regardless of whether it’s right about a true positive, or a true negative. The formula for accuracy is:\n",
    "\n",
    "<center>\n",
    " $\\text{Accuracy} = \\displaystyle \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\n",
    "</center>\n",
    "\n",
    "- Precision - The precision is concerned with the positive. Specifically, it is the fraction of the time the\n",
    "model’s positive result is accurate. Defined precisely, the formula for precision is:\n",
    "\n",
    "<center>\n",
    "    $\\text{Precision} = \\displaystyle \\frac{\\text{True Positives}}{\\text{Predicted Positives}} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n",
    "</center>\n",
    "\n",
    "- Recall - The recall is concerned with how often the model correctly identifies an actual positive. Its formula is:\n",
    "\n",
    "<center>\n",
    "    $\\text{Recall} = \\displaystyle \\frac{\\text{True Positives}}{\\text{Real Positives}} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n",
    "</center>\n",
    "\n",
    "- Specificity - The specificity is concerned with how often the model correctly identifies an actual negative. Its formula is:\n",
    "\n",
    "<center>\n",
    "    $\\text{Specificity} = \\displaystyle \\frac{\\text{True Negatives}}{\\text{Real Negatives}} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}$\n",
    "</center>\n",
    "\n",
    "In a great model, ideally, all of these will be high. However, in reality, frequently tradeoffs must be made,\n",
    "and which of these you choose to optimize will depend on the nature of your problem, and the different\n",
    "costs for different types of mistakes. For example, in our tumor example, which of these do you think\n",
    "would be most important?\n",
    "\n",
    "Everything we’ve talked about so far can be applied to any binary classification problem, and it’s straight-\n",
    "forward to generalize these ideas to non-binary classification as well. However, remember, one of the big ideas behind logistic regression was that it not only gave you a yes/no prediction, but it also gave you a probability. Is there a way of measuring how good those probability scores are? Yes, there is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8dae2-03ea-40eb-b766-8d370aed0e48",
   "metadata": {},
   "source": [
    "### The ROC Curve and AUC\n",
    "\n",
    "The ROC curve (which stands for the receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. The curve plots two parameters, the true positive rate (TPR), which is the recall, on the y-axis, and the false positive rate (FPR), which is 1 minus the specificity, on the x-axis. When the threshold is p = 1, so as strict as it could be, then nothing will be classified as positive, and both the TPR and FPR will be 0. When the threshold is p = 0, so as relaxed as it could be, then everything will be classified as positive, and both the TPR and FPR will be 1. As we move from p = 1 down to p = 0, the TPR and FPR will grow at their own rates, and we can plot their values on a curve. This is the ROC curve.\n",
    "\n",
    "<center>\n",
    "    <img src=\"ROC.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4e13f-27ac-4315-a3dd-438b865b7123",
   "metadata": {},
   "source": [
    "We measure the performance of the model by looking at the area under the ROC curve. This performance\n",
    "measure is called AUC, which just stands for \"Area Under the Curve\". This is a number between 0 and 1,\n",
    "and would be 1 for a perfect model (one that gave every real positive probability 1, and every real negative\n",
    "probability 0). For a random model, you’d get an AUC of about .5. A good AUC is usually between .8 and\n",
    ".9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e06bb7-f75b-4fd5-88a1-08df96523b26",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e456450-2d09-4a39-a828-cd814ce3a1b3",
   "metadata": {},
   "source": [
    "Let's return to the question of how we can find the maximum likelihood estimate for a logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88331746-06b9-4195-ac00-d0fe8c2eaba6",
   "metadata": {},
   "source": [
    "The function that we use for predictions in logistic regression is the sigmoid function:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle S(\\textbf{X}) = \\frac{1}{1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}$\n",
    "</center>\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d4c8d-9bc4-4f36-a2e4-c45d61242e40",
   "metadata": {},
   "source": [
    "If we're given a set of input data and we want to find the best predictor, we don't try to minimize the sum of squares error as we did with linear regression. Instead, with logistic regression, we try to *maximize the likelihood*. In other words, we try to find the values of our coefficients such that the probability of our dataset is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38550f1f-2ac1-4c75-b620-38be3979a01a",
   "metadata": {},
   "source": [
    "Now, unfortunately, the exact values that achieve this aren't always easy to find. For the sigmoid function and logistic regression, it's not really possible to find a closed-form solution the way we can with linear regression. So, to get around this difficulty, we use *gradient descent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98515340-4531-4f5b-b592-911c39b8c184",
   "metadata": {},
   "source": [
    "The basic idea is suppose you’re on a mountain and you’re trying to get down it as quickly as possible, but you’re in a fog and so you can only see a couple feet in front of you. You\n",
    "don’t know where the lowest point on the mountain is, but you do know the direction that will get you\n",
    "down the farthest on your next step. So, you just take a step in that direction, look around, figure out the\n",
    "step that will take you down the farthest from your new position, take that step, and so on. Every time,\n",
    "you’re taking the step that gets you down the farthest locally, and hopefully this approach will get you to\n",
    "the bottom of the mountain ASAP.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"Gradient_Descent.gif\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6646cb-115f-4472-92af-632296d0cbbf",
   "metadata": {},
   "source": [
    "Stated more mathematically, at any given point it's frequently straightforward to find the gradient (the vector of partial derivatives) of our function. This gradient tells us the direction of greatest increase at that point, and so if we're looking to maximize a function, we can take a step in that direction. If we're looking to minimize the function, we can take a step in the other direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ca260-d71c-48d3-9a68-71df3303a4d2",
   "metadata": {},
   "source": [
    "For the sigmoid function, the partial derivative with respect to a coefficient is relatively easy to calculate:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle \\frac{\\partial S}{\\partial c_{i}} = \\frac{X_{i}e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}{\\left(1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}\\right)^{2}} = X_{i}\\left(1-\\frac{1}{1 + e^{-(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}\\right)\\left(\\frac{1}{1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}\\right) = X_{i}(1-S)S$\n",
    "</center>\n",
    "&nbsp;\n",
    "\n",
    "Here $c_{0} = b$ and $X_{0} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f137a6-b6c5-40b6-8978-e10a03bd9b90",
   "metadata": {},
   "source": [
    "From this, we can fairly easily calculate the gradient at any point $\\textbf{X}$. Now, an important hyperparameter here is the step size - how far you move in the direction of (or opposite) the gradient at each step. This is sometimes knows as the *learning rate*, and its study is an important field within machine learning. For today, we'll assume the step size is constant, but please note that isn't always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65526c-4cf2-4183-8957-86dde7a1867e",
   "metadata": {},
   "source": [
    "Something to note is that the product of terms in our likelihood is equal to the number of observations in our dataset. For a relatively small dataset this isn't a problem, but for a huge dataset this can make likelihood calculations rather involved, and iterative likelihood calculations extremely resource intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309f0c1-e02d-44b3-9a05-b4e958c32e9e",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "One way to get around this issue is with *stochastic gradient descent*. The idea behind stochastic gradient descent is that (in the extreme) we only see how our prediction works on one single observation, and then we adjust our parameters accordingly based upon our prediction for that observation. In other words, we move either forward or backward depending on whether our prediction was right or wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691298a-5c40-4d7d-84e6-ea4ed756f0c7",
   "metadata": {},
   "source": [
    "This can make the optimization problem *much* less computationally intensive, although it does introduce some potential problems, and makes your optimization dependent on the order in which you evaluate the observations. A middle ground between pure gradient descent and extreme stochastic gradient descent is batched stochastic gradient descent, where we divide our data into disjoint groups, and run gradient descent over each group individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437bce7-1719-4fe9-9e13-1964d65fc308",
   "metadata": {},
   "source": [
    "Let's take a look at how we could implement logistic regression with stochastic gradient descent. In today's lecture, instead of relying on our standard libraries like pandas and numpy, we're going to try something different and (as much as possible) write everything from scratch, just to see how it would be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6bad4-b8dc-4f3f-879d-bc58dacf917d",
   "metadata": {},
   "source": [
    "First, let's take a look at a simple sample dataset with two predictive inputs and a binary categorical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71db3d1-8ab8-4fe6-b002-de5a41721f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1d97b-028e-40ce-a102-9c761ea15807",
   "metadata": {},
   "source": [
    "For a set of observations and a set of coefficients for our logistic regression model, we can write a function that makes a prediction (0 or 1) for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01e1e0-bd53-42a1-bc3d-609d4e72d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    X = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        X += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121eb9cd-1965-4b33-b0bb-b03da43afbc2",
   "metadata": {},
   "source": [
    "Let's try this out with some initial coefficients all set to $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871a660-c7ca-405c-b45b-e2759d40026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cf0de-1c29-4307-b43b-6b53fdaff1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdb3c6-387a-4560-89d0-0569d364cb41",
   "metadata": {},
   "source": [
    "Pretty much a zero-shot always predict 1 model. Let's see if we can use stochastic gradient descent to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc95676-114e-420f-a02d-8a19f4d93b01",
   "metadata": {},
   "source": [
    "First, let's write a function for performing stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f54086-7953-495f-b7fd-7a6875f35682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [1.0 for i in range(len(train[0]))] #Start everything at 0.0\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9467a-75f4-40de-a5fd-43c17511298a",
   "metadata": {},
   "source": [
    "Then, let's give it a run with a learning rate of $.3$, and 100 epochs (trips through the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eed0a-296a-4667-9348-ed8d18453264",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688d4c2-a7f5-4d66-bd7b-d3d4197497f4",
   "metadata": {},
   "source": [
    "Hopefully, this improves the predictive power of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9189406-29eb-45e9-80fe-1108057b959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f109f-b94f-491d-bb8b-235d7771bf21",
   "metadata": {},
   "source": [
    "Nailed it! In homework assignment #4, you'll try something a bit more involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
