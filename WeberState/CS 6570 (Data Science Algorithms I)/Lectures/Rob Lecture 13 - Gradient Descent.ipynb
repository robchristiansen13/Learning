{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e452f2",
   "metadata": {},
   "source": [
    "# CS-6580 Lecture 13 - Gradient Descent\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fcea2-0b8d-4883-863d-e491c152b526",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e3a8a-cde1-42a1-b29b-6c1ad24f2109",
   "metadata": {},
   "source": [
    "In our last lecture, we discussed how in linear regression the goal is to minimize the sum of square error, while in logistic regression the goal is to maximize the likelihood of the observations. However, unlike with linear regression, generally speaking it's very hard, and in most cases essentially impossible, to optimize precisely. Consequently, the maximum likelihood must be approximated using numeric techniques, and today we'll discuss the foundation for most - gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62737592-4826-4e17-b54d-0189fe558098",
   "metadata": {},
   "source": [
    "The basic idea is suppose you’re on a mountain and you’re trying to get down it as quickly as possible, but you’re in a fog and so you can only see a couple feet in front of you. You\n",
    "don’t know where the lowest point on the mountain is, but you do know the direction that will get you\n",
    "down the farthest on your next step. So, you just take a step in that direction, look around, figure out the\n",
    "step that will take you down the farthest from your new position, take that step, and so on. Every time,\n",
    "you’re taking the step that gets you down the farthest locally, and hopefully this approach will get you to\n",
    "the bottom of the mountain ASAP.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    <img src=\"Gradient_Descent.gif\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce30ab8-2a73-472a-9e91-0b3faeded4b9",
   "metadata": {},
   "source": [
    "Stated more mathematically, at any given point it's frequently straightforward to find the gradient (the vector of partial derivatives) of our function. This gradient tells us the direction of greatest increase at that point, and so if we're looking to maximize a function, we can take a step in that direction. If we're looking to minimize the function, we can take a step in the other direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8d291-b3cf-4a17-8580-4ebc4d6b297c",
   "metadata": {},
   "source": [
    "For the sigmoid function, the partial derivative with respect to a coefficient is relatively easy to calculate:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "    $\\displaystyle \\frac{\\partial S}{\\partial c_{i}} = \\frac{X_{i}e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}{\\left(1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}\\right)^{2}} = X_{i}\\left(1-\\frac{1}{1 + e^{-(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}\\right)\\left(\\frac{1}{1 + e^{−(c_{1}X_{1}+c_{2}X_{2}+···+c_{n}X_{n}+b)}}\\right) = X_{i}(1-S)S$\n",
    "</center>\n",
    "&nbsp;\n",
    "\n",
    "Here $c_{0} = b$ and $X_{0} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbd63f-0a19-40c3-bb6b-470568e217e2",
   "metadata": {},
   "source": [
    "From this, we can fairly easily calculate the gradient at any point $\\textbf{X}$. Now, an important hyperparameter here is the step size - how far you move in the direction of (or opposite) the gradient at each step. This is sometimes knows as the *learning rate*, and its study is an important field within machine learning. For today, we'll assume the step size is constant, but please note that isn't always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abd067-603e-406d-968e-5f8ecf6feaf3",
   "metadata": {},
   "source": [
    "Something to note is that the product of terms in our likelihood is equal to the number of observations in our dataset. For a relatively small dataset this isn't a problem, but for a huge dataset this can make likelihood calculations rather involved, and iterative likelihood calculations extremely resource intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3f4ff-f7cd-4bd2-946f-23f3ae2751d2",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "One way to get around this issue is with *stochastic gradient descent*. The idea behind stochastic gradient descent is that (in the extreme) we only see how our prediction works on one single observation, and then we adjust our parameters accordingly based upon our prediction for that observation. In other words, we move either forward or backward depending on whether our prediction was right or wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f171fc-d554-445a-b1cd-fd930c7a0791",
   "metadata": {},
   "source": [
    "This can make the optimization problem *much* less computationally intensive, although it does introduce some potential problems, and makes your optimization dependent on the order in which you evaluate the observations. A middle ground between pure gradient descent and extreme stochastic gradient descent is batched stochastic gradient descent, where we divide our data into disjoint groups, and run gradient descent over each group individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db165435-5c83-4d96-93ae-42c17342a19c",
   "metadata": {},
   "source": [
    "Let's take a look at how we could implement logistic regression with stochastic gradient descent. In today's lecture, instead of relying on our standard libraries like pandas and numpy, we're going to try something different and (as much as possible) write everything from scratch, just to see how it would be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804d6bc-832a-4120-9a3a-0eb28881d733",
   "metadata": {},
   "source": [
    "First, let's take a look at a simple sample dataset with two predictive inputs and a binary categorical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4fa99-5d1d-419e-9132-956fdbe3c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb0a3b-1801-4909-9b0b-bd5d87c852b7",
   "metadata": {},
   "source": [
    "For a set of observations and a set of coefficients for our logistic regression model, we can write a function that makes a prediction (0 or 1) for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4d896-a6e5-4689-8052-769595b61873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    X = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        X += coefficients[i + 1] * row[i]\n",
    "    return 1.0 / (1.0 + exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cae7c2-d5b8-46e0-a374-80921fc7c077",
   "metadata": {},
   "source": [
    "Let's try this out with some initial coefficients all set to $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871c2c4-9994-4b9c-b640-0ab27f07696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e244c-2525-4901-a7a7-c71945b52cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07496ff6-69eb-4c3f-8ac7-34119a3f6290",
   "metadata": {},
   "source": [
    "Pretty much a zero-shot always predict 1 model. Let's see if we can use stochastic gradient descent to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08758c-8434-4487-8064-7948c8a2fb04",
   "metadata": {},
   "source": [
    "First, let's write a function for performing stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2f735-27b0-4a9f-abf8-21540e5f7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [1.0 for i in range(len(train[0]))] #Start everything at 0.0\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = row[-1] - yhat\n",
    "            coef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122196b9-993a-4b8c-ad34-b945b2ff4e6b",
   "metadata": {},
   "source": [
    "Then, let's give it a run with a learning rate of $.3$, and 100 epochs (trips through the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c51f76-05a6-4707-adf9-4d731deddedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.3\n",
    "n_epoch = 100\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5b7ac-a8cf-4c3e-b9ee-f686bc6376bf",
   "metadata": {},
   "source": [
    "Hopefully, this improves the predictive power of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369b138-34ac-4bd7-9803-4bcde9373dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c470d7-7a62-4b91-9ceb-7873fc5f8d52",
   "metadata": {},
   "source": [
    "Nailed it! Alright, let's try something a bit more challenging. We're going to build a logistic regression model using the Pima Indians diabetes dataset, which you can read more about [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef90ba4-f140-40a1-95e2-b631ada3b318",
   "metadata": {},
   "source": [
    "First, let's load the data. We'le write a function to do that, along with a function for converting strings to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59c222-da0e-4605-9ec3-3e7fabd97619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246d747-f0d7-4674-a457-81856350bf61",
   "metadata": {},
   "source": [
    "Next, we'll write some functions to handle some data normalization. We'll want to do this because we're using the same step size (learning rate) for each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fffee7-bc34-4ba5-8b43-f09137e1a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbb93c-b591-456e-840a-881602baf306",
   "metadata": {},
   "source": [
    "Next, we'll create our own random data splitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd49cf-d7dd-4558-b58e-976ff24830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723d129-8b5d-4b9b-96ac-256e1029747f",
   "metadata": {},
   "source": [
    "A simple function for calculating the accuracy of a binary prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d549745-5017-4915-907a-141b7646067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161ab6e-c305-4a79-81a0-a1068c01272f",
   "metadata": {},
   "source": [
    "A logistic regression function that creates a model on training data, and returns predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b795fa-4590-4646-ad8e-3b9a6022f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        yhat = round(yhat)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff986c19-8737-4cdd-a423-7f259db91b2d",
   "metadata": {},
   "source": [
    "An algorithm that evaluates the success of a binary prediction algorithm using cross validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13980811-a59e-41b1-bd72-26e09a6c4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b65988-65ae-41c3-9c5e-4ef66bd1af10",
   "metadata": {},
   "source": [
    "Finally, we'll test the logistic regression algorithm on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab4cb2-2413-403f-b965-ec8cca33eba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "\n",
    "# Test the logistic regression algorithm on the diabetes dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
