{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f6b73c-dec0-4013-977a-749c05a64f4e",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 9 - Variable Selection for Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ede37-5405-4fce-adfd-fd050983e37f",
   "metadata": {},
   "source": [
    "You might be tempted to throw all these variables into your model and see what you can predict. However, you'll quickly run into two issues:\n",
    "\n",
    "- _Model Interpretability_ - If you want to try to understand why and how your model is predicting the way it is, the more input variables you have, the more complicated, convoluted, and contrived your explanation will become.\n",
    "\n",
    "- _Prediction Accuracy_ - This is even worse. With more variables, your model may start to overfit the data you use to build your model - doing well on data it's seen before, but poorly on the new data that you really want to predict.\n",
    "\n",
    "Unfortunately, the metrics we use to gauge the fit of a regression model, RSS and its scaled counterpart $R^{2}$, offer no help for this problem. When we add more variables to our model, RSS will _always_ decrease, and the $R^{2}$ value will _always_ increase, at least on the data we're using to build the model (frequently known as the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2200e3-2f6f-401f-94d5-26feca5b1906",
   "metadata": {},
   "source": [
    "OK, so if RSS and $R^{2}$ won't allow us to differentiate among potential models - how should we do it? Well, there are a few statistical measures that do take into account the size of our model and penalize larger models. Some of these are:\n",
    "\n",
    "**Mallow's Cp**\n",
    "\n",
    "Named after Colin Lingwood Mallows it is defined as:\n",
    "$\\displaystyle C_{p} = \\frac{1}{n}\\left(RSS + 2d\\hat{\\sigma}^{2}\\right)$\n",
    "\n",
    "where $\\hat{\\sigma}^2$ is an estimate of the variance of the error ϵ associated with each response measurement. Typically σ^2 is estimated using the full model containing all predictors.\n",
    "$\\displaystyle \\hat{\\sigma}^{2} = \\frac{RSS}{n-p-1}$\n",
    "\n",
    "where $p$ is the number of predictors.\n",
    "\n",
    "**Bayesian Information Criterion (BIC)**\n",
    "\n",
    "BIC is derived from a Bayesian point of view, and looks similar to Mallow's $C_{p}$. It is defined (up to irrelevant constants) as:\n",
    "\n",
    "$\\displaystyle BIC = \\frac{1}{n}\\left(RSS + \\log{(n)}d\\hat{\\sigma}^{2}\\right)$\n",
    "\n",
    "Like $C_{p}$, the BIC will tend to take small values for a model with low test error.\n",
    "\n",
    "**Adjusted $R^{2}$**\n",
    "\n",
    "Since the $R^{2}$ always increases as more variables are added, the adjusted $R^{2}$ accounts for that fact and introduces a penalty. The intuition is that once all the correct variables have been included in the model, additional noise variables will lead to a very small decrase in RSS, but an increase in k and hence will decrease the adjusted $R^{2}$. In effect, we pay a price for the inclusion of unnecessary variables in the model.\n",
    "$\\displaystyle R^{2}_{a} = 1-\\frac{RSS/(n-k-1)}{TSS/n-1}$\n",
    "\n",
    "$C_{p}$ and $BIC$ have rigorous theoretical justification that rely on asymptotic arguments, i.e. when the sample size $n$ grows very large, they become precise, whereas the adjusted $R^{2}$, although quite intuitive, is not as well motivated in statistical theory. However... it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e92be-beb0-4c49-bb09-85409f54faba",
   "metadata": {},
   "source": [
    "Based on these measures, how can we then determine an optimal subset of predictor variables to use? Well, generally speaking, there are three approaches:\n",
    "\n",
    "- Check *all* the possible subsets for the best option.\n",
    "- Find the best possibility with 1 predictor, and then add on the best possible second predictor based on this first predictor, and so on. This is called forward stepwise selection.\n",
    "- Find the best possibility with all predictors, and then subtract the least importand predictor based on the model with all predictors, and so on. THis is called backward stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4135ab-c015-4dd1-a861-448d98ad5133",
   "metadata": {},
   "source": [
    "**Best Subset Selection**\n",
    "To perform best subset selection, we fit separate models for each possible combination of the $n$ predictors and then select the best subset. That is we fit:\n",
    "\n",
    "- All models that contains exactly one predictor\n",
    "- All models that contain 2 predictors at the second step: $n \\choose{2}$\n",
    "- Until reaching the end point where all $n$ predictors are included in the model\n",
    "\n",
    "This results in $2^{n}$ possibilities. In our case there are $2^{9} = 512$ possible combinations\n",
    "\n",
    "_Algorithm_\n",
    "\n",
    "- Let $M_{0}$ denote the null model which contains no predictors, this model simply predicts the sample mean of each observation\n",
    "\n",
    "- For $k=1,2,...,n$\n",
    "    - Fit all $n \\choose k$ models that contain exactly k predictors\n",
    "    - Pick the best among these $n \\choose k$ models, and call it $M_{k}$. Here the best is having the smallest $RSS$, or an equivalent measure.\n",
    " mo\n",
    "- Select the single best model among $M_{0},M_{1},...,M_{n}$ using $C_{p}$, $BIC$, adjusted $R^{2}$ or any other method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4aa287-e5e1-4d29-a66a-cf78f935e89e",
   "metadata": {},
   "source": [
    "**Forward Stepwise Selection**\n",
    "\n",
    "For computational reasons, the best subset cannot be applied for large $n$ due to the $2^{n}$ complexity. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one at the time. At each step, the variable that gives the greatest additional improvement to the fit is added to the model.\n",
    "\n",
    "_Algorithm_\n",
    "\n",
    "- Let $M_{0}$ denote the null model which contains no predictors.\n",
    "\n",
    "- For $k=1,2,...,n-1$\n",
    "    - Consider all $n-k$ that augment the predictors in $M_{k}$ with one additional predictor.\n",
    "    - Pick the best among these $n - k$ models, and call in $M_{k+1}$.\n",
    "- Select the single best model among $M_{0},M_{1},...,M_{n}$ using $C_{p}$, $BIC$, adjusted $R^{2}$ or any other method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727f36f-995e-45c7-9607-e661b2f49cdf",
   "metadata": {},
   "source": [
    "In addition to the measures above, there are other widely used methods for adding a \"penalty\" to the use of many variables. Two popular models along these lines are _ridge regression_ and _lasso_ regression:\n",
    "\n",
    "For _ridge regression_, the measure we attempt to minimize is:\n",
    "$\\displaystyle \\sum_{i = 1}^{n}\\left(y_{i} - \\hat{y}_{i}\\right)^{2} + \\lambda \\sum_{j = 1}^{p}\\beta_{j}^{2}$.\n",
    "\n",
    "For _lasso regression_, the model we attempt to minimize is:\n",
    "$\\displaystyle \\sum_{i = 1}^{n}\\left(y_{i} - \\hat{y}_{i}\\right)^{2} + \\lambda \\sum_{j = 1}^{p}|\\beta_{j}|$.\n",
    "\n",
    "Both models penalize large values of the coefficients, and so are biased towards $0$. This means the approach will tend to favor underestimating the coefficients. \n",
    "\n",
    "Why would we want to do this? Well, if we recall from our first lecture, generally speaking the introduction of bias tends to decrease the variance within our model (the bias-variance tradeoff), and so by decreasing the variance, you decrease the chance that your model will be far removed from the actual \"correct\" model.\n",
    "\n",
    "We call the penalty for Ridge regression an $\\ell_{2}$ norm, and the penalty for the Lasso is $\\ell_{1}$ norm. The different numbers refer to the exponent of the individual $\\beta_{i}$ terms. (Note technically to be an $\\ell_{2}$ norm, the penalty for ridge regression should have a square root, but don't worry about that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458b178-9081-44e0-b0b9-9ad0e2d8e199",
   "metadata": {},
   "source": [
    "Now, one of the first thing to notice here is that term $\\lambda$. What is it, and how do we know its value? Well, the term $\\lambda$ is something called a _hyperparameter_. You don't determine it as part of the modeling process - you define it before you start the modeling process. So, you could view both ridge and lasso regression not as single models, but as families of models parametrized by $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9733200-7fa2-4d3f-a8b9-86345ee51d96",
   "metadata": {},
   "source": [
    "At one extreme, $\\lambda = 0$, both ridge and lasso regression reduce to standard regression, in which the goal is to minimize $\\sum_{i = 1}^{n}\\left(y_{i} - \\hat{y}_{i}\\right)^{2}$. On the other extreme, as $\\lambda \\rightarrow \\infty$, the penalty for any positive coefficient goes to $\\infty$, and so the model will tend towards a constant model $Y = \\beta_{0}$, where $\\beta_{0}$ will just be the average of the output values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67187b5b-6d84-4b5a-a15f-cc1e53c667ee",
   "metadata": {},
   "source": [
    "It is between these extremes that we want to build our model, and so how do we figure out $\\lambda$? Well, as usual, if we just try to optimize a value like $RSS$ on our training data, we won't get anywhere. Specifically, we'll always choose $\\lambda = 0$ by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b1d10-733b-46a7-9f3f-afd13d1f2a57",
   "metadata": {},
   "source": [
    "The validation set approach is conceptually simple and easy to implement. However, it has two potential drawbacks:\n",
    "\n",
    "1. The validation estimate of the test error rate can be highly variable, and can be highly dependent on precisely which observations are included in the training set vs. the validation set.\n",
    "\n",
    "\n",
    "2. Only a subset of the observations, those that are included in the training set, are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may _overestimate_ the test error rate for the model when fit on the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ed0ff-00d0-4e56-9659-6412a59b2fb5",
   "metadata": {},
   "source": [
    "One way to, partially, address these concerns is by using _cross-validation_, a refinement of the validation set approach that addresses these two issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fb444-8c04-469e-ac11-b51618efcb19",
   "metadata": {},
   "source": [
    "**Leave-One-Out Cross-Valdation (LOOCV)**\n",
    "\n",
    "For LOOCV, instead of creating two subsets of comparable size as in the validation set approach, a single observation $(x_{i},y_{i})$ is used for the validation set, and the remaining observations make up the training set. The model is then fit on the $n-1$ training observations, and its predicted value $\\hat{y}_{i}$ for $x_{i}$ is tested against the actual output $y_{i}$. So, $MSE_{i} = (y_{i}-\\hat{y}_{i})^{2}$. This procedure is repeated for all $n$ observations, and the estimated $MSE$ is the average of the errors for each model:\n",
    "\n",
    "$\\displaystyle CV_{(n)} = \\frac{1}{n}\\sum_{i = 1}^{n}MSE_{i}$.\n",
    "\n",
    "This approach has far less bias than the validation set approach, and removes the randomness and risk inherent in the training / validation set splits. However, it can be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681f7ba-0525-450f-a757-8ffcec6a28ae",
   "metadata": {},
   "source": [
    "**k-Fold Cross-Validation**\n",
    "\n",
    "An alternative to LOOCV is $k$-fold cross-validation (CV). In this approach, we randomly divide the set of observations into $k$ groups, or _folds_, of approximately equal size. The first fold is treated as the validation set, and the model is fit on the remaining $k-1$ folds. The mean squared error of the model on this first fold, $MSE_{1}$, in then computed. Thes is repeated $k$ times, producing $MSE_{1},MSE_{2},\\ldots,MSE_{k}$. The $MSE$ estimate is then computed as:\n",
    "$\\displaystyle CV_{(k)} = \\frac{1}{k}\\sum_{i = 1}^{n}MSE_{i}$.\n",
    "\n",
    "It isn't hard to see that the validation set approach and LOOCV are just special cases of $k$-fold CV, with $k = 2$ and $k = n$ respectively. Which value of $k$ should you use for your model? Well, that's another hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55b17c-e600-44ce-bd8a-6160c6339562",
   "metadata": {},
   "source": [
    "The problem with using a large value of $k$ isn't just the increase in computation time, which for modern computers is rarely a huge concern. Yet again, the problem with using a large value for $k$ is the bias-variance trade-off. For the extreme case of LOOCV, the training data for each model is almost exactly the same, which means the models are highly correlated. This means that, while a larger value of $k$ will decrease the bias in the individual estimates (the tendency to overestimate the error), it will also increase the variance. There's basically no way of getting out of that tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd8c55-db99-45f0-b48c-c8639a8f08c5",
   "metadata": {},
   "source": [
    "Alright, now we're going to do a bunch of ridge regressions, and record the coefficients we get. Note, there's no concensus as to whether the hyperparameter for ridge or lasso regression is a $\\lambda$ or an $\\alpha$. Most of the teaching literature uses $\\lambda$, so I've used it here. Unfortunately, Python uses $\\alpha$, which you'll see in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa72f5f-1185-48da-9fa9-9e33daf2890e",
   "metadata": {},
   "source": [
    "For ridge regression there is already a function, RidgeCV, that will select the best value of $\\lambda$ using cross-validation. The value of the parameter \"cv\" sets the number of folds, and it defaults to LOOCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781682e-12a1-4a06-aa5e-619d78ca56d9",
   "metadata": {},
   "source": [
    "Note that, while many of these coefficients are small, _none_ are $0$. That will not be the case with lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50756f43-074a-4aa9-b26b-9e943edbabb1",
   "metadata": {},
   "source": [
    "Note that for lasso regression here, it figures out the optimal value using iterative fitting, and we need to specify a maximum number of required iterations. We won't get into why it's different than ridge regression, but it has to do with optimizing squared terms vs. optimizing absolute values - smooth derivatives and all that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e01ff0-a508-4c70-bb64-6b6387799dc5",
   "metadata": {},
   "source": [
    "What about for larger values of $\\lambda$? Well, here we see that lasso sets many coefficients to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942fd35-7ba4-442f-8055-453b0e91c8f1",
   "metadata": {},
   "source": [
    "As expected, most of the coefficients are $0$, which is not what we saw for large values of $\\lambda$ in ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
