{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f1c05e",
   "metadata": {},
   "source": [
    "# CS-6580 Lecture 20 - Introduction to Neural Networks\n",
    "\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6675f",
   "metadata": {},
   "source": [
    "The progress in and application of neural networks is arguably the single most important thing happening in the world today.\n",
    "\n",
    "Yeah, I know that's a bold statement - but it's defensible. In the last decade there has been an explosion in the depth and difficulty of problems neural networks have been able to tackle, and it looks like further algorithmic advances are unnecessary for this progress to continue. The only thing that's necessary for them to keep getting better is more data, more parameters, and more compute. In other words, it's just a resource question.\n",
    "\n",
    "As such, neural networks are in an interesting position regarding how they're used by businesses. If you want to build a model for tackling a prediction problem on a relative small amount of data - say the data most companies deal with regarding things like sales, engagement, expenses, etc.. - then a neural network probably won't be the best option. However, if you've got a big problem, and significant resources for tackling it, a neural network can produce results that are, in some cases, absolutely amazing. The most recent, and significant, advance along these lines have been regarding *large language models* (LLMs), and in particular GPT-3, GPT-4, and the ChatGPT interface. We'll probably see a lot more of this in the next 5 years, and their potential impact on the world could be huge. Like, really huge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c28c8",
   "metadata": {},
   "source": [
    "Today, we're going to talk about the basic idea behind a neural network, and see how one can work in action on a very famous problem - classifying hand written digits. We won't get into too much depth on either the mathematics behind how it works or the specifics of the code. That will come next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac36a7",
   "metadata": {},
   "source": [
    "**The Single-Layer Neural Network (Perceptron) and Logistic Regression**\n",
    "\n",
    "The simplest type of neural network - a neural network without any internal (or \"hidden\") layers, is called a \"perceptron\", represented below:\n",
    "\n",
    "![Perceptron](Perceptron.png)\n",
    "\n",
    "The idea here is that it takes as its input the values from our input variables $X_{1},X_{2}, \\ldots, X_{n}$, multiplies them by their appropriate weights, adds these multiplied weights together, feeds these to an activation function, and then to a unit step function.\n",
    "\n",
    "This might look complicated, but if the activation function is a sigmoid (which it frequently is) then this is just logistic regression.\n",
    "\n",
    "![Sigmoid](Sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2e7c8",
   "metadata": {},
   "source": [
    "How do we figure out what our weights are (or should be) with logistic regression? We can't minimize our loss function exactly (like we do with regular regression), and so we look for a minimum using *gradient descent*. Exactly the same idea applies here. The weights are updated based upon whether the prediction is correct, and this is repeated multiple times in order to tune the model. That's it. That's all there is to it. A perceptron with a sigmoid activation function is just logistic regression.\n",
    "\n",
    "Perceptrons were put forth as a very early model for modeling human cognition in the 1940s and 1950s, and were the subject of the famous 1969 book \"Perceptrons\" by Marvin Minsky and Seymour Papert, in which they proved, among many other things, that it's impossible for a perceptron to learn the XOR function. In other words, while a perceptron may have its place, it's rather limited.\n",
    "\n",
    "How can we expand the power of the perceptron? We go deep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d95fb",
   "metadata": {},
   "source": [
    "**Multi-Layer Neural Network a.k.a. Deep Learning**\n",
    "\n",
    "A multi-layer neural network (which essentally essentially everything that's actually called a neural network) is just a bunch of perceptrons (or whatever activation function you want to use) meshed together.\n",
    "\n",
    "![Multi-Layer Neural Network](Multi-Layer.png)\n",
    "\n",
    "The number of layers and size of each layer is a hyperparameter that is set before the model trains, and then during model training, what you do is update the weights. How these weights are updated is through something called \"back propagation\", which means the errors from the output layer are fed back to the previous layer and the weights are adjusted, the error for the previous layer is then fed back to the layer before it, and so on. It turns out that these steps can be handled sequentially in this manner, so that the entire neural network doesn't need to be modified at once. This significantly decreases the complexity of an update (a step in gradient descent), and the reason you're able to do this is, essentially, just the chain rule from calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa5843",
   "metadata": {},
   "source": [
    "**Classifying Handwritten Digits**\n",
    "\n",
    "One of the classic examples in machine learning is figuring out how to correctly classify a handwritten digit. It's very difficult to say exactly what constituties a 7, but I think we'd all agree we (almost always) know one when we see it. For example, all of these are the different ways of writing the number 7:\n",
    "\n",
    "![Sevens](Sevens.png)\n",
    "\n",
    "Today, we're going to build a neural network to learn how to distinguish between handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252a5a6",
   "metadata": {},
   "source": [
    "The dataset we'll use is taken from the publicly available MNIST (Modified National Institute of Standards and Technology) dataset which you can find here:http://yann.lecun.com/exdb/mnist/ \n",
    "\n",
    "It consists of the following parts:\n",
    "\n",
    "* Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "* Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "* Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "* Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n",
    "Today we will only be working with a subset of MNIST, and so we only need to download the training set images and training set labels.\n",
    "\n",
    "First, we'll unzip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code unzips the MNIST dataset.\n",
    "\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir() if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode='rb') as decompressed, open(z[:-3], 'wb') as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c1269",
   "metadata": {},
   "source": [
    "Next, we'll create a function for loading and processing the training data. Don't worry about the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc6a2f",
   "metadata": {},
   "source": [
    "OK, now that we have our data, let's use the load function we created above to create a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eaa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3154f0",
   "metadata": {},
   "source": [
    "We can use matplotlib to visualize the first digit of each class in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8133b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d2973",
   "metadata": {},
   "source": [
    "And, with a little bet more data cleaning / preparation (again, don't worry about the details right now), we have our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ed6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.savez_compressed('mnist_scaled.npz', \n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test)\n",
    "\n",
    "mnist = np.load('mnist_scaled.npz')\n",
    "mnist.files\n",
    "\n",
    "X_train, y_train, X_test, y_test = [mnist[f] for f in ['X_train', 'y_train', \n",
    "                                    'X_test', 'y_test']]\n",
    "\n",
    "del mnist\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176adca",
   "metadata": {},
   "source": [
    "Now, let's build a multi-layer neural network for classifying this data. Please note this is a lot, and we'll step through the sections in class next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa07c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l2 : float (default: 0.)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0. (default)\n",
    "    epochs : int (default: 100)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatch_size : int (default: 1)\n",
    "        Number of training examples per minibatch.\n",
    "    seed : int (default: None)\n",
    "        Random seed for initializing weights and shuffling.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    eval_ : dict\n",
    "      Dictionary collecting the cost, training accuracy,\n",
    "      and validation accuracy for each epoch during training.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 l2=0., epochs=100, eta=0.001,\n",
    "                 shuffle=True, minibatch_size=1, seed=None):\n",
    "\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def _onehot(self, y, n_classes):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_examples]\n",
    "            Target values.\n",
    "        n_classes : int\n",
    "            Number of classes\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_examples, n_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"Compute forward propagation step\"\"\"\n",
    "\n",
    "        # step 1: net input of hidden layer\n",
    "        # [n_examples, n_features] dot [n_features, n_hidden]\n",
    "        # -> [n_examples, n_hidden]\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "\n",
    "        # step 2: activation of hidden layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "\n",
    "        # step 3: net input of output layer\n",
    "        # [n_examples, n_hidden] dot [n_hidden, n_classlabels]\n",
    "        # -> [n_examples, n_classlabels]\n",
    "\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "\n",
    "        # step 4: activation output layer\n",
    "        a_out = self._sigmoid(z_out)\n",
    "\n",
    "        return z_h, a_h, z_out, a_out\n",
    "\n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_examples, n_labels)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_examples, n_output_units]\n",
    "            Activation of the output layer (forward propagation)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "\n",
    "        \"\"\"\n",
    "        L2_term = (self.l2 *\n",
    "                   (np.sum(self.w_h ** 2.) +\n",
    "                    np.sum(self.w_out ** 2.)))\n",
    "\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1. - y_enc) * np.log(1. - output)\n",
    "        cost = np.sum(term1 - term2) + L2_term\n",
    "        \n",
    "        # If you are applying this cost function to other\n",
    "        # datasets where activation\n",
    "        # values maybe become more extreme (closer to zero or 1)\n",
    "        # you may encounter \"ZeroDivisionError\"s due to numerical\n",
    "        # instabilities in Python & NumPy for the current implementation.\n",
    "        # I.e., the code tries to evaluate log(0), which is undefined.\n",
    "        # To address this issue, you could add a small constant to the\n",
    "        # activation values that are passed to the log function.\n",
    "        #\n",
    "        # For example:\n",
    "        #\n",
    "        # term1 = -y_enc * (np.log(output + 1e-5))\n",
    "        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_examples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_examples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X_train : array, shape = [n_examples, n_features]\n",
    "            Input layer with original features.\n",
    "        y_train : array, shape = [n_examples]\n",
    "            Target class labels.\n",
    "        X_valid : array, shape = [n_examples, n_features]\n",
    "            Sample features for validation during training\n",
    "        y_valid : array, shape = [n_examples]\n",
    "            Sample labels for validation during training\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        ########################\n",
    "        # Weight initialization\n",
    "        ########################\n",
    "\n",
    "        # weights for input -> hidden\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                      size=(n_features, self.n_hidden))\n",
    "\n",
    "        # weights for hidden -> output\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                        size=(self.n_hidden, n_output))\n",
    "\n",
    "        epoch_strlen = len(str(self.epochs))  # for progress formatting\n",
    "        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
    "\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "\n",
    "        # iterate over training epochs\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n",
    "                                   1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                ##################\n",
    "                # Backpropagation\n",
    "                ##################\n",
    "\n",
    "                # [n_examples, n_classlabels]\n",
    "                delta_out = a_out - y_train_enc[batch_idx]\n",
    "\n",
    "                # [n_examples, n_hidden]\n",
    "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "\n",
    "                # [n_examples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "                # -> [n_examples, n_hidden]\n",
    "                delta_h = (np.dot(delta_out, self.w_out.T) *\n",
    "                           sigmoid_derivative_h)\n",
    "\n",
    "                # [n_features, n_examples] dot [n_examples, n_hidden]\n",
    "                # -> [n_features, n_hidden]\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n",
    "                grad_b_h = np.sum(delta_h, axis=0)\n",
    "\n",
    "                # [n_hidden, n_examples] dot [n_examples, n_classlabels]\n",
    "                # -> [n_hidden, n_classlabels]\n",
    "                grad_w_out = np.dot(a_h.T, delta_out)\n",
    "                grad_b_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "                # Regularization and weight updates\n",
    "                delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "                delta_b_h = grad_b_h # bias is not regularized\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "\n",
    "                delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "                delta_b_out = grad_b_out  # bias is not regularized\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "\n",
    "            #############\n",
    "            # Evaluation\n",
    "            #############\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc,\n",
    "                                      output=a_out)\n",
    "\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n",
    "                         X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n",
    "                         X_valid.shape[0])\n",
    "\n",
    "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n",
    "                             '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
    "                             (epoch_strlen, i+1, self.epochs, cost,\n",
    "                              train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "\n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72932dae",
   "metadata": {},
   "source": [
    "OK! Now we've got our neural network. Please note that, of course, Python has built in neural network libraries, so generally speaking wo don't need to build a neural network from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8ead3",
   "metadata": {},
   "source": [
    "Alright, let's set the number of training iterations we want to use, a.k.a \"epochs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412f09a",
   "metadata": {},
   "source": [
    "Finally, we'll run our neural network on our training data. *Warning* - This can take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_hidden=100, \n",
    "                  l2=0.01, \n",
    "                  epochs=n_epochs, \n",
    "                  eta=0.0005,\n",
    "                  minibatch_size=100, \n",
    "                  shuffle=True,\n",
    "                  seed=1)\n",
    "\n",
    "nn.fit(X_train=X_train[:55000], \n",
    "       y_train=y_train[:55000],\n",
    "       X_valid=X_train[55000:],\n",
    "       y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d73b98",
   "metadata": {},
   "source": [
    "We can graph the performance of this neural network over the 200 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(nn.epochs), nn.eval_['cost'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e172bb",
   "metadata": {},
   "source": [
    "We can also look at how the model performed on the training dataset compared with the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374faa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(nn.epochs), nn.eval_['train_acc'], \n",
    "         label='Training')\n",
    "plt.plot(range(nn.epochs), nn.eval_['valid_acc'], \n",
    "         label='Validation', linestyle='--')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531fb474",
   "metadata": {},
   "source": [
    "So, it looks like it legitimately gets better bor about the first 50 epochs or so, but then it just gets better at predicting the training data, although it doesn't look like that introduces significant overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669a6a7",
   "metadata": {},
   "source": [
    "Alright, now for the final test we can look at how it does on the holdout test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nn.predict(X_test)\n",
    "acc = (np.sum(y_test == y_test_pred)\n",
    "       .astype(np.float) / X_test.shape[0])\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62065a12",
   "metadata": {},
   "source": [
    "Pretty good! Are you curious about the ones that it missed? Well, we can check those out. Let's take a look at the first 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b389b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "miscl_img = X_test[y_test != y_test_pred][:25]\n",
    "correct_lab = y_test[y_test != y_test_pred][:25]\n",
    "miscl_lab = y_test_pred[y_test != y_test_pred][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = miscl_img[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab[i], miscl_lab[i]))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd7a6e",
   "metadata": {},
   "source": [
    "Honestly, I can see how it missed some of these. I'm not entirely sure whether 19 is a 4 or a 9, whether 21 is a 2 or a 7, or whether 23 is an 8 or a 9. So, I think the model is getting pretty close to human performance, which will not be perfect. In fact, the dream of a perfect model probably isn't achieveably under any cincumstance, because of inconsistencies in how the data is labeled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
