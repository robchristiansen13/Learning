{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6509c171-c01c-40be-be70-c10adb5699ef",
   "metadata": {},
   "source": [
    "# CS-6570 Lecture 25 - Introduction to Natural Language Processing (NLP)\n",
    "**Dylan Zwick**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60418a09-31de-4b53-931b-f22025548474",
   "metadata": {},
   "source": [
    "***Introduction to Natural Language Processing (NLP)***\n",
    "\n",
    "Natural language processing (NLP) is a branch of data science that consists of analyzing, understanding, and deriving information from text data. With NLP one can organize massive chunks of text data and solve a wide range of problems, including automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. Just to name a few.\n",
    "\n",
    "But, before we dive into any of these, we'll need to explain some important terms, and (of course) import our favorite libraries. The terms are:\n",
    "\n",
    "* *Tokenization* – process of converting a text into tokens\n",
    "* *Tokens* – words or entities present in the text\n",
    "* *Text object* – a sentence or a phrase or a word or an article\n",
    "\n",
    "The libraries are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d158dbb8-781b-4791-b915-f1e513948dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db393a81-ae10-4c59-b198-8d5ebf57e644",
   "metadata": {},
   "source": [
    "In addition, we'll grab the natural language toolkit (nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7e64586-4c83-4cce-a2e0-11a2c9b70e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/e930b992-a9bf-43e9-98f5-\n",
      "[nltk_data]     95aaaf79bdea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/e930b992-a9bf-43e9-98f5-\n",
      "[nltk_data]     95aaaf79bdea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/e930b992-a9bf-43e9-98f5-\n",
      "[nltk_data]     95aaaf79bdea/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee040a-0a0d-4966-a241-c9128001a097",
   "metadata": {},
   "source": [
    "***Text Preprocessing***\n",
    "\n",
    "Text data is some of the most unstructured data available. Many types of noise are typically present in it and the data is usually not readily analyzable. The entire process of cleaning and standardizing text - making it noise-free and ready for analysis - is known as text preprocessing.\n",
    "\n",
    "It is predominantly comprised of three steps:\n",
    "\n",
    "1. Noise Removal\n",
    "2. Lexicon Normalization\n",
    "3. Object Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd23b0-1431-44c3-a8d8-b99f2c5a13b1",
   "metadata": {},
   "source": [
    "**Noise Removal**\n",
    "\n",
    "Any piece of text which is not relevant to the context of the data and the desired output can be viewed as noise.\n",
    "\n",
    "For example, language stopwords (commonly used words like \"is\", \"am\", \"the\", \"of\", \"in\", etc), URLs or links, social media entities (mentions, hashtags), punctuations, and industry specific words.\n",
    "\n",
    "A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.\n",
    "\n",
    "The following is an example of how we can do this with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2ac2729-191c-4790-8794-b28a68855a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text\n"
     ]
    }
   ],
   "source": [
    "# Sample code to remove noisy words from a text\n",
    "\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "\n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "print(_remove_noise(\"this is a sample text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0b180-e1fa-44ae-897a-7c5f68729a7d",
   "metadata": {},
   "source": [
    "Another way would be to use regular expressions, which we haven't covered but maybe we should have, and will probably cover next semester."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814e64d-fd1b-47d9-a717-3e5dc0e04a76",
   "metadata": {},
   "source": [
    "**Lexicon Normalization**\n",
    "\n",
    "Another type of textual noise concerns multiple representations of a single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are different variations of the word “play”. They mean different things but contextually are similar. This step converts all the versions of a word into a normalized form (also known as a lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to a lower dimensional representation (1 feature).\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "* *Stemming*:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "* *Lemmatization*: Lemmatization, on the other hand, is an organized & step by step procedure for obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Below is sample code that performs lemmatization and stemming using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a78ff48c-968f-4a4a-8422-ba71e8796158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "\n",
      "multipli\n",
      "Lemmatization:\n",
      "\n",
      "multiplying\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "\n",
    "print('Stemming:\\n')\n",
    "print(stem.stem(word))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "print('Lemmatization:\\n')\n",
    "print(lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763890c1-23d6-4278-afaf-639047f84897",
   "metadata": {},
   "source": [
    "Note that \"[Porter stemmer](https://www.nltk.org/_modules/nltk/stem/porter.html)\" is a popular stemming algorithm. Information about lemmatization can be  found [here](https://www.nltk.org/_modules/nltk/stem/wordnet.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452e8c2-3ceb-4e73-b811-9de921f84d4f",
   "metadata": {},
   "source": [
    "**Object Standardization**\n",
    "\n",
    "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
    "\n",
    "Some examples are acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed. \n",
    "\n",
    "For example, the code below uses a dictionary lookup method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b714469b-63b5-441e-b3ac-911852ecb98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Shivam Bansal'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {\"rt\" : \"Retweet\", \"dm\" : \"direct message\", \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215f3f8-4b12-4e81-bfe5-2c5e95c769af",
   "metadata": {},
   "source": [
    "Apart from three steps discussed so far, other types of text preprocessing includes encoding-decoding noise, grammar checker, and spelling correction etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254161d-612b-4e53-ac07-e594b738a327",
   "metadata": {},
   "source": [
    "***Text to Features (Feature Engineering on text data)***\n",
    "\n",
    "To analyse preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques including syntactical parsing, entities / n-grams / word-based features, statistical features, and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613bf60-4e3c-4a7e-b03b-75aeb1e2aadd",
   "metadata": {},
   "source": [
    "**Syntactic Parsing**\n",
    "\n",
    "Syntactical parsing involves the analysis of words in a sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.\n",
    "\n",
    "* *Dependency Trees* – Sentences are composed of words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence – “Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.” The relationship among the words can be observed in the form of a tree representation as shown:\n",
    "\n",
    "<center>\n",
    "    <img src = \"https://drive.google.com/uc?export=view&id=1iO_P7xe1rTarVSc920Ep2qx9Vl-846Yk\">\n",
    "</center>\n",
    "\n",
    "The tree shows that “submitted” is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as – (“Bills” <-> “ports” <by> “proposition” relation), (“ports” <-> “immigration” <by> “conjugation” relation).\n",
    "\n",
    "This type of tree, when parsed recursively in top-down manner, gives grammar relation triplets as output which can be used as features for many NLP problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford's NLP Group) and NLTK dependency grammars can be used to generate dependency trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2e2d6-414f-4378-8658-4b395e6e45ac",
   "metadata": {},
   "source": [
    "The tree shows that “submitted” is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as – (“Bills” <-> “ports” <by> “proposition” relation), (“ports” <-> “immigration” <by> “conjugation” relation).\n",
    "\n",
    "This type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper [StanfordCoreNLP](http://stanfordnlp.github.io/CoreNLP/) (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de390e9a-7fc1-45f8-9590-fcdc2739cee9",
   "metadata": {},
   "source": [
    "* *Part of speech tagging* – Apart from the grammar relations, every word in a sentence is also associated with a part of speech (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. The following code uses Pythan's NLTK to perform pos tagging annotation on input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2790bd7b-d0ca-4f73-8895-b001eb424c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7ada0-0d17-4df6-a5ed-e425f1474279",
   "metadata": {},
   "source": [
    "Part of Speech tagging is used for many important purposes in NLP:\n",
    "\n",
    "* *Word sense disambiguation*: Some language words have multiple meanings according to their usage. For example, in the two sentences below:\n",
    "\n",
    "    I. “Please book my flight for Delhi”\n",
    "\n",
    "    II. “I am going to read this book in the flight”\n",
    "\n",
    "    “Book” is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word “book” is used as v erb, while in II it is used as no un. (Lesk Algorithm is also us ed for similar purposes)\n",
    "\n",
    "* *Improving word-based features*: A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n",
    "\n",
    "    Sentence -“book my flight, I will read this book”\n",
    "\n",
    "    Tokens – (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1)\n",
    "\n",
    "    Tokens with POS – (“book_VB”, 1), (“my_PRP$”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)\n",
    "\n",
    "* *Normalization and Lemmatization*: POS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n",
    "\n",
    "* *Efficient stopword removal*: POS tags are also useful in efficient removal of stopwords.\n",
    "\n",
    "    For example, there are some tags which always define the low frequency / less important words of a language. For example: (IN – “within”, “upon”, “except”), (CD – “one”,”two”, “hundred”), (MD – “may”, “mu st” etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67feff59-a27b-4bdf-a0d7-0c92df045ea4",
   "metadata": {},
   "source": [
    "**Entity Extraction (Entities as features)**\n",
    "\n",
    "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, POS tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ee93-a412-4170-b2be-192cd78f2752",
   "metadata": {},
   "source": [
    "* *Named Entity Recognition (NER)*:\n",
    "    The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :\n",
    "\n",
    "    Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
    "\n",
    "    Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
    "\n",
    "    A typical NER model consists of three blocks:\n",
    "\n",
    "    *Noun phrase identification*: This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
    "\n",
    "    *Phrase classification*: This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.\n",
    "\n",
    "    *Entity disambiguation*: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d459a-d896-419d-963a-1b0f838b46d2",
   "metadata": {},
   "source": [
    "* *Topic modeling*:  \n",
    "    The process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "    Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique, Following is the code to implement topic modeling using LDA in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8aca517f-caeb-434c-bf42-8902891d054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.083*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"my\" + 0.033*\"likes\" + 0.033*\"sugar,\" + 0.033*\"not\" + 0.033*\"Sugar\" + 0.033*\"but\" + 0.033*\"have\"'), (1, '0.029*\"driving\" + 0.029*\"time\" + 0.029*\"of\" + 0.029*\"father\" + 0.029*\"around\" + 0.029*\"dance\" + 0.029*\"practice.\" + 0.029*\"spends\" + 0.029*\"a\" + 0.029*\"lot\"'), (2, '0.060*\"driving\" + 0.060*\"cause\" + 0.060*\"Doctors\" + 0.060*\"and\" + 0.060*\"that\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"may\" + 0.060*\"pressure.\" + 0.060*\"stress\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim \n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda3f92-4651-49a1-ad0b-59344a4c5651",
   "metadata": {},
   "source": [
    "**Statistical Features**\n",
    "\n",
    "Text data can also be quantified directly into numbers using several techniques described in this section:\n",
    "\n",
    "* *Term Frequency – Inverse Document Frequency (TF – IDF)*\n",
    "    TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n",
    "\n",
    "    * Term Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "\n",
    "    * Inverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.\n",
    "\n",
    "    * TF . IDF – TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using python’s scikit learn package to convert a text into tf idf vectors:\n",
    "    \n",
    "<center>\n",
    "    <img src = \"https://drive.google.com/uc?export=view&id=1rj9FHRPkKYinsxN6x3SPBiZZ85ZZC-S6\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f2e1ca3-341c-458d-a60f-ad618ede8c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34520501686496574\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 7)\t0.5844829010200651\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (2, 5)\t0.5844829010200651\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 4)\t0.444514311537431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf314a34-c189-4d20-be8c-1700a1d927e9",
   "metadata": {},
   "source": [
    "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078b259-0bc4-4608-af3b-2b8b584cdd2d",
   "metadata": {},
   "source": [
    "**Word Embedding (text vectors)**\n",
    "\n",
    "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) and [GloVe](http://nlp.stanford.edu/projects/glove/) are the two popular models to create word embedding of a text. These models takes a text corpus as input and produces the word vectors as output.\n",
    "\n",
    "The Word2Vec model is composed of a preprocessing module, a shallow neural network model called Continuous Bag of Words and another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations. The following code using gensim package prepares the word embedding as the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f72cc7a0-7b02-41b9-8f1a-da5929cc5eb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train the model on your corpus  \u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, min_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msimilarity(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscience\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'similarity'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bec7ac-50ec-4226-946a-22569af8110a",
   "metadata": {},
   "source": [
    "Important tasks of NLP\n",
    "This section talks about different use cases and problems in the field of natural language processing.\n",
    "\n",
    "4.1 Text Classification\n",
    "Text classification is one of the classical problem of NLP. Notorious examples include – Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines.\n",
    "\n",
    "Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information filtering, and storage purposes.\n",
    "\n",
    "A typical natural language classifier consists of two parts: (a) Training (b) Prediction as shown in image below. Firstly the text input is processes and features are created. The machine learning models then learn these features and is used for predicting against the new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b05d43-fa5c-46c4-8456-a92034bdb827",
   "metadata": {},
   "source": [
    "The text classification model are heavily dependent upon the quality and quantity of features, while applying any machine learning model it is always a good practice to include more and more training data. H ere are some tips that I wrote about improving the text classification accuracy in one of my previous article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc81f31-16e1-4818-a629-7e7136f39d6d",
   "metadata": {},
   "source": [
    "Text Matching / Similarity\n",
    "One of the important areas of NLP is the matching of text objects to find similarities. Important applications of text matching includes automatic spelling correction, data de-duplication and genome analysis etc.\n",
    "\n",
    "A number of text matching techniques are available depending upon the requirement. This section describes the important techniques in detail.\n",
    "\n",
    "A. Levenshtein Distance – The Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with the allowable edit operations being insertion, deletion, or substitution of a single character. Following is the implementation for efficient memory computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ea616-6298-4069-aacc-af6a9aa8b587",
   "metadata": {},
   "source": [
    "Phonetic Matching – A Phonetic matching algorithm takes a keyword as input (person’s name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar. It is very useful for searching large text corpuses, correcting spelling errors and matching relevant names. Soundex and Metaphone are two main phonetic algorithms used for this purpose. Python’s module Fuzzy is used to compute soundex strings for different words, for example –"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd99e30-c886-4320-a339-a00a4557bb80",
   "metadata": {},
   "source": [
    "C. Flexible String Matching – A complete text matching system includes different algorithms pipelined together to compute variety of text variations. Regular expressions are really helpful for this purposes as well. Another common techniques include – exact string matching, lemmatized matching, and compact matching (takes care of spaces, punctuation’s, slangs etc).\n",
    "\n",
    "D. Cosine Similarity – W hen the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. Following code converts a text to vectors (using term frequency) and applies cosine similarity to provide closeness among two text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f901df-37f5-4319-b4c1-fa2411dc1258",
   "metadata": {},
   "source": [
    "4.3 Coreference Resolution\n",
    "Coreference Resolution is a process of finding relational links among the words (or phrases) within the sentences. Consider an example sentence: ” Donald went to John’s office to see the new table. He looked at it for an hour.“\n",
    "\n",
    "Humans can quickly figure out that “he” denotes Donald (and not John), and that “it” denotes the table (and not John’s office). Coreference Resolution is the component of NLP that does this job automatically. It is used in document summarization, question answering, and information extraction. Stanford CoreNLP provides a python wrapper for commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa99840-32bb-40b9-8b0c-5bfbf6c0878f",
   "metadata": {},
   "source": [
    "4.4 Other NLP problems / tasks\n",
    "Text Summarization – Given a text article or paragraph, summarize it automatically to produce most important and relevant sentences in order.\n",
    "Machine Translation – Automatically translate text from one human language to another by taking care of grammar, semantics and information about the real world, etc.\n",
    "Natural Language Generation and Understanding – Convert information from computer databases or semantic intents into readable human language is called language generation. Converting chunks of text into more logical structures that are easier for computer programs to manipulate is called language understanding.\n",
    "Optical Character Recognition – Given an image representing printed text, determine the corresponding text.\n",
    "Document to Information – This involves parsing of textual data present in documents (websites, files, pdfs and images) to analyzable and clean format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
