{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e54c6d",
   "metadata": {},
   "source": [
    "# CS-6570 Assignment #5 - Unsupervised Learning and Hypothesis Testing\n",
    "\n",
    "**YOUR NAME HERE**\n",
    "\n",
    "*Weber State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb2378-e743-4876-b8ef-59ca7edff0f9",
   "metadata": {},
   "source": [
    "In this assignment, we'll work through some examples of unsupervised learning and hypothesis testing based on what we covered in the lectures. Please note many of these sections will require some code plus some explanation. You should use markdown for the explanations, and be sure to include both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07ba1b-23b1-4d0c-bfdf-2a729b2024f3",
   "metadata": {},
   "source": [
    "But first, let's import our standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b58124-0857-488a-8872-9f363008d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac990bed-8f7f-4e8a-8817-f2738df8adef",
   "metadata": {},
   "source": [
    "**PCA and K-Means**\n",
    "\n",
    "We'll import the PCA and K-means functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac13ac-127d-4f3a-877a-85a3904c6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf8b0b-1a32-4469-b32e-109896412671",
   "metadata": {},
   "source": [
    "Next, we'll generate a simulated data set with 20 observations in each of three classes (so 60 observations total) with 50 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f12fed-7cae-4cb2-b55d-fe22cbace6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42);\n",
    "X = np.random.standard_normal((60,50));\n",
    "\n",
    "delta1 = np.random.uniform(.5,1,50)\n",
    "delta2 = np.random.uniform(-1,-.5,50)\n",
    "\n",
    "X[:20,:] += delta1\n",
    "X[20:40,:] += delta2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139d554-b0e4-4ca2-9673-5a62c15111b6",
   "metadata": {},
   "source": [
    "Perform PCA on the 60 observations and plot the first two principle component score vectors. Use a different color to indicate the observations in each of the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a7302-da03-44c6-9fa9-fd511a589338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f0ca4-2ae1-4165-b124-25ea714091e5",
   "metadata": {},
   "source": [
    "Perform $K$-means clustering of the observations with $K = 3$. How well do the clusters that you obtained in $K$-means clustering compare to the true class labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df698c3-4f2c-48d7-8a1e-4808914c2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b1a1a-58fc-4052-8d2a-81288e6115c6",
   "metadata": {},
   "source": [
    "Perform $K$-means clustering with $K$ = 2. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0f73e-5b37-479d-a702-c0e554810f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42ebaf-3a5a-41a6-b4ff-fb6806c0f140",
   "metadata": {},
   "source": [
    "Now perform $K$-means clustering with $K = 4$, and describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f29da-d62c-4269-8391-cfc260c632d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3f3f2-4b35-4c3b-a4f9-efad6a8189dc",
   "metadata": {},
   "source": [
    "Now perform $K$-means clustering with $K = 3$ on the first two principle component score vectors, rather than the raw data. That is, perform $K$-means clustering on the $60 \\times 2$ matrix of which the first columns is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f413891-49ef-44a6-8f09-6970e8225c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c194fc2-0e1b-47e4-975e-1fb8d2a06a71",
   "metadata": {},
   "source": [
    "Using the [StandardScalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) estimator, perform $K$-means clustering with $K = 3$ on the data *after scaling each variable to have standard deviation one*. How do these results compare to those obtained in (b)? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010f89b-5cf8-4598-94c4-14f2c0889585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_std = True, with_mean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3543f48-b35c-435b-9b29-691d34b6b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df8bdb-5e9c-4b67-b725-68db66177ef1",
   "metadata": {},
   "source": [
    "**Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc24e2f-7220-4854-9832-c6988ef5f2a2",
   "metadata": {},
   "source": [
    "Using the [USArrests](https://r-data.pmagunia.com/dataset/r-dataset-package-datasets-usarrests) dataset, perform hierarchical clustering on the states. You can find this dataset on Canvas. Also, you'll want to import the pacakages below and use the compute_linkage function defined below (from the lecture notes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708352b-efb9-479e-bb12-ed7a4b48a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def compute_linkage(hclust):\n",
    "    \"\"\"\n",
    "\n",
    "    Create linkage matrix used to plot a dendrogram\n",
    "\n",
    "    Follows [sklearn example](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    hclust : `sklearn.cluster.AgglomerativeClustering`\n",
    "        Fitted hierarchical clustering object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    linkage_matrix : np.ndarray\n",
    "        Array to be passed to `dendrogram` from `scipy.cluster.hierarchy`.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = np.zeros(hclust.children_.shape[0])\n",
    "    n_samples = len(hclust.labels_)\n",
    "    for i, merge in enumerate(hclust.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([hclust.children_, hclust.distances_,\n",
    "                                      counts]).astype(float)\n",
    "    return linkage_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682129c3-3dd4-46fb-980e-fea076b7c1b4",
   "metadata": {},
   "source": [
    "First, use hierarchical clustering with complete linkage and Euclidean distance to cluster the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6895c8e-e763-4b40-b433-38f73184b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126be2d-25f5-4fed-82a0-272819f6e52a",
   "metadata": {},
   "source": [
    "Cut the dendogram at a heigh that results in three distinct clusters. Which states belong to which clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c516c-e4fe-4b4c-87a5-1c108e29de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c84847-55a7-4aae-ba0b-e27095364bbf",
   "metadata": {},
   "source": [
    "Hierarchically cluster the states using complete linkage and Euclidean distance, *after scaling the variables to have standard deviation one*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76faeef-67de-4c7f-99e3-8d1d4835c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323357-fc55-41b7-b9a3-f47e84fde4cb",
   "metadata": {},
   "source": [
    "What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e71ae-2b05-469c-a225-c39b4599c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd94527-f7ad-4696-9c27-f5510b8c4094",
   "metadata": {},
   "source": [
    "**Hypothesis Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d5faf-ec5f-4b78-9c5c-e67dad334c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.standard_normal((20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42900e6-6975-4c12-8f2b-fb1ff8a4e865",
   "metadata": {},
   "source": [
    "For this part of the assignment, we've simulated (above) for $m = 100$ fund managers. These data represent each fund manager's percentage returns for each of $n = 20$ months. We wish to test the null hypothesis that each fund manager's percentage returns have population mean equal to zero. Notice that we simulated the data in such a way that each fund manager's percentage returns do have population mean zero; in other words, *all* $m$ null hypotheses are true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18bb07c-59f7-42d8-922a-fe4efb91ea28",
   "metadata": {},
   "source": [
    "Conduct a one-sample $t$-test for each fund manager, and plot a histogram of the $p$-values obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2797f-0dca-4974-86b6-06482012e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0da42-bb78-45d8-a3ca-24fba695dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcfd51-d9d1-4b0b-8e48-7cf3c34f87a6",
   "metadata": {},
   "source": [
    "If we control Type I error for each null hypothesis at level $\\alpha = .05$, then how many null hypotheses do we reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f72a5-e419-44e0-9bca-2e16779fbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests as mult_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c626ab-eda4-4a70-a638-583bb2cfa45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e48ce0-a50f-4647-a5d0-0d6069313c8c",
   "metadata": {},
   "source": [
    "If we control the FWER at level $.05$, then how many null hypotheses do we reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bf3d6-6bc8-48ce-94cf-adc70493e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0e3aa-0c85-4d32-9d53-9b5defca3ce5",
   "metadata": {},
   "source": [
    "If we control the FDR at level $.05$, then how many null hypotheses do we reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea65629-63ad-4ee5-a36b-a6d06c07fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f667630-9a27-4fa4-8819-63f50802633e",
   "metadata": {},
   "source": [
    "Now suppose we \"cherry-pick\" the 10 fund managers who perform the  best in our data. If we control the FWER for just these 10 fund managers at level $.05$, then how many null hypotheses do we reject? If we control the FDR for just these 10 fund managers at level $0.05$, then how many null hypotheses do we reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c6234-30a8-4f75-b6f4-4261da7eded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388f6b2-9320-4237-9748-ec8193cd74c0",
   "metadata": {},
   "source": [
    "Explain why the analysis directly above (where we \"cherry-picked\" the data) is misleading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
